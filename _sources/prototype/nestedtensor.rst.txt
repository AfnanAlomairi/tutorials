
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "prototype/nestedtensor.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_prototype_nestedtensor.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_prototype_nestedtensor.py:


NestedTensors
===============================================================

NestedTensors are similar to regular tensors, except for their shape:

* for a regular tensor, each dimension has a size

* for a nestedtensor, not all dimensions have regular sizes; some of them are jagged

Nestedtensors are a natural solution for representing sequential data within various domains:

* in NLP, sentences can have variable lengths, so a batch of sentences forms a nestedtensor

* in CV, images can have variable shapes, so a batch of images forms a nestedtensor

In this tutorial, we will demonstrate basic usage of nestedtensors and motivate their usefulness
for operating on sequential data of varying lengths with a real-world example.

NestedTensor are currently a prototype feature and are subject to change.

.. GENERATED FROM PYTHON SOURCE LINES 30-35

NestedTensor Initialization
----------------------------

From the Python frontend, a nestedtensor can be created from a list of tensors.
We denote nt[i] as the ith tensor component of a nestedtensor.

.. GENERATED FROM PYTHON SOURCE LINES 40-42

By padding every underlying tensor to the same shape,
a nestedtensor can be converted to a regular tensor.

.. GENERATED FROM PYTHON SOURCE LINES 46-47

All tensors posses an attribute for determining if they are nested;

.. GENERATED FROM PYTHON SOURCE LINES 51-54

It is common to construct nestedtensors from batches of irregularly shaped tensors.
i.e. dimension 0 is assumed to be the batch dimension.
Indexing dimension 0 gives back the first underlying tensor component.

.. GENERATED FROM PYTHON SOURCE LINES 54-60

.. code-block:: default




    # When indexing a nestedtensor's 0th dimension, the result is a regular tensor.



.. GENERATED FROM PYTHON SOURCE LINES 61-64

An important note is that slicing in dimension 0 has not been supported yet.
Which means it not currently possible to construct a view that combines the underlying
tensor components.

.. GENERATED FROM PYTHON SOURCE LINES 66-88

Nested Tensor Operations
------------------------

As each operation must be explicitly implemented for nestedtensors,
operation coverage for nestedtensors is currently narrower than that of regular tensors.
For now, only basic operations such as index, dropout, softmax, transpose, reshape, linear, bmm are covered.
However, coverage is being expanded.
If you need certain operations, please file an `issue <https://github.com/pytorch/pytorch>`__
to help us prioritize coverage.

**reshape**

The reshape op is for changing the shape of a tensor.
Its full semantics for regular tensors can be found
`here <https://pytorch.org/docs/stable/generated/torch.reshape.html>`__.
For regular tensors, when specifying the new shape,
a single dimension may be -1, in which case it is inferred
from the remaining dimensions and the number of elements.

The semantics for nestedtensors are similar, except that -1 no longer infers.
Instead, it inherits the old size (here 2 for ``nt[0]`` and 3 for ``nt[1]``).
-1 is the only legal size to specify for a jagged dimension.

.. GENERATED FROM PYTHON SOURCE LINES 92-100

**transpose**

The transpose op is for swapping two dimensions of a tensor.
Its full semantics can be found
`here <https://pytorch.org/docs/stable/generated/torch.transpose.html>`__.
Note that for nestedtensors dimension 0 is special;
it is assumed to be the batch dimension,
so transposes involving nestedtensor dimension 0 are not supported.

.. GENERATED FROM PYTHON SOURCE LINES 104-110

**others**

Other operations have the same semantics as for regular tensors.
Applying the operation on a nestedtensor is equivalent to
applying the operation to the underlying tensor components,
with the result being a nestedtensor as well.

.. GENERATED FROM PYTHON SOURCE LINES 121-124

Why Nested Tensor
-----------------


.. GENERATED FROM PYTHON SOURCE LINES 126-132

When data is sequential, it is often the case that each sample has a different length.
For example, in a batch of sentences, each sentence has a different number of words.
A common technique for handling varying sequences is to manually pad each data tensor
to the same shape in order to form a batch.
For example, we have 2 sentences with different lengths and a vocabulary
In order to represent his as single tensor we pad with 0 to the max length in the batch.

.. GENERATED FROM PYTHON SOURCE LINES 144-150

This techinque of padding a batch of data to its max length is not optimal.
The padded data is not needed for computation and wastes memory by allocating
larger tensors than necessary.
Further, not all operations have the same semnatics when applied to padded data.
For matrix multiplications in order to ignore the padded entries, one needs to pad
with 0 while for softmax one has to pad with -inf to ignore specific entries.

.. GENERATED FROM PYTHON SOURCE LINES 156-159

Let us take a look at a practical example: the multi-head attention component
utilized in `Transformers <https://arxiv.org/pdf/1706.03762.pdf>`__.
The nestedtensor version is straightforward.

.. GENERATED FROM PYTHON SOURCE LINES 245-247

The 0-padded tensor version additionally requires masks
for more complicated treatments at padded entries.

.. GENERATED FROM PYTHON SOURCE LINES 348-349

set hyperparameters following `the Transformer paper <https://arxiv.org/pdf/1706.03762.pdf>`__

.. GENERATED FROM PYTHON SOURCE LINES 354-355

except for dropout probability: set to 0 for correctness check

.. GENERATED FROM PYTHON SOURCE LINES 358-359

Let us generate some realistic fake data from Zipf's law.

.. GENERATED FROM PYTHON SOURCE LINES 380-381

create inputs

.. GENERATED FROM PYTHON SOURCE LINES 381-416

.. code-block:: default


    # create parameters





    # create nested input













    # pad input




    # create attention masks



    #  We need to mask out the padding entries in the attention weights.





.. GENERATED FROM PYTHON SOURCE LINES 417-418

check correctness and performance

.. GENERATED FROM PYTHON SOURCE LINES 442-449

Although the nestedtensor version avoids wasted computation on padding, it is not faster
then the equivalent padded tensor version. This is because the nestedtensor version
has implemented a few of the kernels, like softmax, in a non optimal way.

There are plans to implement performance critical operations using the new Pytorch 2.0 stack
For now, some performant kernels are provided for specific use cases, e.g.
self-attention evaluation by multi-head attention formula.

.. GENERATED FROM PYTHON SOURCE LINES 449-455

.. code-block:: default


    # embeddings are assumed to be the same





.. GENERATED FROM PYTHON SOURCE LINES 456-457

extract parameters for correctness check

.. GENERATED FROM PYTHON SOURCE LINES 467-472

If we set need_weights to False this will enable the fast path in the library.
Under the hood this will call _scaled_dot_product_attention. If your tensors
are on CUDA, than a fused, efficient attention kernel will be used. For
more detailed performance characteristics look at the benchmark in
pytorch/benchmarks/transformer/sdp.py

.. GENERATED FROM PYTHON SOURCE LINES 472-493

.. code-block:: default






















    # %%%%%%RUNNABLE_CODE_REMOVED%%%%%%

.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  0.000 seconds)


.. _sphx_glr_download_prototype_nestedtensor.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example


    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: nestedtensor.py <nestedtensor.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: nestedtensor.ipynb <nestedtensor.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
