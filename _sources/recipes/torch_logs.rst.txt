
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "recipes/torch_logs.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_recipes_torch_logs.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_recipes_torch_logs.py:


(beta) Using TORCH_LOGS python API with torch.compile
==========================================================================================
**Author:** `Michael Lazos <https://github.com/mlazos>`_

.. GENERATED FROM PYTHON SOURCE LINES 6-9

.. code-block:: default


    import logging








.. GENERATED FROM PYTHON SOURCE LINES 10-18

This tutorial introduces the ``TORCH_LOGS`` environment variable, as well as the Python API, and
demonstrates how to apply it to observe the phases  of ``torch.compile``.

.. note::

  This tutorial requires PyTorch 2.2.0 or later.



.. GENERATED FROM PYTHON SOURCE LINES 22-32

Setup
~~~~~~~~~~~~~~~~~~~~~
In this example, we'll set up a simple Python function which performs an elementwise
add and observe the compilation process with ``TORCH_LOGS`` Python API.

.. note::

  There is also an environment variable ``TORCH_LOGS``, which can be used to
  change logging settings at the command line. The equivalent environment
  variable setting is shown for each example.

.. GENERATED FROM PYTHON SOURCE LINES 32-81

.. code-block:: default


    import torch

    # exit cleanly if we are on a device that doesn't support torch.compile
    if torch.cuda.get_device_capability() < (7, 0):
        print("Skipping because torch.compile is not supported on this device.")
    else:
        @torch.compile()
        def fn(x, y):
            z = x + y
            return z + 2


        inputs = (torch.ones(2, 2, device="cuda"), torch.zeros(2, 2, device="cuda"))


    # print separator and reset dynamo
    # between each example
        def separator(name):
            print(f"==================={name}=========================")
            torch._dynamo.reset()


        separator("Dynamo Tracing")
    # View dynamo tracing
    # TORCH_LOGS="+dynamo"
        torch._logging.set_logs(dynamo=logging.DEBUG)
        fn(*inputs)

        separator("Traced Graph")
    # View traced graph
    # TORCH_LOGS="graph"
        torch._logging.set_logs(graph=True)
        fn(*inputs)

        separator("Fusion Decisions")
    # View fusion decisions
    # TORCH_LOGS="fusion"
        torch._logging.set_logs(fusion=True)
        fn(*inputs)

        separator("Output Code")
    # View output code generated by inductor
    # TORCH_LOGS="output_code"
        torch._logging.set_logs(output_code=True)
        fn(*inputs)

        separator("")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    ===================Dynamo Tracing=========================
    [2024-04-10 18:04:07,199] torch._dynamo.convert_frame: [DEBUG] skipping because no torch.* _splitext             /opt/conda/envs/py_3.10/lib/python3.10/genericpath.py 121
    [2024-04-10 18:04:07,200] torch._dynamo.convert_frame: [DEBUG] skipping because no torch.* write             /opt/conda/envs/py_3.10/lib/python3.10/site-packages/sphinx_gallery/gen_rst.py 83
    [2024-04-10 18:04:07,199] torch._dynamo.eval_frame: [DEBUG] Setting top-level compile config hash: a1caab6551bd63b2a48349459b9f4a74
    [2024-04-10 18:04:07,200] torch._dynamo.convert_frame: [DEBUG] skipping because no torch.* verbose             /opt/conda/envs/py_3.10/lib/python3.10/site-packages/sphinx/util/logging.py 127
    [2024-04-10 18:04:07,200] torch._dynamo.convert_frame: [DEBUG] skipping because no torch.* log             /opt/conda/envs/py_3.10/lib/python3.10/site-packages/sphinx/util/logging.py 120
    [2024-04-10 18:04:07,200] torch._dynamo.convert_frame: [DEBUG] skipping because no torch.* process             /opt/conda/envs/py_3.10/lib/python3.10/site-packages/sphinx/util/logging.py 130
    [2024-04-10 18:04:07,200] torch._dynamo.convert_frame: [DEBUG] skipping because no torch.* filter             /opt/conda/envs/py_3.10/lib/python3.10/site-packages/sphinx/util/logging.py 350
    [2024-04-10 18:04:07,201] torch._dynamo.convert_frame: [DEBUG] skipping because no torch.* filter             /opt/conda/envs/py_3.10/lib/python3.10/site-packages/sphinx/util/logging.py 483
    [2024-04-10 18:04:07,201] torch._dynamo.convert_frame: [DEBUG] skipping because no torch.* emit             /opt/conda/envs/py_3.10/lib/python3.10/site-packages/sphinx/util/logging.py 150
    [2024-04-10 18:04:07,201] torch._dynamo.convert_frame: [DEBUG] skipping because no torch.* format             /opt/conda/envs/py_3.10/lib/python3.10/site-packages/sphinx/util/logging.py 528
    [2024-04-10 18:04:07,201] torch._dynamo.convert_frame: [DEBUG] skipping because no torch.* getMessage             /opt/conda/envs/py_3.10/lib/python3.10/site-packages/sphinx/util/logging.py 88
    [2024-04-10 18:04:07,201] torch._dynamo.convert_frame: [DEBUG] skipping because no torch.* write             /opt/conda/envs/py_3.10/lib/python3.10/site-packages/sphinx/util/logging.py 546
    [2024-04-10 18:04:07,202] torch._dynamo.convert_frame: [DEBUG] skipping because no torch.* flush             /opt/conda/envs/py_3.10/lib/python3.10/site-packages/sphinx/util/logging.py 554
    [2024-04-10 18:04:07,202] torch._dynamo.convert_frame: [DEBUG] skipping because no torch.* write             /opt/conda/envs/py_3.10/lib/python3.10/site-packages/sphinx/util/logging.py 564
    [2024-04-10 18:04:07,202] torch._dynamo.convert_frame: [DEBUG] skipping because no torch.* flush             /opt/conda/envs/py_3.10/lib/python3.10/site-packages/sphinx_gallery/gen_rst.py 104
    [2024-04-10 18:04:07,203] [4/0] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing fn /var/lib/workspace/recipes_source/torch_logs.py:39
    [2024-04-10 18:04:07,203] [4/0] torch.fx.experimental.symbolic_shapes: [INFO] create_env
    [2024-04-10 18:04:07,203] [4/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG] TRACE starts_line /var/lib/workspace/recipes_source/torch_logs.py:39 in fn ()
    [2024-04-10 18:04:07,203] [4/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG]         @torch.compile()
    [2024-04-10 18:04:07,205] [4/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG] TRACE starts_line /var/lib/workspace/recipes_source/torch_logs.py:41 in fn (fn)
    [2024-04-10 18:04:07,205] [4/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG]             z = x + y
    [2024-04-10 18:04:07,205] [4/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST x []
    [2024-04-10 18:04:07,205] [4/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST y [LazyVariableTracker()]
    [2024-04-10 18:04:07,205] [4/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE BINARY_ADD None [LazyVariableTracker(), LazyVariableTracker()]
    [2024-04-10 18:04:07,206] [4/0] torch._dynamo.output_graph: [DEBUG] create_graph_input L_x_ L['x']
    [2024-04-10 18:04:07,206] [4/0] torch._dynamo.variables.builder: [DEBUG] wrap_to_fake L['x'] (2, 2) [<DimDynamic.STATIC: 2>, <DimDynamic.STATIC: 2>] [None, None]
    [2024-04-10 18:04:07,207] [4/0] torch._dynamo.output_graph: [DEBUG] create_graph_input L_y_ L['y']
    [2024-04-10 18:04:07,207] [4/0] torch._dynamo.variables.builder: [DEBUG] wrap_to_fake L['y'] (2, 2) [<DimDynamic.STATIC: 2>, <DimDynamic.STATIC: 2>] [None, None]
    [2024-04-10 18:04:07,208] [4/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST z [TensorVariable()]
    [2024-04-10 18:04:07,209] [4/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG] TRACE starts_line /var/lib/workspace/recipes_source/torch_logs.py:42 in fn (fn)
    [2024-04-10 18:04:07,209] [4/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG]             return z + 2
    [2024-04-10 18:04:07,209] [4/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST z []
    [2024-04-10 18:04:07,209] [4/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_CONST 2 [TensorVariable()]
    [2024-04-10 18:04:07,209] [4/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE BINARY_ADD None [TensorVariable(), ConstantVariable(int)]
    [2024-04-10 18:04:07,210] [4/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE RETURN_VALUE None [TensorVariable()]
    [2024-04-10 18:04:07,210] [4/0] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing fn (RETURN_VALUE)
    [2024-04-10 18:04:07,210] [4/0] torch._dynamo.symbolic_convert: [DEBUG] RETURN_VALUE triggered compile
    [2024-04-10 18:04:07,210] [4/0] torch._dynamo.output_graph: [DEBUG] COMPILING GRAPH due to GraphCompileReason(reason='return_value', user_stack=[<FrameSummary file /var/lib/workspace/recipes_source/torch_logs.py, line 42 in fn>], graph_break=False)
    [2024-04-10 18:04:07,211] [4/0] torch._dynamo.output_graph.__graph_code: [DEBUG] TRACED GRAPH
    [2024-04-10 18:04:07,211] [4/0] torch._dynamo.output_graph.__graph_code: [DEBUG]  ===== __compiled_fn_4 =====
    [2024-04-10 18:04:07,211] [4/0] torch._dynamo.output_graph.__graph_code: [DEBUG]  <eval_with_key>.878 class GraphModule(torch.nn.Module):
    [2024-04-10 18:04:07,211] [4/0] torch._dynamo.output_graph.__graph_code: [DEBUG]     def forward(self, L_x_ : torch.Tensor, L_y_ : torch.Tensor):
    [2024-04-10 18:04:07,211] [4/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         l_x_ = L_x_
    [2024-04-10 18:04:07,211] [4/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         l_y_ = L_y_
    [2024-04-10 18:04:07,211] [4/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         
    [2024-04-10 18:04:07,211] [4/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         # File: /var/lib/workspace/recipes_source/torch_logs.py:41, code: z = x + y
    [2024-04-10 18:04:07,211] [4/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         z = l_x_ + l_y_;  l_x_ = l_y_ = None
    [2024-04-10 18:04:07,211] [4/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         
    [2024-04-10 18:04:07,211] [4/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         # File: /var/lib/workspace/recipes_source/torch_logs.py:42, code: return z + 2
    [2024-04-10 18:04:07,211] [4/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         add_1 = z + 2;  z = None
    [2024-04-10 18:04:07,211] [4/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         return (add_1,)
    [2024-04-10 18:04:07,211] [4/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         
    [2024-04-10 18:04:07,211] [4/0] torch._dynamo.output_graph.__graph_code: [DEBUG] 
    [2024-04-10 18:04:07,213] [4/0] torch._dynamo.output_graph.__graph: [DEBUG] TRACED GRAPH
    [2024-04-10 18:04:07,213] [4/0] torch._dynamo.output_graph.__graph: [DEBUG]  __compiled_fn_4 <eval_with_key>.878 opcode         name    target                   args          kwargs
    [2024-04-10 18:04:07,213] [4/0] torch._dynamo.output_graph.__graph: [DEBUG] -------------  ------  -----------------------  ------------  --------
    [2024-04-10 18:04:07,213] [4/0] torch._dynamo.output_graph.__graph: [DEBUG] placeholder    l_x_    L_x_                     ()            {}
    [2024-04-10 18:04:07,213] [4/0] torch._dynamo.output_graph.__graph: [DEBUG] placeholder    l_y_    L_y_                     ()            {}
    [2024-04-10 18:04:07,213] [4/0] torch._dynamo.output_graph.__graph: [DEBUG] call_function  z       <built-in function add>  (l_x_, l_y_)  {}
    [2024-04-10 18:04:07,213] [4/0] torch._dynamo.output_graph.__graph: [DEBUG] call_function  add_1   <built-in function add>  (z, 2)        {}
    [2024-04-10 18:04:07,213] [4/0] torch._dynamo.output_graph.__graph: [DEBUG] output         output  output                   ((add_1,),)   {}
    [2024-04-10 18:04:07,213] [4/0] torch._dynamo.output_graph.__graph: [DEBUG] 
    [2024-04-10 18:04:07,217] [4/0] torch._dynamo.output_graph.__graph_sizes: [DEBUG] TRACED GRAPH TENSOR SIZES
    [2024-04-10 18:04:07,217] [4/0] torch._dynamo.output_graph.__graph_sizes: [DEBUG] ===== __compiled_fn_4 =====
    [2024-04-10 18:04:07,217] [4/0] torch._dynamo.output_graph.__graph_sizes: [DEBUG] l_x_: (2, 2)
    [2024-04-10 18:04:07,217] [4/0] torch._dynamo.output_graph.__graph_sizes: [DEBUG] l_y_: (2, 2)
    [2024-04-10 18:04:07,217] [4/0] torch._dynamo.output_graph.__graph_sizes: [DEBUG] z: (2, 2)
    [2024-04-10 18:04:07,217] [4/0] torch._dynamo.output_graph.__graph_sizes: [DEBUG] add_1: (2, 2)
    [2024-04-10 18:04:07,217] [4/0] torch._dynamo.output_graph.__graph_sizes: [DEBUG] 
    [2024-04-10 18:04:07,217] [4/0] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function inductor
    [2024-04-10 18:04:07,237] [4/0] torch.fx.experimental.symbolic_shapes: [DEBUG] eval True == True [statically known]
    [2024-04-10 18:04:08,234] [4/0] torch._dynamo.output_graph: [INFO] Step 2: done compiler function inductor
    [2024-04-10 18:04:08,235] [4/0] torch.fx.experimental.symbolic_shapes: [INFO] produce_guards
    [2024-04-10 18:04:08,236] [4/0] torch.fx.experimental.symbolic_shapes: [DEBUG] track_symint L['x'].size()[0] 2 None
    [2024-04-10 18:04:08,236] [4/0] torch.fx.experimental.symbolic_shapes: [DEBUG] track_symint L['x'].size()[1] 2 None
    [2024-04-10 18:04:08,236] [4/0] torch.fx.experimental.symbolic_shapes: [DEBUG] track_symint L['x'].stride()[0] 2 None
    [2024-04-10 18:04:08,236] [4/0] torch.fx.experimental.symbolic_shapes: [DEBUG] track_symint L['x'].stride()[1] 1 None
    [2024-04-10 18:04:08,236] [4/0] torch.fx.experimental.symbolic_shapes: [DEBUG] track_symint L['x'].storage_offset() 0 None
    [2024-04-10 18:04:08,237] [4/0] torch.fx.experimental.symbolic_shapes: [DEBUG] track_symint L['y'].size()[0] 2 None
    [2024-04-10 18:04:08,237] [4/0] torch.fx.experimental.symbolic_shapes: [DEBUG] track_symint L['y'].size()[1] 2 None
    [2024-04-10 18:04:08,237] [4/0] torch.fx.experimental.symbolic_shapes: [DEBUG] track_symint L['y'].stride()[0] 2 None
    [2024-04-10 18:04:08,237] [4/0] torch.fx.experimental.symbolic_shapes: [DEBUG] track_symint L['y'].stride()[1] 1 None
    [2024-04-10 18:04:08,237] [4/0] torch.fx.experimental.symbolic_shapes: [DEBUG] track_symint L['y'].storage_offset() 0 None
    [2024-04-10 18:04:08,237] [4/0] torch.fx.experimental.symbolic_shapes: [DEBUG] Skipping guard L['x'].size()[0] == 2
    [2024-04-10 18:04:08,237] [4/0] torch.fx.experimental.symbolic_shapes: [DEBUG] Skipping guard L['x'].size()[1] == 2
    [2024-04-10 18:04:08,238] [4/0] torch.fx.experimental.symbolic_shapes: [DEBUG] Skipping guard L['x'].stride()[0] == 2
    [2024-04-10 18:04:08,238] [4/0] torch.fx.experimental.symbolic_shapes: [DEBUG] Skipping guard L['x'].stride()[1] == 1
    [2024-04-10 18:04:08,238] [4/0] torch.fx.experimental.symbolic_shapes: [DEBUG] Skipping guard L['x'].storage_offset() == 0
    [2024-04-10 18:04:08,238] [4/0] torch.fx.experimental.symbolic_shapes: [DEBUG] Skipping guard L['y'].size()[0] == 2
    [2024-04-10 18:04:08,238] [4/0] torch.fx.experimental.symbolic_shapes: [DEBUG] Skipping guard L['y'].size()[1] == 2
    [2024-04-10 18:04:08,238] [4/0] torch.fx.experimental.symbolic_shapes: [DEBUG] Skipping guard L['y'].stride()[0] == 2
    [2024-04-10 18:04:08,238] [4/0] torch.fx.experimental.symbolic_shapes: [DEBUG] Skipping guard L['y'].stride()[1] == 1
    [2024-04-10 18:04:08,239] [4/0] torch.fx.experimental.symbolic_shapes: [DEBUG] Skipping guard L['y'].storage_offset() == 0
    [2024-04-10 18:04:08,239] [4/0] torch._dynamo.guards.__guards: [DEBUG] GUARDS:
    [2024-04-10 18:04:08,239] [4/0] torch._dynamo.guards.__guards: [DEBUG] hasattr(L['x'], '_dynamo_dynamic_indices') == False           # z = x + y  # ar/lib/workspace/recipes_source/torch_logs.py:41 in fn
    [2024-04-10 18:04:08,240] [4/0] torch._dynamo.guards.__guards: [DEBUG] hasattr(L['y'], '_dynamo_dynamic_indices') == False           # z = x + y  # ar/lib/workspace/recipes_source/torch_logs.py:41 in fn
    [2024-04-10 18:04:08,241] [4/0] torch._dynamo.guards.__guards: [DEBUG] utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:379 in init_ambient_guards
    [2024-04-10 18:04:08,241] [4/0] torch._dynamo.guards.__guards: [DEBUG] (___skip_backend_check() or ___current_backend() == ___lookup_backend(139734879898160))  # _dynamo/output_graph.py:385 in init_ambient_guards
    [2024-04-10 18:04:08,241] [4/0] torch._dynamo.guards.__guards: [DEBUG] ___compile_config_hash() == 'a1caab6551bd63b2a48349459b9f4a74'  # _dynamo/output_graph.py:387 in init_ambient_guards
    [2024-04-10 18:04:08,242] [4/0] torch._dynamo.guards.__guards: [DEBUG] check_tensor(L['x'], Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.float32, device=0, requires_grad=False, size=[2, 2], stride=[2, 1])  # z = x + y  # ar/lib/workspace/recipes_source/torch_logs.py:41 in fn
    [2024-04-10 18:04:08,242] [4/0] torch._dynamo.guards.__guards: [DEBUG] check_tensor(L['y'], Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.float32, device=0, requires_grad=False, size=[2, 2], stride=[2, 1])  # z = x + y  # ar/lib/workspace/recipes_source/torch_logs.py:41 in fn
    [2024-04-10 18:04:08,245] torch._dynamo.eval_frame: [DEBUG] Unsetting top-level compile config hash: a1caab6551bd63b2a48349459b9f4a74
    ===================Traced Graph=========================
    [2024-04-10 18:04:08,254] [5/0] torch._dynamo.output_graph.__graph: [DEBUG] TRACED GRAPH
    [2024-04-10 18:04:08,254] [5/0] torch._dynamo.output_graph.__graph: [DEBUG]  __compiled_fn_5 <eval_with_key>.885 opcode         name    target                   args          kwargs
    [2024-04-10 18:04:08,254] [5/0] torch._dynamo.output_graph.__graph: [DEBUG] -------------  ------  -----------------------  ------------  --------
    [2024-04-10 18:04:08,254] [5/0] torch._dynamo.output_graph.__graph: [DEBUG] placeholder    l_x_    L_x_                     ()            {}
    [2024-04-10 18:04:08,254] [5/0] torch._dynamo.output_graph.__graph: [DEBUG] placeholder    l_y_    L_y_                     ()            {}
    [2024-04-10 18:04:08,254] [5/0] torch._dynamo.output_graph.__graph: [DEBUG] call_function  z       <built-in function add>  (l_x_, l_y_)  {}
    [2024-04-10 18:04:08,254] [5/0] torch._dynamo.output_graph.__graph: [DEBUG] call_function  add_1   <built-in function add>  (z, 2)        {}
    [2024-04-10 18:04:08,254] [5/0] torch._dynamo.output_graph.__graph: [DEBUG] output         output  output                   ((add_1,),)   {}
    [2024-04-10 18:04:08,254] [5/0] torch._dynamo.output_graph.__graph: [DEBUG] 
    ===================Fusion Decisions=========================
    [2024-04-10 18:04:08,342] [6/0] torch._inductor.scheduler.__fusion: [DEBUG] ===== attempting fusion (1/10): 1 nodes =====
    [2024-04-10 18:04:08,343] [6/0] torch._inductor.scheduler.__fusion: [DEBUG] 
    [2024-04-10 18:04:08,343] [6/0] torch._inductor.scheduler.__fusion: [DEBUG] found 0 possible fusions:
    [2024-04-10 18:04:08,343] [6/0] torch._inductor.scheduler.__fusion: [DEBUG] 
    [2024-04-10 18:04:08,343] [6/0] torch._inductor.scheduler.__fusion: [DEBUG] completed fusion round (1/10): fused 1 nodes into 1 nodes
    [2024-04-10 18:04:08,343] [6/0] torch._inductor.scheduler.__fusion: [DEBUG] 
    [2024-04-10 18:04:08,343] [6/0] torch._inductor.scheduler.__fusion: [DEBUG] ===== fusion complete (1 iterations) =====
    ===================Output Code=========================
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] Output code: 
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] 
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] from ctypes import c_void_p, c_long
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] import torch
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] import math
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] import random
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] import os
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] import tempfile
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] from math import inf, nan
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] from torch._inductor.hooks import run_intermediate_hooks
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] from torch._inductor.utils import maybe_profile
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] from torch._inductor.codegen.memory_planning import _align as align
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] 
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] from torch import device, empty, empty_strided
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] from torch._inductor.codecache import AsyncCompile
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] from torch._inductor.select_algorithm import extern_kernels
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] 
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] aten = torch.ops.aten
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] inductor_ops = torch.ops.inductor
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] assert_size_stride = torch._C._dynamo.guards.assert_size_stride
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] alloc_from_pool = torch.ops.inductor._alloc_from_pool
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] reinterpret_tensor = torch.ops.inductor._reinterpret_tensor
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] async_compile = AsyncCompile()
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] 
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] 
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] # kernel path: /tmp/torchinductor_root/m2/cm2xck7sgjgp6lkfeey7p42pd23kbwccqkr2hi7pcglpljrtzgqt.py
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] # Source Nodes: [add_1, z], Original ATen: [aten.add]
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] # add_1 => add_1
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] # z => add
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] triton_poi_fused_add_0 = async_compile.triton('triton_', '''
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] import triton
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] import triton.language as tl
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] from torch._inductor.ir import ReductionHint
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] from torch._inductor.ir import TileHint
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] from torch._inductor.triton_heuristics import AutotuneHint, pointwise
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] from torch._inductor.utils import instance_descriptor
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] from torch._inductor import triton_helpers
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] 
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] @pointwise(
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG]     size_hints=[4], 
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG]     filename=__file__,
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG]     triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=())]},
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG]     inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_0', 'mutated_arg_names': []},
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG]     min_elem_per_thread=0
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] )
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] @triton.jit
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] def triton_(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG]     xnumel = 4
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG]     xoffset = tl.program_id(0) * XBLOCK
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG]     xindex = xoffset + tl.arange(0, XBLOCK)[:]
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG]     xmask = xindex < xnumel
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG]     x0 = xindex
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG]     tmp0 = tl.load(in_ptr0 + (x0), xmask)
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG]     tmp1 = tl.load(in_ptr1 + (x0), xmask)
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG]     tmp2 = tmp0 + tmp1
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG]     tmp3 = 2.0
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG]     tmp4 = tmp2 + tmp3
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG]     tl.store(out_ptr0 + (x0), tmp4, xmask)
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] ''')
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] 
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] import triton
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] import triton.language as tl
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] from torch._inductor.triton_heuristics import grid, start_graph, end_graph
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] from torch._C import _cuda_getCurrentRawStream as get_cuda_stream
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] 
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] 
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] async_compile.wait(globals())
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] del async_compile
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] 
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] def call(args):
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG]     arg0_1, arg1_1 = args
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG]     args.clear()
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG]     assert_size_stride(arg0_1, (2, 2), (2, 1))
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG]     assert_size_stride(arg1_1, (2, 2), (2, 1))
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG]     with torch.cuda._DeviceGuard(0):
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG]         torch.cuda.set_device(0) # no-op to ensure context
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG]         buf0 = empty((2, 2), device='cuda', dtype=torch.float32)
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG]         # Source Nodes: [add_1, z], Original ATen: [aten.add]
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG]         stream0 = get_cuda_stream(0)
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG]         triton_poi_fused_add_0.run(arg0_1, arg1_1, buf0, 4, grid=grid(4), stream=stream0)
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG]         del arg0_1
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG]         del arg1_1
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG]         return (buf0, )
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] 
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] 
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] def benchmark_compiled_module(times=10, repeat=10):
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG]     from torch._dynamo.testing import rand_strided
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG]     from torch._inductor.utils import print_performance
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG]     arg0_1 = rand_strided((2, 2), (2, 1), device='cuda:0', dtype=torch.float32)
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG]     arg1_1 = rand_strided((2, 2), (2, 1), device='cuda:0', dtype=torch.float32)
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG]     fn = lambda: call([arg0_1, arg1_1])
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG]     return print_performance(fn, times=times, repeat=repeat)
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] 
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] 
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] if __name__ == "__main__":
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG]     from torch._inductor.wrapper_benchmark import compiled_module_main
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG]     compiled_module_main('None', benchmark_compiled_module)
    [2024-04-10 18:04:08,424] [7/0] torch._inductor.graph.__output_code: [DEBUG] 
    [2024-04-10 18:04:08,431] [7/0] torch._inductor.graph.__output_code: [INFO] Output code written to: /tmp/torchinductor_root/hf/chfxo7ncemojfgbfwnmcepng6zr6w4ok6zdzpybq6ifdlrqmu3zh.py
    ============================================




.. GENERATED FROM PYTHON SOURCE LINES 82-97

Conclusion
~~~~~~~~~~

In this tutorial we introduced the TORCH_LOGS environment variable and python API
by experimenting with a small number of the available logging options.
To view descriptions of all available options, run any python script
which imports torch and set TORCH_LOGS to "help".

Alternatively, you can view the `torch._logging documentation`_ to see
descriptions of all available logging options.

For more information on torch.compile, see the `torch.compile tutorial`_.

.. _torch._logging documentation: https://pytorch.org/docs/main/logging.html
.. _torch.compile tutorial: https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  1.245 seconds)


.. _sphx_glr_download_recipes_torch_logs.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example


    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: torch_logs.py <torch_logs.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: torch_logs.ipynb <torch_logs.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
