
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "advanced/usb_semisup_learn.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_advanced_usb_semisup_learn.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_advanced_usb_semisup_learn.py:


Semi-Supervised Learning using USB built upon PyTorch
=====================================================

**Author**: `Hao Chen <https://github.com/Hhhhhhao>`_

Unified Semi-supervised learning Benchmark (USB) is a semi-supervised
learning (SSL) framework built upon PyTorch.
Based on Datasets and Modules provided by PyTorch, USB becomes a flexible,
modular, and easy-to-use framework for semi-supervised learning.
It supports a variety of semi-supervised learning algorithms, including
``FixMatch``, ``FreeMatch``, ``DeFixMatch``, ``SoftMatch``, and so on.
It also supports a variety of imbalanced semi-supervised learning algorithms.
The benchmark results across different datasets of computer vision, natural
language processing, and speech processing are included in USB.

This tutorial will walk you through the basics of using the USB lighting
package.
Let's get started by training a ``FreeMatch``/``SoftMatch`` model on
CIFAR-10 using pretrained Vision Transformers (ViT)!
And we will show it is easy to change the semi-supervised algorithm and train
on imbalanced datasets.


.. figure:: /_static/img/usb_semisup_learn/code.png
   :alt: USB framework illustration

.. GENERATED FROM PYTHON SOURCE LINES 31-48

Introduction to ``FreeMatch`` and ``SoftMatch`` in Semi-Supervised Learning
---------------------------------------------------------------------------

Here we provide a brief introduction to ``FreeMatch`` and ``SoftMatch``.
First, we introduce a famous baseline for semi-supervised learning called ``FixMatch``.
``FixMatch`` is a very simple framework for semi-supervised learning, where it
utilizes a strong augmentation to generate pseudo labels for unlabeled data.
It adopts a confidence thresholding strategy to filter out the low-confidence
pseudo labels with a fixed threshold set.
``FreeMatch`` and ``SoftMatch`` are two algorithms that improve upon ``FixMatch``.
``FreeMatch`` proposes adaptive thresholding strategy to replace the fixed
thresholding strategy in ``FixMatch``. The adaptive thresholding progressively
increases the threshold according to the learning status of the model on each
class. ``SoftMatch`` absorbs the idea of confidence thresholding as an
weighting mechanism. It proposes a Gaussian weighting mechanism to overcome
the quantity-quality trade-off in pseudo-labels. In this tutorial, we will
use USB to train ``FreeMatch`` and ``SoftMatch``.

.. GENERATED FROM PYTHON SOURCE LINES 51-87

Use USB to Train ``FreeMatch``/``SoftMatch`` on CIFAR-10 with only 40 labels
----------------------------------------------------------------------------

USB is easy to use and extend, affordable to small groups, and comprehensive
for developing and evaluating SSL algorithms.
USB provides the implementation of 14 SSL algorithms based on Consistency
Regularization, and 15 tasks for evaluation from CV, NLP, and Audio domain.
It has a modular design that allows users to easily extend the package by
adding new algorithms and tasks.
It also supports a Python API for easier adaptation to different SSL
algorithms on new data.


Now, let's use USB to train ``FreeMatch`` and ``SoftMatch`` on CIFAR-10.
First, we need to install USB package ``semilearn`` and import necessary API
functions from USB.
If you are running this in Google Colab, install ``semilearn`` by running:
``!pip install semilearn``.

Below is a list of functions we will use from ``semilearn``:

- ``get_dataset`` to load dataset, here we use CIFAR-10
- ``get_data_loader`` to create train (labeled and unlabeled) and test data
loaders, the train unlabeled loaders will provide both strong and weak
augmentation of unlabeled data
- ``get_net_builder`` to create a model, here we use pretrained ViT
- ``get_algorithm`` to create the semi-supervised learning algorithm,
here we use ``FreeMatch`` and ``SoftMatch``
- ``get_config``: to get default configuration of the algorithm
- ``Trainer``: a Trainer class for training and evaluating the
algorithm on dataset

Note that a CUDA-enabled backend is required for training with the ``semilearn`` package.
See `Enabling CUDA in Google Colab <https://pytorch.org/tutorials/beginner/colab#enabling-cuda>`__ for instructions
on enabling CUDA in Google Colab.


.. GENERATED FROM PYTHON SOURCE LINES 87-90

.. code-block:: default

    import semilearn
    from semilearn import get_dataset, get_data_loader, get_net_builder, get_algorithm, get_config, Trainer





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /opt/conda/envs/py_3.10/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning:

    torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.





.. GENERATED FROM PYTHON SOURCE LINES 91-94

After importing necessary functions, we first set the hyper-parameters of the
algorithm.


.. GENERATED FROM PYTHON SOURCE LINES 94-138

.. code-block:: default

    config = {
        'algorithm': 'freematch',
        'net': 'vit_tiny_patch2_32',
        'use_pretrain': True, 
        'pretrain_path': 'https://github.com/microsoft/Semi-supervised-learning/releases/download/v.0.0.0/vit_tiny_patch2_32_mlp_im_1k_32.pth',

        # optimization configs
        'epoch': 1,  
        'num_train_iter': 500,
        'num_eval_iter': 500,  
        'num_log_iter': 50,  
        'optim': 'AdamW',
        'lr': 5e-4,
        'layer_decay': 0.5,
        'batch_size': 16,
        'eval_batch_size': 16,


        # dataset configs
        'dataset': 'cifar10',
        'num_labels': 40,
        'num_classes': 10,
        'img_size': 32,
        'crop_ratio': 0.875,
        'data_dir': './data',
        'ulb_samples_per_class': None,

        # algorithm specific configs
        'hard_label': True,
        'T': 0.5,
        'ema_p': 0.999,
        'ent_loss_ratio': 0.001,
        'uratio': 2,
        'ulb_loss_ratio': 1.0,

        # device configs
        'gpu': 0,
        'world_size': 1,
        'distributed': False,
        "num_workers": 4,
    }
    config = get_config(config)









.. GENERATED FROM PYTHON SOURCE LINES 139-142

Then, we load the dataset and create data loaders for training and testing.
And we specify the model and algorithm to use.


.. GENERATED FROM PYTHON SOURCE LINES 142-149

.. code-block:: default

    dataset_dict = get_dataset(config, config.algorithm, config.dataset, config.num_labels, config.num_classes, data_dir=config.data_dir, include_lb_to_ulb=config.include_lb_to_ulb)
    train_lb_loader = get_data_loader(config, dataset_dict['train_lb'], config.batch_size)
    train_ulb_loader = get_data_loader(config, dataset_dict['train_ulb'], int(config.batch_size * config.uratio))
    eval_loader = get_data_loader(config, dataset_dict['eval'], config.eval_batch_size)
    algorithm = get_algorithm(config,  get_net_builder(config.net, from_name=False), tb_log=None, logger=None)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar10/cifar-10-python.tar.gz

      0%|          | 0/170498071 [00:00<?, ?it/s]
      0%|          | 720896/170498071 [00:00<00:23, 7195208.78it/s]
      5%|4         | 8028160/170498071 [00:00<00:03, 45861564.92it/s]
     11%|#         | 18087936/170498071 [00:00<00:02, 70654109.34it/s]
     16%|#6        | 28114944/170498071 [00:00<00:01, 82249551.11it/s]
     22%|##2       | 38043648/170498071 [00:00<00:01, 88373933.47it/s]
     28%|##8       | 48070656/170498071 [00:00<00:01, 92374700.49it/s]
     34%|###4      | 58064896/170498071 [00:00<00:01, 94760008.75it/s]
     40%|###9      | 67633152/170498071 [00:00<00:01, 94944798.72it/s]
     45%|####5     | 77135872/170498071 [00:00<00:00, 94415169.68it/s]
     51%|#####     | 86605824/170498071 [00:01<00:00, 93216070.55it/s]
     56%|#####6    | 95944704/170498071 [00:01<00:00, 90992349.41it/s]
     62%|######1   | 105086976/170498071 [00:01<00:00, 89605498.57it/s]
     67%|######7   | 114262016/170498071 [00:01<00:00, 90223207.54it/s]
     72%|#######2  | 123305984/170498071 [00:01<00:00, 88857643.16it/s]
     78%|#######8  | 133005312/170498071 [00:01<00:00, 91244698.17it/s]
     83%|########3 | 142147584/170498071 [00:01<00:00, 91120209.04it/s]
     89%|########8 | 151289856/170498071 [00:01<00:00, 89990800.33it/s]
     94%|#########4| 160989184/170498071 [00:01<00:00, 91857544.34it/s]
    100%|#########9| 170196992/170498071 [00:01<00:00, 89630756.32it/s]
    100%|##########| 170498071/170498071 [00:01<00:00, 87537642.93it/s]
    Extracting ./data/cifar10/cifar-10-python.tar.gz to ./data/cifar10
    lb count: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4]
    ulb count: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]
    Files already downloaded and verified
    Files already downloaded and verified
    lb count: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4]
    ulb count: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]
    Files already downloaded and verified
    unlabeled data number: 50000, labeled data number 40
    Create train and test data loaders
    [!] data loader keys: dict_keys(['train_lb', 'train_ulb', 'eval'])
    Downloading: "https://github.com/microsoft/Semi-supervised-learning/releases/download/v.0.0.0/vit_tiny_patch2_32_mlp_im_1k_32.pth" to /root/.cache/torch/hub/checkpoints/vit_tiny_patch2_32_mlp_im_1k_32.pth

      0%|          | 0.00/115M [00:00<?, ?B/s]
     23%|##2       | 26.1M/115M [00:00<00:00, 274MB/s]
     47%|####7     | 54.3M/115M [00:00<00:00, 287MB/s]
     72%|#######2  | 82.7M/115M [00:00<00:00, 292MB/s]
     97%|#########6| 111M/115M [00:00<00:00, 293MB/s] 
    100%|##########| 115M/115M [00:00<00:00, 291MB/s]
    _IncompatibleKeys(missing_keys=['head.weight', 'head.bias'], unexpected_keys=[])
    Create optimizer and scheduler




.. GENERATED FROM PYTHON SOURCE LINES 150-153

We can start training the algorithms on CIFAR-10 with 40 labels now.
We train for 500 iterations and evaluate every 500 iterations.


.. GENERATED FROM PYTHON SOURCE LINES 153-157

.. code-block:: default

    trainer = Trainer(config, algorithm)
    trainer.fit(train_lb_loader, train_ulb_loader, eval_loader)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Epoch: 0
    50 iteration USE_EMA: True, train/sup_loss: 2.3529, train/unsup_loss: 0.5777, train/total_loss: 2.9054, train/util_ratio: 1.0000, train/run_time: 0.5818, lr: 0.0005, train/prefetch_time: 0.0041 
    100 iteration USE_EMA: True, train/sup_loss: 1.7521, train/unsup_loss: 0.4623, train/total_loss: 2.1889, train/util_ratio: 1.0000, train/run_time: 0.5825, lr: 0.0005, train/prefetch_time: 0.0040 
    150 iteration USE_EMA: True, train/sup_loss: 0.9473, train/unsup_loss: 1.0113, train/total_loss: 1.9409, train/util_ratio: 1.0000, train/run_time: 0.5818, lr: 0.0005, train/prefetch_time: 0.0043 
    200 iteration USE_EMA: True, train/sup_loss: 0.3124, train/unsup_loss: 0.6291, train/total_loss: 0.9217, train/util_ratio: 1.0000, train/run_time: 0.5816, lr: 0.0004, train/prefetch_time: 0.0036 
    250 iteration USE_EMA: True, train/sup_loss: 0.1295, train/unsup_loss: 0.8346, train/total_loss: 0.9567, train/util_ratio: 1.0000, train/run_time: 0.5818, lr: 0.0004, train/prefetch_time: 0.0039 
    300 iteration USE_EMA: True, train/sup_loss: 0.0960, train/unsup_loss: 0.7705, train/total_loss: 0.8565, train/util_ratio: 1.0000, train/run_time: 0.5815, lr: 0.0003, train/prefetch_time: 0.0036 
    350 iteration USE_EMA: True, train/sup_loss: 0.1819, train/unsup_loss: 0.8313, train/total_loss: 1.0031, train/util_ratio: 1.0000, train/run_time: 0.5827, lr: 0.0003, train/prefetch_time: 0.0039 
    400 iteration USE_EMA: True, train/sup_loss: 0.0499, train/unsup_loss: 0.5025, train/total_loss: 0.5422, train/util_ratio: 1.0000, train/run_time: 0.5827, lr: 0.0002, train/prefetch_time: 0.0036 
    450 iteration USE_EMA: True, train/sup_loss: 0.0783, train/unsup_loss: 0.7793, train/total_loss: 0.8474, train/util_ratio: 1.0000, train/run_time: 0.5817, lr: 0.0002, train/prefetch_time: 0.0040 
    validating...
    /opt/conda/envs/py_3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning:

    Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.

    confusion matrix:
    [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
     [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
     [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
     [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
     [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
     [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
     [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
     [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
     [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
     [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
    model saved: ./saved_models/fixmatch/latest_model.pth
    model saved: ./saved_models/fixmatch/model_best.pth
    500 iteration, USE_EMA: True, train/sup_loss: 0.0213, train/unsup_loss: 0.9055, train/total_loss: 0.9165, train/util_ratio: 0.9688, train/run_time: 0.5824, eval/loss: 5.0303, eval/top-1-acc: 0.1005, eval/balanced_acc: 0.1000, eval/precision: 0.0100, eval/recall: 0.1000, eval/F1: 0.0183, lr: 0.0001, train/prefetch_time: 0.0033 BEST_EVAL_ACC: 0.1005, at 500 iters
    [2024-03-27 22:04:27,775 INFO] confusion matrix
    [2024-03-27 22:04:27,775 INFO] [[0.95149254 0.00621891 0.         0.         0.00124378 0.
      0.00248756 0.         0.01243781 0.0261194 ]
     [0.00123916 0.98141264 0.         0.         0.         0.
      0.         0.         0.         0.0173482 ]
     [0.05099502 0.         0.14054726 0.00124378 0.14179104 0.01616915
      0.05348259 0.5920398  0.00248756 0.00124378]
     [0.00501882 0.00250941 0.00125471 0.17691343 0.02007528 0.0476788
      0.05018821 0.68005019 0.00752823 0.00878294]
     [0.         0.         0.         0.         0.82089552 0.
      0.00497512 0.17288557 0.00124378 0.        ]
     [0.         0.         0.         0.00127226 0.00890585 0.6043257
      0.00127226 0.38295165 0.         0.00127226]
     [0.00361882 0.         0.00120627 0.         0.00120627 0.
      0.98190591 0.01085645 0.         0.00120627]
     [0.00512164 0.         0.         0.         0.03072983 0.00256082
      0.         0.95902689 0.         0.00256082]
     [0.11581569 0.05728518 0.00124533 0.         0.         0.00249066
      0.00124533 0.00124533 0.78455791 0.03611457]
     [0.00127389 0.03694268 0.         0.         0.         0.
      0.         0.         0.00127389 0.96050955]]
    [2024-03-27 22:04:27,778 INFO] evaluation metric
    [2024-03-27 22:04:27,779 INFO] acc: 0.7366
    [2024-03-27 22:04:27,779 INFO] precision: 0.8518
    [2024-03-27 22:04:27,779 INFO] recall: 0.7362
    [2024-03-27 22:04:27,779 INFO] f1: 0.7152
    model saved: ./saved_models/fixmatch/latest_model.pth
    model saved: ./saved_models/fixmatch/model_best.pth
    [2024-03-27 22:04:28,510 INFO] Best acc 0.7366 at epoch 0
    [2024-03-27 22:04:28,510 INFO] Training finished.




.. GENERATED FROM PYTHON SOURCE LINES 158-161

Finally, let's evaluate the trained model on the validation set.
After training 500 iterations with ``FreeMatch`` on only 40 labels of
CIFAR-10, we obtain a classifier that achieves around 87% accuracy on the validation set.

.. GENERATED FROM PYTHON SOURCE LINES 161-165

.. code-block:: default

    trainer.evaluate(eval_loader)







.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [2024-03-27 22:04:50,512 INFO] confusion matrix
    [2024-03-27 22:04:50,513 INFO] [[0.95149254 0.00621891 0.         0.         0.00124378 0.
      0.00248756 0.         0.01243781 0.0261194 ]
     [0.00123916 0.98141264 0.         0.         0.         0.
      0.         0.         0.         0.0173482 ]
     [0.05099502 0.         0.14054726 0.00124378 0.14179104 0.01616915
      0.05348259 0.5920398  0.00248756 0.00124378]
     [0.00501882 0.00250941 0.00125471 0.17691343 0.02007528 0.0476788
      0.05018821 0.68005019 0.00752823 0.00878294]
     [0.         0.         0.         0.         0.82089552 0.
      0.00497512 0.17288557 0.00124378 0.        ]
     [0.         0.         0.         0.00127226 0.00890585 0.6043257
      0.00127226 0.38295165 0.         0.00127226]
     [0.00361882 0.         0.00120627 0.         0.00120627 0.
      0.98190591 0.01085645 0.         0.00120627]
     [0.00512164 0.         0.         0.         0.03072983 0.00256082
      0.         0.95902689 0.         0.00256082]
     [0.11581569 0.05728518 0.00124533 0.         0.         0.00249066
      0.00124533 0.00124533 0.78455791 0.03611457]
     [0.00127389 0.03694268 0.         0.         0.         0.
      0.         0.         0.00127389 0.96050955]]
    [2024-03-27 22:04:50,516 INFO] evaluation metric
    [2024-03-27 22:04:50,516 INFO] acc: 0.7366
    [2024-03-27 22:04:50,516 INFO] precision: 0.8518
    [2024-03-27 22:04:50,516 INFO] recall: 0.7362
    [2024-03-27 22:04:50,517 INFO] f1: 0.7152

    {'acc': 0.736625, 'precision': 0.8518272669470696, 'recall': 0.7361587349204767, 'f1': 0.7152204083612528}



.. GENERATED FROM PYTHON SOURCE LINES 166-176

Use USB to Train ``SoftMatch`` with specific imbalanced algorithm on imbalanced CIFAR-10
----------------------------------------------------------------------------------------

Now let's say we have imbalanced labeled set and unlabeled set of CIFAR-10,
and we want to train a ``SoftMatch`` model on it.
We create an imbalanced labeled set and imbalanced unlabeled set of CIFAR-10,
by setting the ``lb_imb_ratio`` and ``ulb_imb_ratio`` to 10.
Also, we replace the ``algorithm`` with ``softmatch`` and set the ``imbalanced``
to ``True``.


.. GENERATED FROM PYTHON SOURCE LINES 176-222

.. code-block:: default

    config = {
        'algorithm': 'softmatch',
        'net': 'vit_tiny_patch2_32',
        'use_pretrain': True, 
        'pretrain_path': 'https://github.com/microsoft/Semi-supervised-learning/releases/download/v.0.0.0/vit_tiny_patch2_32_mlp_im_1k_32.pth',

        # optimization configs
        'epoch': 1,  
        'num_train_iter': 500,
        'num_eval_iter': 500,  
        'num_log_iter': 50,  
        'optim': 'AdamW',
        'lr': 5e-4,
        'layer_decay': 0.5,
        'batch_size': 16,
        'eval_batch_size': 16,


        # dataset configs
        'dataset': 'cifar10',
        'num_labels': 1500,
        'num_classes': 10,
        'img_size': 32,
        'crop_ratio': 0.875,
        'data_dir': './data',
        'ulb_samples_per_class': None,
        'lb_imb_ratio': 10,
        'ulb_imb_ratio': 10,
        'ulb_num_labels': 3000,

        # algorithm specific configs
        'hard_label': True,
        'T': 0.5,
        'ema_p': 0.999,
        'ent_loss_ratio': 0.001,
        'uratio': 2,
        'ulb_loss_ratio': 1.0,

        # device configs
        'gpu': 0,
        'world_size': 1,
        'distributed': False,
        "num_workers": 4,
    }
    config = get_config(config)








.. GENERATED FROM PYTHON SOURCE LINES 223-226

Then, we re-load the dataset and create data loaders for training and testing.
And we specify the model and algorithm to use.


.. GENERATED FROM PYTHON SOURCE LINES 226-233

.. code-block:: default

    dataset_dict = get_dataset(config, config.algorithm, config.dataset, config.num_labels, config.num_classes, data_dir=config.data_dir, include_lb_to_ulb=config.include_lb_to_ulb)
    train_lb_loader = get_data_loader(config, dataset_dict['train_lb'], config.batch_size)
    train_ulb_loader = get_data_loader(config, dataset_dict['train_ulb'], int(config.batch_size * config.uratio))
    eval_loader = get_data_loader(config, dataset_dict['eval'], config.eval_batch_size)
    algorithm = get_algorithm(config,  get_net_builder(config.net, from_name=False), tb_log=None, logger=None)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Files already downloaded and verified
    lb count: [1500, 1161, 899, 696, 539, 417, 323, 250, 193, 150]
    ulb count: [4500, 3483, 2697, 2088, 1617, 1251, 969, 750, 580, 450]
    Files already downloaded and verified
    Files already downloaded and verified
    lb count: [1500, 1161, 899, 696, 539, 417, 323, 250, 193, 150]
    ulb count: [4500, 3483, 2697, 2088, 1617, 1251, 969, 750, 580, 450]
    Files already downloaded and verified
    unlabeled data number: 18385, labeled data number 6128
    Create train and test data loaders
    [!] data loader keys: dict_keys(['train_lb', 'train_ulb', 'eval'])
    _IncompatibleKeys(missing_keys=['head.weight', 'head.bias'], unexpected_keys=[])
    Create optimizer and scheduler
    distribution alignment p_target: tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,
            0.1000])




.. GENERATED FROM PYTHON SOURCE LINES 234-237

We can start Train the algorithms on CIFAR-10 with 40 labels now.
We train for 500 iterations and evaluate every 500 iterations.


.. GENERATED FROM PYTHON SOURCE LINES 237-241

.. code-block:: default

    trainer = Trainer(config, algorithm)
    trainer.fit(train_lb_loader, train_ulb_loader, eval_loader)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Epoch: 0
    50 iteration USE_EMA: True, train/sup_loss: 2.5469, train/unsup_loss: 0.7099, train/total_loss: 3.2567, train/util_ratio: 1.0000, train/run_time: 0.5832, lr: 0.0005, train/prefetch_time: 0.0036 
    100 iteration USE_EMA: True, train/sup_loss: 1.7985, train/unsup_loss: 0.5797, train/total_loss: 2.3782, train/util_ratio: 1.0000, train/run_time: 0.5836, lr: 0.0005, train/prefetch_time: 0.0038 
    150 iteration USE_EMA: True, train/sup_loss: 1.1758, train/unsup_loss: 0.6415, train/total_loss: 1.8173, train/util_ratio: 1.0000, train/run_time: 0.5837, lr: 0.0005, train/prefetch_time: 0.0036 
    200 iteration USE_EMA: True, train/sup_loss: 0.8190, train/unsup_loss: 0.7390, train/total_loss: 1.5580, train/util_ratio: 1.0000, train/run_time: 0.5827, lr: 0.0004, train/prefetch_time: 0.0036 
    250 iteration USE_EMA: True, train/sup_loss: 0.7668, train/unsup_loss: 0.8981, train/total_loss: 1.6649, train/util_ratio: 1.0000, train/run_time: 0.5830, lr: 0.0004, train/prefetch_time: 0.0038 
    300 iteration USE_EMA: True, train/sup_loss: 0.5546, train/unsup_loss: 0.6713, train/total_loss: 1.2259, train/util_ratio: 1.0000, train/run_time: 0.5827, lr: 0.0003, train/prefetch_time: 0.0036 
    350 iteration USE_EMA: True, train/sup_loss: 0.4318, train/unsup_loss: 0.5933, train/total_loss: 1.0252, train/util_ratio: 1.0000, train/run_time: 0.5837, lr: 0.0003, train/prefetch_time: 0.0035 
    400 iteration USE_EMA: True, train/sup_loss: 0.2138, train/unsup_loss: 0.7006, train/total_loss: 0.9143, train/util_ratio: 1.0000, train/run_time: 0.5837, lr: 0.0002, train/prefetch_time: 0.0036 
    450 iteration USE_EMA: True, train/sup_loss: 0.2172, train/unsup_loss: 0.8016, train/total_loss: 1.0188, train/util_ratio: 0.9996, train/run_time: 0.5824, lr: 0.0002, train/prefetch_time: 0.0035 
    validating...
    /opt/conda/envs/py_3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning:

    Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.

    confusion matrix:
    [[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
     [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
     [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
     [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
     [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
     [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
     [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
     [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
     [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
     [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]
    model saved: ./saved_models/fixmatch/latest_model.pth
    model saved: ./saved_models/fixmatch/model_best.pth
    500 iteration, USE_EMA: True, train/sup_loss: 0.3218, train/unsup_loss: 0.5166, train/total_loss: 0.8384, train/util_ratio: 0.9990, train/run_time: 0.5824, eval/loss: 4.8757, eval/top-1-acc: 0.1036, eval/balanced_acc: 0.1000, eval/precision: 0.0104, eval/recall: 0.1000, eval/F1: 0.0188, lr: 0.0001, train/prefetch_time: 0.0033 BEST_EVAL_ACC: 0.1036, at 500 iters
    [2024-03-27 22:10:50,422 INFO] confusion matrix
    [2024-03-27 22:10:50,423 INFO] [[0.99502488 0.00373134 0.         0.         0.         0.
      0.         0.         0.         0.00124378]
     [0.00123916 0.99876084 0.         0.         0.         0.
      0.         0.         0.         0.        ]
     [0.02985075 0.         0.9340796  0.01492537 0.01492537 0.00124378
      0.00497512 0.         0.         0.        ]
     [0.00627353 0.00250941 0.00250941 0.9272271  0.01631117 0.03262233
      0.00878294 0.00250941 0.         0.00125471]
     [0.         0.         0.00870647 0.00995025 0.97014925 0.
      0.00621891 0.00497512 0.         0.        ]
     [0.         0.         0.00254453 0.06743003 0.01017812 0.90966921
      0.         0.01017812 0.         0.        ]
     [0.00603136 0.         0.00603136 0.00723764 0.00120627 0.
      0.97949337 0.         0.         0.        ]
     [0.00768246 0.         0.00896287 0.02816901 0.2496799  0.0524968
      0.         0.65300896 0.         0.        ]
     [0.25155666 0.09464508 0.00249066 0.         0.         0.
      0.         0.         0.64757161 0.00373599]
     [0.02547771 0.50191083 0.         0.         0.         0.
      0.         0.         0.         0.47261146]]
    [2024-03-27 22:10:50,427 INFO] evaluation metric
    [2024-03-27 22:10:50,427 INFO] acc: 0.8505
    [2024-03-27 22:10:50,427 INFO] precision: 0.8856
    [2024-03-27 22:10:50,427 INFO] recall: 0.8488
    [2024-03-27 22:10:50,428 INFO] f1: 0.8441
    model saved: ./saved_models/fixmatch/latest_model.pth
    model saved: ./saved_models/fixmatch/model_best.pth
    [2024-03-27 22:10:51,249 INFO] Best acc 0.8505 at epoch 0
    [2024-03-27 22:10:51,249 INFO] Training finished.




.. GENERATED FROM PYTHON SOURCE LINES 242-244

Finally, let's evaluate the trained model on the validation set.


.. GENERATED FROM PYTHON SOURCE LINES 244-248

.. code-block:: default

    trainer.evaluate(eval_loader)







.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [2024-03-27 22:11:13,331 INFO] confusion matrix
    [2024-03-27 22:11:13,331 INFO] [[0.99502488 0.00373134 0.         0.         0.         0.
      0.         0.         0.         0.00124378]
     [0.00123916 0.99876084 0.         0.         0.         0.
      0.         0.         0.         0.        ]
     [0.02985075 0.         0.9340796  0.01492537 0.01492537 0.00124378
      0.00497512 0.         0.         0.        ]
     [0.00627353 0.00250941 0.00250941 0.9272271  0.01631117 0.03262233
      0.00878294 0.00250941 0.         0.00125471]
     [0.         0.         0.00870647 0.00995025 0.97014925 0.
      0.00621891 0.00497512 0.         0.        ]
     [0.         0.         0.00254453 0.06743003 0.01017812 0.90966921
      0.         0.01017812 0.         0.        ]
     [0.00603136 0.         0.00603136 0.00723764 0.00120627 0.
      0.97949337 0.         0.         0.        ]
     [0.00768246 0.         0.00896287 0.02816901 0.2496799  0.0524968
      0.         0.65300896 0.         0.        ]
     [0.25155666 0.09464508 0.00249066 0.         0.         0.
      0.         0.         0.64757161 0.00373599]
     [0.02547771 0.50191083 0.         0.         0.         0.
      0.         0.         0.         0.47261146]]
    [2024-03-27 22:11:13,336 INFO] evaluation metric
    [2024-03-27 22:11:13,336 INFO] acc: 0.8505
    [2024-03-27 22:11:13,336 INFO] precision: 0.8856
    [2024-03-27 22:11:13,337 INFO] recall: 0.8488
    [2024-03-27 22:11:13,337 INFO] f1: 0.8441

    {'acc': 0.8505, 'precision': 0.8856186417668039, 'recall': 0.8487596286609934, 'f1': 0.8441275441763487}



.. GENERATED FROM PYTHON SOURCE LINES 249-254

References:
- [1] USB: https://github.com/microsoft/Semi-supervised-learning
- [2] Kihyuk Sohn et al. FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence
- [3] Yidong Wang et al. FreeMatch: Self-adaptive Thresholding for Semi-supervised Learning
- [4] Hao Chen et al. SoftMatch: Addressing the Quantity-Quality Trade-off in Semi-supervised Learning


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 12 minutes  52.190 seconds)


.. _sphx_glr_download_advanced_usb_semisup_learn.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example


    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: usb_semisup_learn.py <usb_semisup_learn.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: usb_semisup_learn.ipynb <usb_semisup_learn.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
