
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "beginner/hyperparameter_tuning_tutorial.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_beginner_hyperparameter_tuning_tutorial.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_beginner_hyperparameter_tuning_tutorial.py:


Hyperparameter tuning with Ray Tune
===================================

Hyperparameter tuning can make the difference between an average model and a highly
accurate one. Often simple things like choosing a different learning rate or changing
a network layer size can have a dramatic impact on your model performance.

Fortunately, there are tools that help with finding the best combination of parameters.
`Ray Tune <https://docs.ray.io/en/latest/tune.html>`_ is an industry standard tool for
distributed hyperparameter tuning. Ray Tune includes the latest hyperparameter search
algorithms, integrates with TensorBoard and other analysis libraries, and natively
supports distributed training through `Ray's distributed machine learning engine
<https://ray.io/>`_.

In this tutorial, we will show you how to integrate Ray Tune into your PyTorch
training workflow. We will extend `this tutorial from the PyTorch documentation
<https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html>`_ for training
a CIFAR10 image classifier.

As you will see, we only need to add some slight modifications. In particular, we
need to

1. wrap data loading and training in functions,
2. make some network parameters configurable,
3. add checkpointing (optional),
4. and define the search space for the model tuning

|

To run this tutorial, please make sure the following packages are
installed:

-  ``ray[tune]``: Distributed hyperparameter tuning library
-  ``torchvision``: For the data transformers

Setup / Imports
---------------
Let's start with the imports:

.. GENERATED FROM PYTHON SOURCE LINES 42-55

.. code-block:: default

    from functools import partial
    import os
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    import torch.optim as optim
    from torch.utils.data import random_split
    import torchvision
    import torchvision.transforms as transforms
    from ray import tune
    from ray.air import Checkpoint, session
    from ray.tune.schedulers import ASHAScheduler








.. GENERATED FROM PYTHON SOURCE LINES 56-63

Most of the imports are needed for building the PyTorch model. Only the last three
imports are for Ray Tune.

Data loaders
------------
We wrap the data loaders in their own function and pass a global data directory.
This way we can share a data directory between different trials.

.. GENERATED FROM PYTHON SOURCE LINES 63-81

.. code-block:: default



    def load_data(data_dir="./data"):
        transform = transforms.Compose(
            [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]
        )

        trainset = torchvision.datasets.CIFAR10(
            root=data_dir, train=True, download=True, transform=transform
        )

        testset = torchvision.datasets.CIFAR10(
            root=data_dir, train=False, download=True, transform=transform
        )

        return trainset, testset









.. GENERATED FROM PYTHON SOURCE LINES 82-87

Configurable neural network
---------------------------
We can only tune those parameters that are configurable.
In this example, we can specify
the layer sizes of the fully connected layers:

.. GENERATED FROM PYTHON SOURCE LINES 87-109

.. code-block:: default



    class Net(nn.Module):
        def __init__(self, l1=120, l2=84):
            super(Net, self).__init__()
            self.conv1 = nn.Conv2d(3, 6, 5)
            self.pool = nn.MaxPool2d(2, 2)
            self.conv2 = nn.Conv2d(6, 16, 5)
            self.fc1 = nn.Linear(16 * 5 * 5, l1)
            self.fc2 = nn.Linear(l1, l2)
            self.fc3 = nn.Linear(l2, 10)

        def forward(self, x):
            x = self.pool(F.relu(self.conv1(x)))
            x = self.pool(F.relu(self.conv2(x)))
            x = torch.flatten(x, 1)  # flatten all dimensions except batch
            x = F.relu(self.fc1(x))
            x = F.relu(self.fc2(x))
            x = self.fc3(x)
            return x









.. GENERATED FROM PYTHON SOURCE LINES 110-213

The train function
------------------
Now it gets interesting, because we introduce some changes to the example `from the PyTorch
documentation <https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html>`_.

We wrap the training script in a function ``train_cifar(config, data_dir=None)``.
The ``config`` parameter will receive the hyperparameters we would like to
train with. The ``data_dir`` specifies the directory where we load and store the data,
so that multiple runs can share the same data source.
We also load the model and optimizer state at the start of the run, if a checkpoint
is provided. Further down in this tutorial you will find information on how
to save the checkpoint and what it is used for.

.. code-block:: python

    net = Net(config["l1"], config["l2"])

    checkpoint = session.get_checkpoint()

    if checkpoint:
        checkpoint_state = checkpoint.to_dict()
        start_epoch = checkpoint_state["epoch"]
        net.load_state_dict(checkpoint_state["net_state_dict"])
        optimizer.load_state_dict(checkpoint_state["optimizer_state_dict"])
    else:
        start_epoch = 0

The learning rate of the optimizer is made configurable, too:

.. code-block:: python

    optimizer = optim.SGD(net.parameters(), lr=config["lr"], momentum=0.9)

We also split the training data into a training and validation subset. We thus train on
80% of the data and calculate the validation loss on the remaining 20%. The batch sizes
with which we iterate through the training and test sets are configurable as well.

Adding (multi) GPU support with DataParallel
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Image classification benefits largely from GPUs. Luckily, we can continue to use
PyTorch's abstractions in Ray Tune. Thus, we can wrap our model in ``nn.DataParallel``
to support data parallel training on multiple GPUs:

.. code-block:: python

    device = "cpu"
    if torch.cuda.is_available():
        device = "cuda:0"
        if torch.cuda.device_count() > 1:
            net = nn.DataParallel(net)
    net.to(device)

By using a ``device`` variable we make sure that training also works when we have
no GPUs available. PyTorch requires us to send our data to the GPU memory explicitly,
like this:

.. code-block:: python

    for i, data in enumerate(trainloader, 0):
        inputs, labels = data
        inputs, labels = inputs.to(device), labels.to(device)

The code now supports training on CPUs, on a single GPU, and on multiple GPUs. Notably, Ray
also supports `fractional GPUs <https://docs.ray.io/en/master/using-ray-with-gpus.html#fractional-gpus>`_
so we can share GPUs among trials, as long as the model still fits on the GPU memory. We'll come back
to that later.

Communicating with Ray Tune
~~~~~~~~~~~~~~~~~~~~~~~~~~~

The most interesting part is the communication with Ray Tune:

.. code-block:: python

    checkpoint_data = {
        "epoch": epoch,
        "net_state_dict": net.state_dict(),
        "optimizer_state_dict": optimizer.state_dict(),
    }
    checkpoint = Checkpoint.from_dict(checkpoint_data)

    session.report(
        {"loss": val_loss / val_steps, "accuracy": correct / total},
        checkpoint=checkpoint,
    )

Here we first save a checkpoint and then report some metrics back to Ray Tune. Specifically,
we send the validation loss and accuracy back to Ray Tune. Ray Tune can then use these metrics
to decide which hyperparameter configuration lead to the best results. These metrics
can also be used to stop bad performing trials early in order to avoid wasting
resources on those trials.

The checkpoint saving is optional, however, it is necessary if we wanted to use advanced
schedulers like
`Population Based Training <https://docs.ray.io/en/latest/tune/examples/pbt_guide.html>`_.
Also, by saving the checkpoint we can later load the trained models and validate them
on a test set. Lastly, saving checkpoints is useful for fault tolerance, and it allows
us to interrupt training and continue training later.

Full training function
~~~~~~~~~~~~~~~~~~~~~~

The full code example looks like this:

.. GENERATED FROM PYTHON SOURCE LINES 213-312

.. code-block:: default



    def train_cifar(config, data_dir=None):
        net = Net(config["l1"], config["l2"])

        device = "cpu"
        if torch.cuda.is_available():
            device = "cuda:0"
            if torch.cuda.device_count() > 1:
                net = nn.DataParallel(net)
        net.to(device)

        criterion = nn.CrossEntropyLoss()
        optimizer = optim.SGD(net.parameters(), lr=config["lr"], momentum=0.9)

        checkpoint = session.get_checkpoint()

        if checkpoint:
            checkpoint_state = checkpoint.to_dict()
            start_epoch = checkpoint_state["epoch"]
            net.load_state_dict(checkpoint_state["net_state_dict"])
            optimizer.load_state_dict(checkpoint_state["optimizer_state_dict"])
        else:
            start_epoch = 0

        trainset, testset = load_data(data_dir)

        test_abs = int(len(trainset) * 0.8)
        train_subset, val_subset = random_split(
            trainset, [test_abs, len(trainset) - test_abs]
        )

        trainloader = torch.utils.data.DataLoader(
            train_subset, batch_size=int(config["batch_size"]), shuffle=True, num_workers=8
        )
        valloader = torch.utils.data.DataLoader(
            val_subset, batch_size=int(config["batch_size"]), shuffle=True, num_workers=8
        )

        for epoch in range(start_epoch, 10):  # loop over the dataset multiple times
            running_loss = 0.0
            epoch_steps = 0
            for i, data in enumerate(trainloader, 0):
                # get the inputs; data is a list of [inputs, labels]
                inputs, labels = data
                inputs, labels = inputs.to(device), labels.to(device)

                # zero the parameter gradients
                optimizer.zero_grad()

                # forward + backward + optimize
                outputs = net(inputs)
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()

                # print statistics
                running_loss += loss.item()
                epoch_steps += 1
                if i % 2000 == 1999:  # print every 2000 mini-batches
                    print(
                        "[%d, %5d] loss: %.3f"
                        % (epoch + 1, i + 1, running_loss / epoch_steps)
                    )
                    running_loss = 0.0

            # Validation loss
            val_loss = 0.0
            val_steps = 0
            total = 0
            correct = 0
            for i, data in enumerate(valloader, 0):
                with torch.no_grad():
                    inputs, labels = data
                    inputs, labels = inputs.to(device), labels.to(device)

                    outputs = net(inputs)
                    _, predicted = torch.max(outputs.data, 1)
                    total += labels.size(0)
                    correct += (predicted == labels).sum().item()

                    loss = criterion(outputs, labels)
                    val_loss += loss.cpu().numpy()
                    val_steps += 1

            checkpoint_data = {
                "epoch": epoch,
                "net_state_dict": net.state_dict(),
                "optimizer_state_dict": optimizer.state_dict(),
            }
            checkpoint = Checkpoint.from_dict(checkpoint_data)

            session.report(
                {"loss": val_loss / val_steps, "accuracy": correct / total},
                checkpoint=checkpoint,
            )
        print("Finished Training")









.. GENERATED FROM PYTHON SOURCE LINES 313-320

As you can see, most of the code is adapted directly from the original example.

Test set accuracy
-----------------
Commonly the performance of a machine learning model is tested on a hold-out test
set with data that has not been used for training the model. We also wrap this in a
function:

.. GENERATED FROM PYTHON SOURCE LINES 320-343

.. code-block:: default



    def test_accuracy(net, device="cpu"):
        trainset, testset = load_data()

        testloader = torch.utils.data.DataLoader(
            testset, batch_size=4, shuffle=False, num_workers=2
        )

        correct = 0
        total = 0
        with torch.no_grad():
            for data in testloader:
                images, labels = data
                images, labels = images.to(device), labels.to(device)
                outputs = net(images)
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

        return correct / total









.. GENERATED FROM PYTHON SOURCE LINES 344-402

The function also expects a ``device`` parameter, so we can do the
test set validation on a GPU.

Configuring the search space
----------------------------
Lastly, we need to define Ray Tune's search space. Here is an example:

.. code-block:: python

    config = {
        "l1": tune.choice([2 ** i for i in range(9)]),
        "l2": tune.choice([2 ** i for i in range(9)]),
        "lr": tune.loguniform(1e-4, 1e-1),
        "batch_size": tune.choice([2, 4, 8, 16])
    }

The ``tune.choice()`` accepts a list of values that are uniformly sampled from.
In this example, the ``l1`` and ``l2`` parameters
should be powers of 2 between 4 and 256, so either 4, 8, 16, 32, 64, 128, or 256.
The ``lr`` (learning rate) should be uniformly sampled between 0.0001 and 0.1. Lastly,
the batch size is a choice between 2, 4, 8, and 16.

At each trial, Ray Tune will now randomly sample a combination of parameters from these
search spaces. It will then train a number of models in parallel and find the best
performing one among these. We also use the ``ASHAScheduler`` which will terminate bad
performing trials early.

We wrap the ``train_cifar`` function with ``functools.partial`` to set the constant
``data_dir`` parameter. We can also tell Ray Tune what resources should be
available for each trial:

.. code-block:: python

    gpus_per_trial = 2
    # ...
    result = tune.run(
        partial(train_cifar, data_dir=data_dir),
        resources_per_trial={"cpu": 8, "gpu": gpus_per_trial},
        config=config,
        num_samples=num_samples,
        scheduler=scheduler,
        checkpoint_at_end=True)

You can specify the number of CPUs, which are then available e.g.
to increase the ``num_workers`` of the PyTorch ``DataLoader`` instances. The selected
number of GPUs are made visible to PyTorch in each trial. Trials do not have access to
GPUs that haven't been requested for them - so you don't have to care about two trials
using the same set of resources.

Here we can also specify fractional GPUs, so something like ``gpus_per_trial=0.5`` is
completely valid. The trials will then share GPUs among each other.
You just have to make sure that the models still fit in the GPU memory.

After training the models, we will find the best performing one and load the trained
network from the checkpoint file. We then obtain the test set accuracy and report
everything by printing.

The full main function looks like this:

.. GENERATED FROM PYTHON SOURCE LINES 402-455

.. code-block:: default



    def main(num_samples=10, max_num_epochs=10, gpus_per_trial=2):
        data_dir = os.path.abspath("./data")
        load_data(data_dir)
        config = {
            "l1": tune.choice([2**i for i in range(9)]),
            "l2": tune.choice([2**i for i in range(9)]),
            "lr": tune.loguniform(1e-4, 1e-1),
            "batch_size": tune.choice([2, 4, 8, 16]),
        }
        scheduler = ASHAScheduler(
            metric="loss",
            mode="min",
            max_t=max_num_epochs,
            grace_period=1,
            reduction_factor=2,
        )
        result = tune.run(
            partial(train_cifar, data_dir=data_dir),
            resources_per_trial={"cpu": 2, "gpu": gpus_per_trial},
            config=config,
            num_samples=num_samples,
            scheduler=scheduler,
        )

        best_trial = result.get_best_trial("loss", "min", "last")
        print(f"Best trial config: {best_trial.config}")
        print(f"Best trial final validation loss: {best_trial.last_result['loss']}")
        print(f"Best trial final validation accuracy: {best_trial.last_result['accuracy']}")

        best_trained_model = Net(best_trial.config["l1"], best_trial.config["l2"])
        device = "cpu"
        if torch.cuda.is_available():
            device = "cuda:0"
            if gpus_per_trial > 1:
                best_trained_model = nn.DataParallel(best_trained_model)
        best_trained_model.to(device)

        best_checkpoint = best_trial.checkpoint.to_air_checkpoint()
        best_checkpoint_data = best_checkpoint.to_dict()

        best_trained_model.load_state_dict(best_checkpoint_data["net_state_dict"])

        test_acc = test_accuracy(best_trained_model, device)
        print("Best trial test set accuracy: {}".format(test_acc))


    if __name__ == "__main__":
        # You can change the number of GPUs per trial here:
        main(num_samples=10, max_num_epochs=10, gpus_per_trial=0)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /var/lib/jenkins/workspace/beginner_source/data/cifar-10-python.tar.gz

      0% 0/170498071 [00:00<?, ?it/s]
      0% 458752/170498071 [00:00<00:38, 4431258.16it/s]
      5% 7831552/170498071 [00:00<00:03, 44582922.42it/s]
     11% 19136512/170498071 [00:00<00:02, 75529262.90it/s]
     18% 30441472/170498071 [00:00<00:01, 90242236.93it/s]
     25% 41844736/170498071 [00:00<00:01, 98775518.15it/s]
     31% 53116928/170498071 [00:00<00:01, 103491874.59it/s]
     38% 64552960/170498071 [00:00<00:00, 107021539.57it/s]
     44% 75857920/170498071 [00:00<00:00, 108874301.45it/s]
     51% 86966272/170498071 [00:00<00:00, 109540583.97it/s]
     58% 98467840/170498071 [00:01<00:00, 111204747.87it/s]
     64% 109838336/170498071 [00:01<00:00, 111822476.66it/s]
     71% 121077760/170498071 [00:01<00:00, 111933682.12it/s]
     78% 132284416/170498071 [00:01<00:00, 111778739.60it/s]
     84% 143556608/170498071 [00:01<00:00, 111959085.89it/s]
     91% 155025408/170498071 [00:01<00:00, 112634419.20it/s]
     98% 166428672/170498071 [00:01<00:00, 112987879.89it/s]
    100% 170498071/170498071 [00:01<00:00, 103687932.76it/s]
    Extracting /var/lib/jenkins/workspace/beginner_source/data/cifar-10-python.tar.gz to /var/lib/jenkins/workspace/beginner_source/data
    Files already downloaded and verified
    2023-12-14 16:22:04,854 WARNING services.py:1816 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 2147479552 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=10.24gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.
    2023-12-14 16:22:04,897 INFO worker.py:1625 -- Started a local Ray instance.
    2023-12-14 16:22:05,731 INFO tune.py:218 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `tune.run(...)`.
    (pid=2713) /opt/conda/envs/py_3.10/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
    (pid=2713)   _torch_pytree._register_pytree_node(
    == Status ==
    Current time: 2023-12-14 16:22:09 (running for 00:00:04.04)
    Using AsyncHyperBand: num_stopped=0
    Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None
    Logical resource usage: 2.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (9 PENDING, 1 RUNNING)
    +-------------------------+----------+-----------------+--------------+------+------+-------------+
    | Trial name              | status   | loc             |   batch_size |   l1 |   l2 |          lr |
    |-------------------------+----------+-----------------+--------------+------+------+-------------|
    | train_cifar_ebd82_00000 | RUNNING  | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |
    | train_cifar_ebd82_00001 | PENDING  |                 |            4 |    1 |    2 | 0.013416    |
    | train_cifar_ebd82_00002 | PENDING  |                 |            2 |  256 |   64 | 0.0113784   |
    | train_cifar_ebd82_00003 | PENDING  |                 |            8 |   64 |  256 | 0.0274071   |
    | train_cifar_ebd82_00004 | PENDING  |                 |            4 |   16 |    2 | 0.056666    |
    | train_cifar_ebd82_00005 | PENDING  |                 |            4 |    8 |   64 | 0.000353097 |
    | train_cifar_ebd82_00006 | PENDING  |                 |            8 |   16 |    4 | 0.000147684 |
    | train_cifar_ebd82_00007 | PENDING  |                 |            8 |  256 |  256 | 0.00477469  |
    | train_cifar_ebd82_00008 | PENDING  |                 |            8 |  128 |  256 | 0.0306227   |
    | train_cifar_ebd82_00009 | PENDING  |                 |            2 |    2 |   16 | 0.0286986   |
    +-------------------------+----------+-----------------+--------------+------+------+-------------+


    (func pid=2713) Files already downloaded and verified
    (func pid=2713) Files already downloaded and verified
    == Status ==
    Current time: 2023-12-14 16:22:14 (running for 00:00:09.11)
    Using AsyncHyperBand: num_stopped=0
    Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None
    Logical resource usage: 16.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (2 PENDING, 8 RUNNING)
    +-------------------------+----------+-----------------+--------------+------+------+-------------+
    | Trial name              | status   | loc             |   batch_size |   l1 |   l2 |          lr |
    |-------------------------+----------+-----------------+--------------+------+------+-------------|
    | train_cifar_ebd82_00000 | RUNNING  | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |
    | train_cifar_ebd82_00001 | RUNNING  | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |
    | train_cifar_ebd82_00002 | RUNNING  | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |
    | train_cifar_ebd82_00003 | RUNNING  | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |
    | train_cifar_ebd82_00004 | RUNNING  | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |
    | train_cifar_ebd82_00005 | RUNNING  | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |
    | train_cifar_ebd82_00006 | RUNNING  | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |
    | train_cifar_ebd82_00007 | RUNNING  | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |
    | train_cifar_ebd82_00008 | PENDING  |                 |            8 |  128 |  256 | 0.0306227   |
    | train_cifar_ebd82_00009 | PENDING  |                 |            2 |    2 |   16 | 0.0286986   |
    +-------------------------+----------+-----------------+--------------+------+------+-------------+


    (func pid=2780) Files already downloaded and verified [repeated 8x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)
    (func pid=2713) [1,  2000] loss: 2.312
    == Status ==
    Current time: 2023-12-14 16:22:19 (running for 00:00:14.12)
    Using AsyncHyperBand: num_stopped=0
    Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None
    Logical resource usage: 16.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (2 PENDING, 8 RUNNING)
    +-------------------------+----------+-----------------+--------------+------+------+-------------+
    | Trial name              | status   | loc             |   batch_size |   l1 |   l2 |          lr |
    |-------------------------+----------+-----------------+--------------+------+------+-------------|
    | train_cifar_ebd82_00000 | RUNNING  | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |
    | train_cifar_ebd82_00001 | RUNNING  | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |
    | train_cifar_ebd82_00002 | RUNNING  | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |
    | train_cifar_ebd82_00003 | RUNNING  | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |
    | train_cifar_ebd82_00004 | RUNNING  | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |
    | train_cifar_ebd82_00005 | RUNNING  | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |
    | train_cifar_ebd82_00006 | RUNNING  | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |
    | train_cifar_ebd82_00007 | RUNNING  | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |
    | train_cifar_ebd82_00008 | PENDING  |                 |            8 |  128 |  256 | 0.0306227   |
    | train_cifar_ebd82_00009 | PENDING  |                 |            2 |    2 |   16 | 0.0286986   |
    +-------------------------+----------+-----------------+--------------+------+------+-------------+


    == Status ==
    Current time: 2023-12-14 16:22:24 (running for 00:00:19.13)
    Using AsyncHyperBand: num_stopped=0
    Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None
    Logical resource usage: 16.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (2 PENDING, 8 RUNNING)
    +-------------------------+----------+-----------------+--------------+------+------+-------------+
    | Trial name              | status   | loc             |   batch_size |   l1 |   l2 |          lr |
    |-------------------------+----------+-----------------+--------------+------+------+-------------|
    | train_cifar_ebd82_00000 | RUNNING  | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |
    | train_cifar_ebd82_00001 | RUNNING  | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |
    | train_cifar_ebd82_00002 | RUNNING  | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |
    | train_cifar_ebd82_00003 | RUNNING  | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |
    | train_cifar_ebd82_00004 | RUNNING  | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |
    | train_cifar_ebd82_00005 | RUNNING  | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |
    | train_cifar_ebd82_00006 | RUNNING  | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |
    | train_cifar_ebd82_00007 | RUNNING  | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |
    | train_cifar_ebd82_00008 | PENDING  |                 |            8 |  128 |  256 | 0.0306227   |
    | train_cifar_ebd82_00009 | PENDING  |                 |            2 |    2 |   16 | 0.0286986   |
    +-------------------------+----------+-----------------+--------------+------+------+-------------+


    (func pid=2796) Files already downloaded and verified [repeated 6x across cluster]
    (func pid=2778) [1,  2000] loss: 2.312
    (func pid=2780) [1,  2000] loss: 2.264
    == Status ==
    Current time: 2023-12-14 16:22:29 (running for 00:00:24.13)
    Using AsyncHyperBand: num_stopped=0
    Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None
    Logical resource usage: 16.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (2 PENDING, 8 RUNNING)
    +-------------------------+----------+-----------------+--------------+------+------+-------------+
    | Trial name              | status   | loc             |   batch_size |   l1 |   l2 |          lr |
    |-------------------------+----------+-----------------+--------------+------+------+-------------|
    | train_cifar_ebd82_00000 | RUNNING  | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |
    | train_cifar_ebd82_00001 | RUNNING  | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |
    | train_cifar_ebd82_00002 | RUNNING  | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |
    | train_cifar_ebd82_00003 | RUNNING  | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |
    | train_cifar_ebd82_00004 | RUNNING  | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |
    | train_cifar_ebd82_00005 | RUNNING  | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |
    | train_cifar_ebd82_00006 | RUNNING  | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |
    | train_cifar_ebd82_00007 | RUNNING  | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |
    | train_cifar_ebd82_00008 | PENDING  |                 |            8 |  128 |  256 | 0.0306227   |
    | train_cifar_ebd82_00009 | PENDING  |                 |            2 |    2 |   16 | 0.0286986   |
    +-------------------------+----------+-----------------+--------------+------+------+-------------+


    (func pid=2778) [1,  4000] loss: 1.155 [repeated 7x across cluster]
    == Status ==
    Current time: 2023-12-14 16:22:34 (running for 00:00:29.14)
    Using AsyncHyperBand: num_stopped=0
    Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None
    Logical resource usage: 16.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (2 PENDING, 8 RUNNING)
    +-------------------------+----------+-----------------+--------------+------+------+-------------+
    | Trial name              | status   | loc             |   batch_size |   l1 |   l2 |          lr |
    |-------------------------+----------+-----------------+--------------+------+------+-------------|
    | train_cifar_ebd82_00000 | RUNNING  | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |
    | train_cifar_ebd82_00001 | RUNNING  | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |
    | train_cifar_ebd82_00002 | RUNNING  | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |
    | train_cifar_ebd82_00003 | RUNNING  | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |
    | train_cifar_ebd82_00004 | RUNNING  | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |
    | train_cifar_ebd82_00005 | RUNNING  | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |
    | train_cifar_ebd82_00006 | RUNNING  | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |
    | train_cifar_ebd82_00007 | RUNNING  | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |
    | train_cifar_ebd82_00008 | PENDING  |                 |            8 |  128 |  256 | 0.0306227   |
    | train_cifar_ebd82_00009 | PENDING  |                 |            2 |    2 |   16 | 0.0286986   |
    +-------------------------+----------+-----------------+--------------+------+------+-------------+


    == Status ==
    Current time: 2023-12-14 16:22:39 (running for 00:00:34.15)
    Using AsyncHyperBand: num_stopped=0
    Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None
    Logical resource usage: 16.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (2 PENDING, 8 RUNNING)
    +-------------------------+----------+-----------------+--------------+------+------+-------------+
    | Trial name              | status   | loc             |   batch_size |   l1 |   l2 |          lr |
    |-------------------------+----------+-----------------+--------------+------+------+-------------|
    | train_cifar_ebd82_00000 | RUNNING  | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |
    | train_cifar_ebd82_00001 | RUNNING  | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |
    | train_cifar_ebd82_00002 | RUNNING  | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |
    | train_cifar_ebd82_00003 | RUNNING  | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |
    | train_cifar_ebd82_00004 | RUNNING  | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |
    | train_cifar_ebd82_00005 | RUNNING  | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |
    | train_cifar_ebd82_00006 | RUNNING  | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |
    | train_cifar_ebd82_00007 | RUNNING  | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |
    | train_cifar_ebd82_00008 | PENDING  |                 |            8 |  128 |  256 | 0.0306227   |
    | train_cifar_ebd82_00009 | PENDING  |                 |            2 |    2 |   16 | 0.0286986   |
    +-------------------------+----------+-----------------+--------------+------+------+-------------+


    (func pid=2798) [1,  4000] loss: 0.773 [repeated 7x across cluster]
    == Status ==
    Current time: 2023-12-14 16:22:44 (running for 00:00:39.16)
    Using AsyncHyperBand: num_stopped=0
    Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None
    Logical resource usage: 16.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (2 PENDING, 8 RUNNING)
    +-------------------------+----------+-----------------+--------------+------+------+-------------+
    | Trial name              | status   | loc             |   batch_size |   l1 |   l2 |          lr |
    |-------------------------+----------+-----------------+--------------+------+------+-------------|
    | train_cifar_ebd82_00000 | RUNNING  | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |
    | train_cifar_ebd82_00001 | RUNNING  | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |
    | train_cifar_ebd82_00002 | RUNNING  | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |
    | train_cifar_ebd82_00003 | RUNNING  | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |
    | train_cifar_ebd82_00004 | RUNNING  | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |
    | train_cifar_ebd82_00005 | RUNNING  | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |
    | train_cifar_ebd82_00006 | RUNNING  | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |
    | train_cifar_ebd82_00007 | RUNNING  | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |
    | train_cifar_ebd82_00008 | PENDING  |                 |            8 |  128 |  256 | 0.0306227   |
    | train_cifar_ebd82_00009 | PENDING  |                 |            2 |    2 |   16 | 0.0286986   |
    +-------------------------+----------+-----------------+--------------+------+------+-------------+


    Result for train_cifar_ebd82_00006:
      accuracy: 0.1381
      date: 2023-12-14_16-22-47
      done: false
      hostname: 5c54f08e0f8e
      iterations_since_restore: 1
      loss: 2.310631537818909
      node_ip: 172.17.0.2
      pid: 2796
      should_checkpoint: true
      time_since_restore: 32.63727331161499
      time_this_iter_s: 32.63727331161499
      time_total_s: 32.63727331161499
      timestamp: 1702570967
      training_iteration: 1
      trial_id: ebd82_00006
  
    Result for train_cifar_ebd82_00003:
      accuracy: 0.1758
      date: 2023-12-14_16-22-47
      done: false
      hostname: 5c54f08e0f8e
      iterations_since_restore: 1
      loss: 2.1607268846511842
      node_ip: 172.17.0.2
      pid: 2781
      should_checkpoint: true
      time_since_restore: 32.82297611236572
      time_this_iter_s: 32.82297611236572
      time_total_s: 32.82297611236572
      timestamp: 1702570967
      training_iteration: 1
      trial_id: ebd82_00003
  
    Result for train_cifar_ebd82_00007:
      accuracy: 0.4802
      date: 2023-12-14_16-22-50
      done: false
      hostname: 5c54f08e0f8e
      iterations_since_restore: 1
      loss: 1.460614619064331
      node_ip: 172.17.0.2
      pid: 2798
      should_checkpoint: true
      time_since_restore: 36.633832931518555
      time_this_iter_s: 36.633832931518555
      time_total_s: 36.633832931518555
      timestamp: 1702570970
      training_iteration: 1
      trial_id: ebd82_00007
  
    == Status ==
    Current time: 2023-12-14 16:22:50 (running for 00:00:45.20)
    Using AsyncHyperBand: num_stopped=0
    Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -2.1607268846511842
    Logical resource usage: 16.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (2 PENDING, 8 RUNNING)
    +-------------------------+----------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status   | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+----------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00000 | RUNNING  | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |        |                  |         |            |
    | train_cifar_ebd82_00001 | RUNNING  | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |        |                  |         |            |
    | train_cifar_ebd82_00002 | RUNNING  | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |        |                  |         |            |
    | train_cifar_ebd82_00003 | RUNNING  | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      1 |          32.823  | 2.16073 |     0.1758 |
    | train_cifar_ebd82_00004 | RUNNING  | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |        |                  |         |            |
    | train_cifar_ebd82_00005 | RUNNING  | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |        |                  |         |            |
    | train_cifar_ebd82_00006 | RUNNING  | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |      1 |          32.6373 | 2.31063 |     0.1381 |
    | train_cifar_ebd82_00007 | RUNNING  | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |      1 |          36.6338 | 1.46061 |     0.4802 |
    | train_cifar_ebd82_00008 | PENDING  |                 |            8 |  128 |  256 | 0.0306227   |        |                  |         |            |
    | train_cifar_ebd82_00009 | PENDING  |                 |            2 |    2 |   16 | 0.0286986   |        |                  |         |            |
    +-------------------------+----------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    (func pid=2713) [1, 10000] loss: 0.396 [repeated 6x across cluster]
    == Status ==
    Current time: 2023-12-14 16:22:56 (running for 00:00:50.21)
    Using AsyncHyperBand: num_stopped=0
    Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -2.1607268846511842
    Logical resource usage: 16.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (2 PENDING, 8 RUNNING)
    +-------------------------+----------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status   | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+----------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00000 | RUNNING  | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |        |                  |         |            |
    | train_cifar_ebd82_00001 | RUNNING  | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |        |                  |         |            |
    | train_cifar_ebd82_00002 | RUNNING  | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |        |                  |         |            |
    | train_cifar_ebd82_00003 | RUNNING  | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      1 |          32.823  | 2.16073 |     0.1758 |
    | train_cifar_ebd82_00004 | RUNNING  | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |        |                  |         |            |
    | train_cifar_ebd82_00005 | RUNNING  | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |        |                  |         |            |
    | train_cifar_ebd82_00006 | RUNNING  | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |      1 |          32.6373 | 2.31063 |     0.1381 |
    | train_cifar_ebd82_00007 | RUNNING  | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |      1 |          36.6338 | 1.46061 |     0.4802 |
    | train_cifar_ebd82_00008 | PENDING  |                 |            8 |  128 |  256 | 0.0306227   |        |                  |         |            |
    | train_cifar_ebd82_00009 | PENDING  |                 |            2 |    2 |   16 | 0.0286986   |        |                  |         |            |
    +-------------------------+----------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    (func pid=2796) [2,  2000] loss: 2.307 [repeated 5x across cluster]
    == Status ==
    Current time: 2023-12-14 16:23:01 (running for 00:00:55.22)
    Using AsyncHyperBand: num_stopped=0
    Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -2.1607268846511842
    Logical resource usage: 16.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (2 PENDING, 8 RUNNING)
    +-------------------------+----------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status   | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+----------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00000 | RUNNING  | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |        |                  |         |            |
    | train_cifar_ebd82_00001 | RUNNING  | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |        |                  |         |            |
    | train_cifar_ebd82_00002 | RUNNING  | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |        |                  |         |            |
    | train_cifar_ebd82_00003 | RUNNING  | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      1 |          32.823  | 2.16073 |     0.1758 |
    | train_cifar_ebd82_00004 | RUNNING  | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |        |                  |         |            |
    | train_cifar_ebd82_00005 | RUNNING  | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |        |                  |         |            |
    | train_cifar_ebd82_00006 | RUNNING  | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |      1 |          32.6373 | 2.31063 |     0.1381 |
    | train_cifar_ebd82_00007 | RUNNING  | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |      1 |          36.6338 | 1.46061 |     0.4802 |
    | train_cifar_ebd82_00008 | PENDING  |                 |            8 |  128 |  256 | 0.0306227   |        |                  |         |            |
    | train_cifar_ebd82_00009 | PENDING  |                 |            2 |    2 |   16 | 0.0286986   |        |                  |         |            |
    +-------------------------+----------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    (func pid=2798) [2,  2000] loss: 1.371 [repeated 7x across cluster]
    == Status ==
    Current time: 2023-12-14 16:23:06 (running for 00:01:00.23)
    Using AsyncHyperBand: num_stopped=0
    Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -2.1607268846511842
    Logical resource usage: 16.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (2 PENDING, 8 RUNNING)
    +-------------------------+----------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status   | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+----------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00000 | RUNNING  | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |        |                  |         |            |
    | train_cifar_ebd82_00001 | RUNNING  | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |        |                  |         |            |
    | train_cifar_ebd82_00002 | RUNNING  | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |        |                  |         |            |
    | train_cifar_ebd82_00003 | RUNNING  | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      1 |          32.823  | 2.16073 |     0.1758 |
    | train_cifar_ebd82_00004 | RUNNING  | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |        |                  |         |            |
    | train_cifar_ebd82_00005 | RUNNING  | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |        |                  |         |            |
    | train_cifar_ebd82_00006 | RUNNING  | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |      1 |          32.6373 | 2.31063 |     0.1381 |
    | train_cifar_ebd82_00007 | RUNNING  | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |      1 |          36.6338 | 1.46061 |     0.4802 |
    | train_cifar_ebd82_00008 | PENDING  |                 |            8 |  128 |  256 | 0.0306227   |        |                  |         |            |
    | train_cifar_ebd82_00009 | PENDING  |                 |            2 |    2 |   16 | 0.0286986   |        |                  |         |            |
    +-------------------------+----------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    (func pid=2781) [2,  4000] loss: 1.064 [repeated 3x across cluster]
    Result for train_cifar_ebd82_00004:
      accuracy: 0.1015
      date: 2023-12-14_16-23-09
      done: true
      hostname: 5c54f08e0f8e
      iterations_since_restore: 1
      loss: 2.337112897205353
      node_ip: 172.17.0.2
      pid: 2786
      should_checkpoint: true
      time_since_restore: 54.92345643043518
      time_this_iter_s: 54.92345643043518
      time_total_s: 54.92345643043518
      timestamp: 1702570989
      training_iteration: 1
      trial_id: ebd82_00004
  
    Trial train_cifar_ebd82_00004 completed.
    Result for train_cifar_ebd82_00001:
      accuracy: 0.101
      date: 2023-12-14_16-23-09
      done: true
      hostname: 5c54f08e0f8e
      iterations_since_restore: 1
      loss: 2.308739793109894
      node_ip: 172.17.0.2
      pid: 2778
      should_checkpoint: true
      time_since_restore: 55.09592270851135
      time_this_iter_s: 55.09592270851135
      time_total_s: 55.09592270851135
      timestamp: 1702570989
      training_iteration: 1
      trial_id: ebd82_00001
  
    Trial train_cifar_ebd82_00001 completed.
    (func pid=2778) Files already downloaded and verified
    Result for train_cifar_ebd82_00005:
      accuracy: 0.3488
      date: 2023-12-14_16-23-10
      done: false
      hostname: 5c54f08e0f8e
      iterations_since_restore: 1
      loss: 1.7458032784700395
      node_ip: 172.17.0.2
      pid: 2794
      should_checkpoint: true
      time_since_restore: 55.36774778366089
      time_this_iter_s: 55.36774778366089
      time_total_s: 55.36774778366089
      timestamp: 1702570990
      training_iteration: 1
      trial_id: ebd82_00005
  
    == Status ==
    Current time: 2023-12-14 16:23:15 (running for 00:01:09.27)
    Using AsyncHyperBand: num_stopped=2
    Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -2.234733338880539
    Logical resource usage: 16.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (8 RUNNING, 2 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00000 | RUNNING    | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |        |                  |         |            |
    | train_cifar_ebd82_00002 | RUNNING    | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |        |                  |         |            |
    | train_cifar_ebd82_00003 | RUNNING    | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      1 |          32.823  | 2.16073 |     0.1758 |
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      1 |          55.3677 | 1.7458  |     0.3488 |
    | train_cifar_ebd82_00006 | RUNNING    | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |      1 |          32.6373 | 2.31063 |     0.1381 |
    | train_cifar_ebd82_00007 | RUNNING    | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |      1 |          36.6338 | 1.46061 |     0.4802 |
    | train_cifar_ebd82_00008 | RUNNING    | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |        |                  |         |            |
    | train_cifar_ebd82_00009 | RUNNING    | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |        |                  |         |            |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    (func pid=2798) [2,  4000] loss: 0.665 [repeated 2x across cluster]
    (func pid=2786) Files already downloaded and verified [repeated 3x across cluster]
    Result for train_cifar_ebd82_00006:
      accuracy: 0.1456
      date: 2023-12-14_16-23-18
      done: false
      hostname: 5c54f08e0f8e
      iterations_since_restore: 2
      loss: 2.282326633262634
      node_ip: 172.17.0.2
      pid: 2796
      should_checkpoint: true
      time_since_restore: 63.65555667877197
      time_this_iter_s: 31.018283367156982
      time_total_s: 63.65555667877197
      timestamp: 1702570998
      training_iteration: 2
      trial_id: ebd82_00006
  
    Result for train_cifar_ebd82_00003:
      accuracy: 0.2055
      date: 2023-12-14_16-23-18
      done: false
      hostname: 5c54f08e0f8e
      iterations_since_restore: 2
      loss: 2.127709931564331
      node_ip: 172.17.0.2
      pid: 2781
      should_checkpoint: true
      time_since_restore: 64.08999395370483
      time_this_iter_s: 31.26701784133911
      time_total_s: 64.08999395370483
      timestamp: 1702570998
      training_iteration: 2
      trial_id: ebd82_00003
  
    (func pid=2794) [2,  2000] loss: 1.717 [repeated 4x across cluster]
    == Status ==
    Current time: 2023-12-14 16:23:23 (running for 00:01:17.86)
    Using AsyncHyperBand: num_stopped=2
    Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: -2.2050182824134827 | Iter 1.000: -2.234733338880539
    Logical resource usage: 16.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (8 RUNNING, 2 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00000 | RUNNING    | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |        |                  |         |            |
    | train_cifar_ebd82_00002 | RUNNING    | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |        |                  |         |            |
    | train_cifar_ebd82_00003 | RUNNING    | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      2 |          64.09   | 2.12771 |     0.2055 |
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      1 |          55.3677 | 1.7458  |     0.3488 |
    | train_cifar_ebd82_00006 | RUNNING    | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |      2 |          63.6556 | 2.28233 |     0.1456 |
    | train_cifar_ebd82_00007 | RUNNING    | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |      1 |          36.6338 | 1.46061 |     0.4802 |
    | train_cifar_ebd82_00008 | RUNNING    | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |        |                  |         |            |
    | train_cifar_ebd82_00009 | RUNNING    | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |        |                  |         |            |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    (func pid=2713) [1, 18000] loss: 0.214 [repeated 2x across cluster]
    Result for train_cifar_ebd82_00007:
      accuracy: 0.5405
      date: 2023-12-14_16-23-25
      done: false
      hostname: 5c54f08e0f8e
      iterations_since_restore: 2
      loss: 1.2973753428459167
      node_ip: 172.17.0.2
      pid: 2798
      should_checkpoint: true
      time_since_restore: 71.39781165122986
      time_this_iter_s: 34.763978719711304
      time_total_s: 71.39781165122986
      timestamp: 1702571005
      training_iteration: 2
      trial_id: ebd82_00007
  
    == Status ==
    Current time: 2023-12-14 16:23:30 (running for 00:01:24.97)
    Using AsyncHyperBand: num_stopped=2
    Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: -2.127709931564331 | Iter 1.000: -2.234733338880539
    Logical resource usage: 16.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (8 RUNNING, 2 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00000 | RUNNING    | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |        |                  |         |            |
    | train_cifar_ebd82_00002 | RUNNING    | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |        |                  |         |            |
    | train_cifar_ebd82_00003 | RUNNING    | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      2 |          64.09   | 2.12771 |     0.2055 |
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      1 |          55.3677 | 1.7458  |     0.3488 |
    | train_cifar_ebd82_00006 | RUNNING    | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |      2 |          63.6556 | 2.28233 |     0.1456 |
    | train_cifar_ebd82_00007 | RUNNING    | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |      2 |          71.3978 | 1.29738 |     0.5405 |
    | train_cifar_ebd82_00008 | RUNNING    | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |        |                  |         |            |
    | train_cifar_ebd82_00009 | RUNNING    | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |        |                  |         |            |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    (func pid=2713) [1, 20000] loss: 0.196 [repeated 6x across cluster]
    == Status ==
    Current time: 2023-12-14 16:23:35 (running for 00:01:29.98)
    Using AsyncHyperBand: num_stopped=2
    Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: -2.127709931564331 | Iter 1.000: -2.234733338880539
    Logical resource usage: 16.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (8 RUNNING, 2 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00000 | RUNNING    | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |        |                  |         |            |
    | train_cifar_ebd82_00002 | RUNNING    | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |        |                  |         |            |
    | train_cifar_ebd82_00003 | RUNNING    | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      2 |          64.09   | 2.12771 |     0.2055 |
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      1 |          55.3677 | 1.7458  |     0.3488 |
    | train_cifar_ebd82_00006 | RUNNING    | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |      2 |          63.6556 | 2.28233 |     0.1456 |
    | train_cifar_ebd82_00007 | RUNNING    | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |      2 |          71.3978 | 1.29738 |     0.5405 |
    | train_cifar_ebd82_00008 | RUNNING    | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |        |                  |         |            |
    | train_cifar_ebd82_00009 | RUNNING    | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |        |                  |         |            |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    (func pid=2796) [3,  4000] loss: 1.102 [repeated 6x across cluster]
    == Status ==
    Current time: 2023-12-14 16:23:40 (running for 00:01:34.99)
    Using AsyncHyperBand: num_stopped=2
    Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: -2.127709931564331 | Iter 1.000: -2.234733338880539
    Logical resource usage: 16.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (8 RUNNING, 2 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00000 | RUNNING    | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |        |                  |         |            |
    | train_cifar_ebd82_00002 | RUNNING    | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |        |                  |         |            |
    | train_cifar_ebd82_00003 | RUNNING    | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      2 |          64.09   | 2.12771 |     0.2055 |
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      1 |          55.3677 | 1.7458  |     0.3488 |
    | train_cifar_ebd82_00006 | RUNNING    | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |      2 |          63.6556 | 2.28233 |     0.1456 |
    | train_cifar_ebd82_00007 | RUNNING    | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |      2 |          71.3978 | 1.29738 |     0.5405 |
    | train_cifar_ebd82_00008 | RUNNING    | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |        |                  |         |            |
    | train_cifar_ebd82_00009 | RUNNING    | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |        |                  |         |            |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    Result for train_cifar_ebd82_00008:
      accuracy: 0.2204
      date: 2023-12-14_16-23-43
      done: false
      hostname: 5c54f08e0f8e
      iterations_since_restore: 1
      loss: 2.077128022480011
      node_ip: 172.17.0.2
      pid: 2786
      should_checkpoint: true
      time_since_restore: 33.98687815666199
      time_this_iter_s: 33.98687815666199
      time_total_s: 33.98687815666199
      timestamp: 1702571023
      training_iteration: 1
      trial_id: ebd82_00008
  
    (func pid=2778) [1,  8000] loss: 0.584 [repeated 2x across cluster]
    == Status ==
    Current time: 2023-12-14 16:23:48 (running for 00:01:42.68)
    Using AsyncHyperBand: num_stopped=2
    Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: -2.127709931564331 | Iter 1.000: -2.1607268846511842
    Logical resource usage: 16.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (8 RUNNING, 2 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00000 | RUNNING    | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |        |                  |         |            |
    | train_cifar_ebd82_00002 | RUNNING    | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |        |                  |         |            |
    | train_cifar_ebd82_00003 | RUNNING    | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      2 |          64.09   | 2.12771 |     0.2055 |
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      1 |          55.3677 | 1.7458  |     0.3488 |
    | train_cifar_ebd82_00006 | RUNNING    | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |      2 |          63.6556 | 2.28233 |     0.1456 |
    | train_cifar_ebd82_00007 | RUNNING    | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |      2 |          71.3978 | 1.29738 |     0.5405 |
    | train_cifar_ebd82_00008 | RUNNING    | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |      1 |          33.9869 | 2.07713 |     0.2204 |
    | train_cifar_ebd82_00009 | RUNNING    | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |        |                  |         |            |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    Result for train_cifar_ebd82_00000:
      accuracy: 0.1941
      date: 2023-12-14_16-23-48
      done: false
      hostname: 5c54f08e0f8e
      iterations_since_restore: 1
      loss: 1.9196418261528014
      node_ip: 172.17.0.2
      pid: 2713
      should_checkpoint: true
      time_since_restore: 98.94916844367981
      time_this_iter_s: 98.94916844367981
      time_total_s: 98.94916844367981
      timestamp: 1702571028
      training_iteration: 1
      trial_id: ebd82_00000
  
    Result for train_cifar_ebd82_00006:
      accuracy: 0.1763
      date: 2023-12-14_16-23-49
      done: false
      hostname: 5c54f08e0f8e
      iterations_since_restore: 3
      loss: 2.108349656009674
      node_ip: 172.17.0.2
      pid: 2796
      should_checkpoint: true
      time_since_restore: 95.18456482887268
      time_this_iter_s: 31.529008150100708
      time_total_s: 95.18456482887268
      timestamp: 1702571029
      training_iteration: 3
      trial_id: ebd82_00006
  
    (func pid=2798) [3,  4000] loss: 0.611 [repeated 3x across cluster]
    Result for train_cifar_ebd82_00003:
      accuracy: 0.1758
      date: 2023-12-14_16-23-50
      done: false
      hostname: 5c54f08e0f8e
      iterations_since_restore: 3
      loss: 2.119562737083435
      node_ip: 172.17.0.2
      pid: 2781
      should_checkpoint: true
      time_since_restore: 95.52996921539307
      time_this_iter_s: 31.439975261688232
      time_total_s: 95.52996921539307
      timestamp: 1702571030
      training_iteration: 3
      trial_id: ebd82_00003
  
    == Status ==
    Current time: 2023-12-14 16:23:55 (running for 00:01:49.30)
    Using AsyncHyperBand: num_stopped=2
    Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: -2.127709931564331 | Iter 1.000: -2.1189274535655978
    Logical resource usage: 16.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (8 RUNNING, 2 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00000 | RUNNING    | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |      1 |          98.9492 | 1.91964 |     0.1941 |
    | train_cifar_ebd82_00002 | RUNNING    | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |        |                  |         |            |
    | train_cifar_ebd82_00003 | RUNNING    | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      3 |          95.53   | 2.11956 |     0.1758 |
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      1 |          55.3677 | 1.7458  |     0.3488 |
    | train_cifar_ebd82_00006 | RUNNING    | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |      3 |          95.1846 | 2.10835 |     0.1763 |
    | train_cifar_ebd82_00007 | RUNNING    | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |      2 |          71.3978 | 1.29738 |     0.5405 |
    | train_cifar_ebd82_00008 | RUNNING    | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |      1 |          33.9869 | 2.07713 |     0.2204 |
    | train_cifar_ebd82_00009 | RUNNING    | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |        |                  |         |            |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    (func pid=2786) [2,  2000] loss: 2.148 [repeated 2x across cluster]
    == Status ==
    Current time: 2023-12-14 16:24:00 (running for 00:01:54.31)
    Using AsyncHyperBand: num_stopped=2
    Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: -2.127709931564331 | Iter 1.000: -2.1189274535655978
    Logical resource usage: 16.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (8 RUNNING, 2 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00000 | RUNNING    | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |      1 |          98.9492 | 1.91964 |     0.1941 |
    | train_cifar_ebd82_00002 | RUNNING    | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |        |                  |         |            |
    | train_cifar_ebd82_00003 | RUNNING    | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      3 |          95.53   | 2.11956 |     0.1758 |
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      1 |          55.3677 | 1.7458  |     0.3488 |
    | train_cifar_ebd82_00006 | RUNNING    | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |      3 |          95.1846 | 2.10835 |     0.1763 |
    | train_cifar_ebd82_00007 | RUNNING    | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |      2 |          71.3978 | 1.29738 |     0.5405 |
    | train_cifar_ebd82_00008 | RUNNING    | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |      1 |          33.9869 | 2.07713 |     0.2204 |
    | train_cifar_ebd82_00009 | RUNNING    | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |        |                  |         |            |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    Result for train_cifar_ebd82_00007:
      accuracy: 0.5506
      date: 2023-12-14_16-24-00
      done: false
      hostname: 5c54f08e0f8e
      iterations_since_restore: 3
      loss: 1.2689746131896973
      node_ip: 172.17.0.2
      pid: 2798
      should_checkpoint: true
      time_since_restore: 106.13738989830017
      time_this_iter_s: 34.73957824707031
      time_total_s: 106.13738989830017
      timestamp: 1702571040
      training_iteration: 3
      trial_id: ebd82_00007
  
    (func pid=2796) [4,  2000] loss: 2.075 [repeated 3x across cluster]
    Result for train_cifar_ebd82_00002:
      accuracy: 0.1013
      date: 2023-12-14_16-24-00
      done: true
      hostname: 5c54f08e0f8e
      iterations_since_restore: 1
      loss: 2.313855174303055
      node_ip: 172.17.0.2
      pid: 2780
      should_checkpoint: true
      time_since_restore: 106.25660991668701
      time_this_iter_s: 106.25660991668701
      time_total_s: 106.25660991668701
      timestamp: 1702571040
      training_iteration: 1
      trial_id: ebd82_00002
  
    Trial train_cifar_ebd82_00002 completed.
    Result for train_cifar_ebd82_00005:
      accuracy: 0.4517
      date: 2023-12-14_16-24-04
      done: false
      hostname: 5c54f08e0f8e
      iterations_since_restore: 2
      loss: 1.491031481575966
      node_ip: 172.17.0.2
      pid: 2794
      should_checkpoint: true
      time_since_restore: 109.48353743553162
      time_this_iter_s: 54.11578965187073
      time_total_s: 109.48353743553162
      timestamp: 1702571044
      training_iteration: 2
      trial_id: ebd82_00005
  
    (func pid=2713) [2,  4000] loss: 0.963 [repeated 3x across cluster]
    == Status ==
    Current time: 2023-12-14 16:24:09 (running for 00:02:03.37)
    Using AsyncHyperBand: num_stopped=3
    Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: -1.8093707065701485 | Iter 1.000: -2.1607268846511842
    Logical resource usage: 14.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (7 RUNNING, 3 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00000 | RUNNING    | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |      1 |          98.9492 | 1.91964 |     0.1941 |
    | train_cifar_ebd82_00003 | RUNNING    | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      3 |          95.53   | 2.11956 |     0.1758 |
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      2 |         109.484  | 1.49103 |     0.4517 |
    | train_cifar_ebd82_00006 | RUNNING    | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |      3 |          95.1846 | 2.10835 |     0.1763 |
    | train_cifar_ebd82_00007 | RUNNING    | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |      3 |         106.137  | 1.26897 |     0.5506 |
    | train_cifar_ebd82_00008 | RUNNING    | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |      1 |          33.9869 | 2.07713 |     0.2204 |
    | train_cifar_ebd82_00009 | RUNNING    | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |        |                  |         |            |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00002 | TERMINATED | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |      1 |         106.257  | 2.31386 |     0.1013 |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    (func pid=2781) [4,  4000] loss: 1.077 [repeated 4x across cluster]
    == Status ==
    Current time: 2023-12-14 16:24:14 (running for 00:02:08.39)
    Using AsyncHyperBand: num_stopped=3
    Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: -1.8093707065701485 | Iter 1.000: -2.1607268846511842
    Logical resource usage: 14.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (7 RUNNING, 3 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00000 | RUNNING    | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |      1 |          98.9492 | 1.91964 |     0.1941 |
    | train_cifar_ebd82_00003 | RUNNING    | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      3 |          95.53   | 2.11956 |     0.1758 |
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      2 |         109.484  | 1.49103 |     0.4517 |
    | train_cifar_ebd82_00006 | RUNNING    | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |      3 |          95.1846 | 2.10835 |     0.1763 |
    | train_cifar_ebd82_00007 | RUNNING    | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |      3 |         106.137  | 1.26897 |     0.5506 |
    | train_cifar_ebd82_00008 | RUNNING    | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |      1 |          33.9869 | 2.07713 |     0.2204 |
    | train_cifar_ebd82_00009 | RUNNING    | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |        |                  |         |            |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00002 | TERMINATED | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |      1 |         106.257  | 2.31386 |     0.1013 |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    Result for train_cifar_ebd82_00008:
      accuracy: 0.199
      date: 2023-12-14_16-24-15
      done: true
      hostname: 5c54f08e0f8e
      iterations_since_restore: 2
      loss: 2.0581000661849975
      node_ip: 172.17.0.2
      pid: 2786
      should_checkpoint: true
      time_since_restore: 66.38613247871399
      time_this_iter_s: 32.399254322052
      time_total_s: 66.38613247871399
      timestamp: 1702571055
      training_iteration: 2
      trial_id: ebd82_00008
  
    Trial train_cifar_ebd82_00008 completed.
    (func pid=2778) [1, 16000] loss: 0.292 [repeated 4x across cluster]
    Result for train_cifar_ebd82_00006:
      accuracy: 0.2248
      date: 2023-12-14_16-24-19
      done: false
      hostname: 5c54f08e0f8e
      iterations_since_restore: 4
      loss: 1.9316639205932618
      node_ip: 172.17.0.2
      pid: 2796
      should_checkpoint: true
      time_since_restore: 124.96366477012634
      time_this_iter_s: 29.779099941253662
      time_total_s: 124.96366477012634
      timestamp: 1702571059
      training_iteration: 4
      trial_id: ebd82_00006
  
    == Status ==
    Current time: 2023-12-14 16:24:19 (running for 00:02:13.60)
    Using AsyncHyperBand: num_stopped=4
    Bracket: Iter 8.000: None | Iter 4.000: -1.9316639205932618 | Iter 2.000: -2.0581000661849975 | Iter 1.000: -2.1607268846511842
    Logical resource usage: 12.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (6 RUNNING, 4 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00000 | RUNNING    | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |      1 |          98.9492 | 1.91964 |     0.1941 |
    | train_cifar_ebd82_00003 | RUNNING    | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      3 |          95.53   | 2.11956 |     0.1758 |
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      2 |         109.484  | 1.49103 |     0.4517 |
    | train_cifar_ebd82_00006 | RUNNING    | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |      4 |         124.964  | 1.93166 |     0.2248 |
    | train_cifar_ebd82_00007 | RUNNING    | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |      3 |         106.137  | 1.26897 |     0.5506 |
    | train_cifar_ebd82_00009 | RUNNING    | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |        |                  |         |            |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00002 | TERMINATED | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |      1 |         106.257  | 2.31386 |     0.1013 |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    | train_cifar_ebd82_00008 | TERMINATED | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |      2 |          66.3861 | 2.0581  |     0.199  |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    Result for train_cifar_ebd82_00003:
      accuracy: 0.1661
      date: 2023-12-14_16-24-19
      done: true
      hostname: 5c54f08e0f8e
      iterations_since_restore: 4
      loss: 2.1302012906074523
      node_ip: 172.17.0.2
      pid: 2781
      should_checkpoint: true
      time_since_restore: 125.33101320266724
      time_this_iter_s: 29.80104398727417
      time_total_s: 125.33101320266724
      timestamp: 1702571059
      training_iteration: 4
      trial_id: ebd82_00003
  
    Trial train_cifar_ebd82_00003 completed.
    (func pid=2778) [1, 18000] loss: 0.259 [repeated 4x across cluster]
    == Status ==
    Current time: 2023-12-14 16:24:24 (running for 00:02:19.11)
    Using AsyncHyperBand: num_stopped=5
    Bracket: Iter 8.000: None | Iter 4.000: -2.030932605600357 | Iter 2.000: -2.0581000661849975 | Iter 1.000: -2.1607268846511842
    Logical resource usage: 10.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (5 RUNNING, 5 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00000 | RUNNING    | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |      1 |          98.9492 | 1.91964 |     0.1941 |
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      2 |         109.484  | 1.49103 |     0.4517 |
    | train_cifar_ebd82_00006 | RUNNING    | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |      4 |         124.964  | 1.93166 |     0.2248 |
    | train_cifar_ebd82_00007 | RUNNING    | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |      3 |         106.137  | 1.26897 |     0.5506 |
    | train_cifar_ebd82_00009 | RUNNING    | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |        |                  |         |            |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00002 | TERMINATED | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |      1 |         106.257  | 2.31386 |     0.1013 |
    | train_cifar_ebd82_00003 | TERMINATED | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      4 |         125.331  | 2.1302  |     0.1661 |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    | train_cifar_ebd82_00008 | TERMINATED | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |      2 |          66.3861 | 2.0581  |     0.199  |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    == Status ==
    Current time: 2023-12-14 16:24:29 (running for 00:02:24.12)
    Using AsyncHyperBand: num_stopped=5
    Bracket: Iter 8.000: None | Iter 4.000: -2.030932605600357 | Iter 2.000: -2.0581000661849975 | Iter 1.000: -2.1607268846511842
    Logical resource usage: 10.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (5 RUNNING, 5 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00000 | RUNNING    | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |      1 |          98.9492 | 1.91964 |     0.1941 |
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      2 |         109.484  | 1.49103 |     0.4517 |
    | train_cifar_ebd82_00006 | RUNNING    | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |      4 |         124.964  | 1.93166 |     0.2248 |
    | train_cifar_ebd82_00007 | RUNNING    | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |      3 |         106.137  | 1.26897 |     0.5506 |
    | train_cifar_ebd82_00009 | RUNNING    | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |        |                  |         |            |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00002 | TERMINATED | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |      1 |         106.257  | 2.31386 |     0.1013 |
    | train_cifar_ebd82_00003 | TERMINATED | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      4 |         125.331  | 2.1302  |     0.1661 |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    | train_cifar_ebd82_00008 | TERMINATED | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |      2 |          66.3861 | 2.0581  |     0.199  |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    (func pid=2778) [1, 20000] loss: 0.234 [repeated 4x across cluster]
    Result for train_cifar_ebd82_00007:
      accuracy: 0.5752
      date: 2023-12-14_16-24-31
      done: false
      hostname: 5c54f08e0f8e
      iterations_since_restore: 4
      loss: 1.215903848862648
      node_ip: 172.17.0.2
      pid: 2798
      should_checkpoint: true
      time_since_restore: 137.29435300827026
      time_this_iter_s: 31.156963109970093
      time_total_s: 137.29435300827026
      timestamp: 1702571071
      training_iteration: 4
      trial_id: ebd82_00007
  
    == Status ==
    Current time: 2023-12-14 16:24:36 (running for 00:02:30.86)
    Using AsyncHyperBand: num_stopped=5
    Bracket: Iter 8.000: None | Iter 4.000: -1.9316639205932618 | Iter 2.000: -2.0581000661849975 | Iter 1.000: -2.1607268846511842
    Logical resource usage: 10.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (5 RUNNING, 5 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00000 | RUNNING    | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |      1 |          98.9492 | 1.91964 |     0.1941 |
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      2 |         109.484  | 1.49103 |     0.4517 |
    | train_cifar_ebd82_00006 | RUNNING    | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |      4 |         124.964  | 1.93166 |     0.2248 |
    | train_cifar_ebd82_00007 | RUNNING    | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |      4 |         137.294  | 1.2159  |     0.5752 |
    | train_cifar_ebd82_00009 | RUNNING    | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |        |                  |         |            |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00002 | TERMINATED | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |      1 |         106.257  | 2.31386 |     0.1013 |
    | train_cifar_ebd82_00003 | TERMINATED | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      4 |         125.331  | 2.1302  |     0.1661 |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    | train_cifar_ebd82_00008 | TERMINATED | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |      2 |          66.3861 | 2.0581  |     0.199  |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    (func pid=2796) [5,  4000] loss: 0.948 [repeated 3x across cluster]
    Result for train_cifar_ebd82_00009:
      accuracy: 0.1016
      date: 2023-12-14_16-24-41
      done: true
      hostname: 5c54f08e0f8e
      iterations_since_restore: 1
      loss: 2.3437471272945403
      node_ip: 172.17.0.2
      pid: 2778
      should_checkpoint: true
      time_since_restore: 91.53854250907898
      time_this_iter_s: 91.53854250907898
      time_total_s: 91.53854250907898
      timestamp: 1702571081
      training_iteration: 1
      trial_id: ebd82_00009
  
    Trial train_cifar_ebd82_00009 completed.
    (func pid=2794) [3, 10000] loss: 0.277 [repeated 3x across cluster]
    Result for train_cifar_ebd82_00006:
      accuracy: 0.25
      date: 2023-12-14_16-24-44
      done: false
      hostname: 5c54f08e0f8e
      iterations_since_restore: 5
      loss: 1.8746475464820862
      node_ip: 172.17.0.2
      pid: 2796
      should_checkpoint: true
      time_since_restore: 150.29833507537842
      time_this_iter_s: 25.334670305252075
      time_total_s: 150.29833507537842
      timestamp: 1702571084
      training_iteration: 5
      trial_id: ebd82_00006
  
    == Status ==
    Current time: 2023-12-14 16:24:44 (running for 00:02:38.93)
    Using AsyncHyperBand: num_stopped=6
    Bracket: Iter 8.000: None | Iter 4.000: -1.9316639205932618 | Iter 2.000: -2.0581000661849975 | Iter 1.000: -2.234733338880539
    Logical resource usage: 8.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (4 RUNNING, 6 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00000 | RUNNING    | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |      1 |          98.9492 | 1.91964 |     0.1941 |
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      2 |         109.484  | 1.49103 |     0.4517 |
    | train_cifar_ebd82_00006 | RUNNING    | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |      5 |         150.298  | 1.87465 |     0.25   |
    | train_cifar_ebd82_00007 | RUNNING    | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |      4 |         137.294  | 1.2159  |     0.5752 |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00002 | TERMINATED | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |      1 |         106.257  | 2.31386 |     0.1013 |
    | train_cifar_ebd82_00003 | TERMINATED | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      4 |         125.331  | 2.1302  |     0.1661 |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    | train_cifar_ebd82_00008 | TERMINATED | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |      2 |          66.3861 | 2.0581  |     0.199  |
    | train_cifar_ebd82_00009 | TERMINATED | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |      1 |          91.5385 | 2.34375 |     0.1016 |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    Result for train_cifar_ebd82_00005:
      accuracy: 0.4873
      date: 2023-12-14_16-24-49
      done: false
      hostname: 5c54f08e0f8e
      iterations_since_restore: 3
      loss: 1.4391990311801433
      node_ip: 172.17.0.2
      pid: 2794
      should_checkpoint: true
      time_since_restore: 154.4924943447113
      time_this_iter_s: 45.00895690917969
      time_total_s: 154.4924943447113
      timestamp: 1702571089
      training_iteration: 3
      trial_id: ebd82_00005
  
    (func pid=2798) [5,  4000] loss: 0.552 [repeated 2x across cluster]
    == Status ==
    Current time: 2023-12-14 16:24:54 (running for 00:02:48.39)
    Using AsyncHyperBand: num_stopped=6
    Bracket: Iter 8.000: None | Iter 4.000: -1.9316639205932618 | Iter 2.000: -2.0581000661849975 | Iter 1.000: -2.234733338880539
    Logical resource usage: 8.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (4 RUNNING, 6 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00000 | RUNNING    | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |      1 |          98.9492 | 1.91964 |     0.1941 |
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      3 |         154.492  | 1.4392  |     0.4873 |
    | train_cifar_ebd82_00006 | RUNNING    | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |      5 |         150.298  | 1.87465 |     0.25   |
    | train_cifar_ebd82_00007 | RUNNING    | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |      4 |         137.294  | 1.2159  |     0.5752 |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00002 | TERMINATED | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |      1 |         106.257  | 2.31386 |     0.1013 |
    | train_cifar_ebd82_00003 | TERMINATED | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      4 |         125.331  | 2.1302  |     0.1661 |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    | train_cifar_ebd82_00008 | TERMINATED | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |      2 |          66.3861 | 2.0581  |     0.199  |
    | train_cifar_ebd82_00009 | TERMINATED | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |      1 |          91.5385 | 2.34375 |     0.1016 |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    (func pid=2794) [4,  2000] loss: 1.364 [repeated 3x across cluster]
    Result for train_cifar_ebd82_00007:
      accuracy: 0.5706
      date: 2023-12-14_16-24-59
      done: false
      hostname: 5c54f08e0f8e
      iterations_since_restore: 5
      loss: 1.2345459265261889
      node_ip: 172.17.0.2
      pid: 2798
      should_checkpoint: true
      time_since_restore: 164.81135034561157
      time_this_iter_s: 27.51699733734131
      time_total_s: 164.81135034561157
      timestamp: 1702571099
      training_iteration: 5
      trial_id: ebd82_00007
  
    (func pid=2794) [4,  4000] loss: 0.669 [repeated 3x across cluster]
    == Status ==
    Current time: 2023-12-14 16:25:04 (running for 00:02:58.38)
    Using AsyncHyperBand: num_stopped=6
    Bracket: Iter 8.000: None | Iter 4.000: -1.9316639205932618 | Iter 2.000: -2.0581000661849975 | Iter 1.000: -2.234733338880539
    Logical resource usage: 8.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (4 RUNNING, 6 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00000 | RUNNING    | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |      1 |          98.9492 | 1.91964 |     0.1941 |
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      3 |         154.492  | 1.4392  |     0.4873 |
    | train_cifar_ebd82_00006 | RUNNING    | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |      5 |         150.298  | 1.87465 |     0.25   |
    | train_cifar_ebd82_00007 | RUNNING    | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |      5 |         164.811  | 1.23455 |     0.5706 |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00002 | TERMINATED | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |      1 |         106.257  | 2.31386 |     0.1013 |
    | train_cifar_ebd82_00003 | TERMINATED | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      4 |         125.331  | 2.1302  |     0.1661 |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    | train_cifar_ebd82_00008 | TERMINATED | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |      2 |          66.3861 | 2.0581  |     0.199  |
    | train_cifar_ebd82_00009 | TERMINATED | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |      1 |          91.5385 | 2.34375 |     0.1016 |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    Result for train_cifar_ebd82_00000:
      accuracy: 0.1865
      date: 2023-12-14_16-25-08
      done: false
      hostname: 5c54f08e0f8e
      iterations_since_restore: 2
      loss: 1.9017305408239364
      node_ip: 172.17.0.2
      pid: 2713
      should_checkpoint: true
      time_since_restore: 178.76227021217346
      time_this_iter_s: 79.81310176849365
      time_total_s: 178.76227021217346
      timestamp: 1702571108
      training_iteration: 2
      trial_id: ebd82_00000
  
    (func pid=2798) [6,  2000] loss: 0.975
    Result for train_cifar_ebd82_00006:
      accuracy: 0.2806
      date: 2023-12-14_16-25-09
      done: false
      hostname: 5c54f08e0f8e
      iterations_since_restore: 6
      loss: 1.8175249040603638
      node_ip: 172.17.0.2
      pid: 2796
      should_checkpoint: true
      time_since_restore: 174.62725567817688
      time_this_iter_s: 24.328920602798462
      time_total_s: 174.62725567817688
      timestamp: 1702571109
      training_iteration: 6
      trial_id: ebd82_00006
  
    (func pid=2794) [4,  6000] loss: 0.451
    == Status ==
    Current time: 2023-12-14 16:25:14 (running for 00:03:08.26)
    Using AsyncHyperBand: num_stopped=6
    Bracket: Iter 8.000: None | Iter 4.000: -1.9316639205932618 | Iter 2.000: -1.979915303504467 | Iter 1.000: -2.234733338880539
    Logical resource usage: 8.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (4 RUNNING, 6 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00000 | RUNNING    | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |      2 |         178.762  | 1.90173 |     0.1865 |
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      3 |         154.492  | 1.4392  |     0.4873 |
    | train_cifar_ebd82_00006 | RUNNING    | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |      6 |         174.627  | 1.81752 |     0.2806 |
    | train_cifar_ebd82_00007 | RUNNING    | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |      5 |         164.811  | 1.23455 |     0.5706 |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00002 | TERMINATED | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |      1 |         106.257  | 2.31386 |     0.1013 |
    | train_cifar_ebd82_00003 | TERMINATED | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      4 |         125.331  | 2.1302  |     0.1661 |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    | train_cifar_ebd82_00008 | TERMINATED | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |      2 |          66.3861 | 2.0581  |     0.199  |
    | train_cifar_ebd82_00009 | TERMINATED | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |      1 |          91.5385 | 2.34375 |     0.1016 |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    (func pid=2794) [4,  8000] loss: 0.330 [repeated 2x across cluster]
    == Status ==
    Current time: 2023-12-14 16:25:19 (running for 00:03:13.27)
    Using AsyncHyperBand: num_stopped=6
    Bracket: Iter 8.000: None | Iter 4.000: -1.9316639205932618 | Iter 2.000: -1.979915303504467 | Iter 1.000: -2.234733338880539
    Logical resource usage: 8.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (4 RUNNING, 6 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00000 | RUNNING    | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |      2 |         178.762  | 1.90173 |     0.1865 |
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      3 |         154.492  | 1.4392  |     0.4873 |
    | train_cifar_ebd82_00006 | RUNNING    | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |      6 |         174.627  | 1.81752 |     0.2806 |
    | train_cifar_ebd82_00007 | RUNNING    | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |      5 |         164.811  | 1.23455 |     0.5706 |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00002 | TERMINATED | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |      1 |         106.257  | 2.31386 |     0.1013 |
    | train_cifar_ebd82_00003 | TERMINATED | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      4 |         125.331  | 2.1302  |     0.1661 |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    | train_cifar_ebd82_00008 | TERMINATED | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |      2 |          66.3861 | 2.0581  |     0.199  |
    | train_cifar_ebd82_00009 | TERMINATED | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |      1 |          91.5385 | 2.34375 |     0.1016 |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    (func pid=2794) [4, 10000] loss: 0.256 [repeated 4x across cluster]
    == Status ==
    Current time: 2023-12-14 16:25:24 (running for 00:03:18.28)
    Using AsyncHyperBand: num_stopped=6
    Bracket: Iter 8.000: None | Iter 4.000: -1.9316639205932618 | Iter 2.000: -1.979915303504467 | Iter 1.000: -2.234733338880539
    Logical resource usage: 8.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (4 RUNNING, 6 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00000 | RUNNING    | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |      2 |         178.762  | 1.90173 |     0.1865 |
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      3 |         154.492  | 1.4392  |     0.4873 |
    | train_cifar_ebd82_00006 | RUNNING    | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |      6 |         174.627  | 1.81752 |     0.2806 |
    | train_cifar_ebd82_00007 | RUNNING    | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |      5 |         164.811  | 1.23455 |     0.5706 |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00002 | TERMINATED | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |      1 |         106.257  | 2.31386 |     0.1013 |
    | train_cifar_ebd82_00003 | TERMINATED | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      4 |         125.331  | 2.1302  |     0.1661 |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    | train_cifar_ebd82_00008 | TERMINATED | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |      2 |          66.3861 | 2.0581  |     0.199  |
    | train_cifar_ebd82_00009 | TERMINATED | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |      1 |          91.5385 | 2.34375 |     0.1016 |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    Result for train_cifar_ebd82_00007:
      accuracy: 0.5727
      date: 2023-12-14_16-25-25
      done: false
      hostname: 5c54f08e0f8e
      iterations_since_restore: 6
      loss: 1.253844186592102
      node_ip: 172.17.0.2
      pid: 2798
      should_checkpoint: true
      time_since_restore: 191.6284191608429
      time_this_iter_s: 26.817068815231323
      time_total_s: 191.6284191608429
      timestamp: 1702571125
      training_iteration: 6
      trial_id: ebd82_00007
  
    Result for train_cifar_ebd82_00005:
      accuracy: 0.524
      date: 2023-12-14_16-25-29
      done: false
      hostname: 5c54f08e0f8e
      iterations_since_restore: 4
      loss: 1.3101335559010505
      node_ip: 172.17.0.2
      pid: 2794
      should_checkpoint: true
      time_since_restore: 195.1006395816803
      time_this_iter_s: 40.608145236968994
      time_total_s: 195.1006395816803
      timestamp: 1702571129
      training_iteration: 4
      trial_id: ebd82_00005
  
    == Status ==
    Current time: 2023-12-14 16:25:29 (running for 00:03:23.99)
    Using AsyncHyperBand: num_stopped=6
    Bracket: Iter 8.000: None | Iter 4.000: -1.6208987382471562 | Iter 2.000: -1.979915303504467 | Iter 1.000: -2.234733338880539
    Logical resource usage: 8.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (4 RUNNING, 6 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00000 | RUNNING    | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |      2 |         178.762  | 1.90173 |     0.1865 |
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      4 |         195.101  | 1.31013 |     0.524  |
    | train_cifar_ebd82_00006 | RUNNING    | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |      6 |         174.627  | 1.81752 |     0.2806 |
    | train_cifar_ebd82_00007 | RUNNING    | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |      6 |         191.628  | 1.25384 |     0.5727 |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00002 | TERMINATED | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |      1 |         106.257  | 2.31386 |     0.1013 |
    | train_cifar_ebd82_00003 | TERMINATED | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      4 |         125.331  | 2.1302  |     0.1661 |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    | train_cifar_ebd82_00008 | TERMINATED | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |      2 |          66.3861 | 2.0581  |     0.199  |
    | train_cifar_ebd82_00009 | TERMINATED | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |      1 |          91.5385 | 2.34375 |     0.1016 |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    Result for train_cifar_ebd82_00006:
      accuracy: 0.3164
      date: 2023-12-14_16-25-32
      done: false
      hostname: 5c54f08e0f8e
      iterations_since_restore: 7
      loss: 1.7597737907409667
      node_ip: 172.17.0.2
      pid: 2796
      should_checkpoint: true
      time_since_restore: 198.49870991706848
      time_this_iter_s: 23.8714542388916
      time_total_s: 198.49870991706848
      timestamp: 1702571132
      training_iteration: 7
      trial_id: ebd82_00006
  
    (func pid=2713) [3,  8000] loss: 0.475 [repeated 3x across cluster]
    == Status ==
    Current time: 2023-12-14 16:25:37 (running for 00:03:32.13)
    Using AsyncHyperBand: num_stopped=6
    Bracket: Iter 8.000: None | Iter 4.000: -1.6208987382471562 | Iter 2.000: -1.979915303504467 | Iter 1.000: -2.234733338880539
    Logical resource usage: 8.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (4 RUNNING, 6 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00000 | RUNNING    | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |      2 |         178.762  | 1.90173 |     0.1865 |
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      4 |         195.101  | 1.31013 |     0.524  |
    | train_cifar_ebd82_00006 | RUNNING    | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |      7 |         198.499  | 1.75977 |     0.3164 |
    | train_cifar_ebd82_00007 | RUNNING    | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |      6 |         191.628  | 1.25384 |     0.5727 |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00002 | TERMINATED | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |      1 |         106.257  | 2.31386 |     0.1013 |
    | train_cifar_ebd82_00003 | TERMINATED | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      4 |         125.331  | 2.1302  |     0.1661 |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    | train_cifar_ebd82_00008 | TERMINATED | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |      2 |          66.3861 | 2.0581  |     0.199  |
    | train_cifar_ebd82_00009 | TERMINATED | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |      1 |          91.5385 | 2.34375 |     0.1016 |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    (func pid=2713) [3, 10000] loss: 0.381 [repeated 3x across cluster]
    == Status ==
    Current time: 2023-12-14 16:25:42 (running for 00:03:37.15)
    Using AsyncHyperBand: num_stopped=6
    Bracket: Iter 8.000: None | Iter 4.000: -1.6208987382471562 | Iter 2.000: -1.979915303504467 | Iter 1.000: -2.234733338880539
    Logical resource usage: 8.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (4 RUNNING, 6 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00000 | RUNNING    | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |      2 |         178.762  | 1.90173 |     0.1865 |
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      4 |         195.101  | 1.31013 |     0.524  |
    | train_cifar_ebd82_00006 | RUNNING    | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |      7 |         198.499  | 1.75977 |     0.3164 |
    | train_cifar_ebd82_00007 | RUNNING    | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |      6 |         191.628  | 1.25384 |     0.5727 |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00002 | TERMINATED | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |      1 |         106.257  | 2.31386 |     0.1013 |
    | train_cifar_ebd82_00003 | TERMINATED | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      4 |         125.331  | 2.1302  |     0.1661 |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    | train_cifar_ebd82_00008 | TERMINATED | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |      2 |          66.3861 | 2.0581  |     0.199  |
    | train_cifar_ebd82_00009 | TERMINATED | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |      1 |          91.5385 | 2.34375 |     0.1016 |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    (func pid=2713) [3, 12000] loss: 0.318 [repeated 3x across cluster]
    == Status ==
    Current time: 2023-12-14 16:25:47 (running for 00:03:42.15)
    Using AsyncHyperBand: num_stopped=6
    Bracket: Iter 8.000: None | Iter 4.000: -1.6208987382471562 | Iter 2.000: -1.979915303504467 | Iter 1.000: -2.234733338880539
    Logical resource usage: 8.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (4 RUNNING, 6 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00000 | RUNNING    | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |      2 |         178.762  | 1.90173 |     0.1865 |
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      4 |         195.101  | 1.31013 |     0.524  |
    | train_cifar_ebd82_00006 | RUNNING    | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |      7 |         198.499  | 1.75977 |     0.3164 |
    | train_cifar_ebd82_00007 | RUNNING    | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |      6 |         191.628  | 1.25384 |     0.5727 |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00002 | TERMINATED | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |      1 |         106.257  | 2.31386 |     0.1013 |
    | train_cifar_ebd82_00003 | TERMINATED | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      4 |         125.331  | 2.1302  |     0.1661 |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    | train_cifar_ebd82_00008 | TERMINATED | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |      2 |          66.3861 | 2.0581  |     0.199  |
    | train_cifar_ebd82_00009 | TERMINATED | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |      1 |          91.5385 | 2.34375 |     0.1016 |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    (func pid=2713) [3, 14000] loss: 0.274 [repeated 3x across cluster]
    Result for train_cifar_ebd82_00007:
      accuracy: 0.5851
      date: 2023-12-14_16-25-52
      done: false
      hostname: 5c54f08e0f8e
      iterations_since_restore: 7
      loss: 1.2456342673361303
      node_ip: 172.17.0.2
      pid: 2798
      should_checkpoint: true
      time_since_restore: 218.28253436088562
      time_this_iter_s: 26.654115200042725
      time_total_s: 218.28253436088562
      timestamp: 1702571152
      training_iteration: 7
      trial_id: ebd82_00007
  
    (func pid=2713) [3, 16000] loss: 0.243 [repeated 2x across cluster]
    Result for train_cifar_ebd82_00006:
      accuracy: 0.3408
      date: 2023-12-14_16-25-56
      done: false
      hostname: 5c54f08e0f8e
      iterations_since_restore: 8
      loss: 1.699555038356781
      node_ip: 172.17.0.2
      pid: 2796
      should_checkpoint: true
      time_since_restore: 222.26735067367554
      time_this_iter_s: 23.768640756607056
      time_total_s: 222.26735067367554
      timestamp: 1702571156
      training_iteration: 8
      trial_id: ebd82_00006
  
    == Status ==
    Current time: 2023-12-14 16:25:56 (running for 00:03:50.91)
    Using AsyncHyperBand: num_stopped=6
    Bracket: Iter 8.000: -1.699555038356781 | Iter 4.000: -1.6208987382471562 | Iter 2.000: -1.979915303504467 | Iter 1.000: -2.234733338880539
    Logical resource usage: 8.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (4 RUNNING, 6 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00000 | RUNNING    | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |      2 |         178.762  | 1.90173 |     0.1865 |
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      4 |         195.101  | 1.31013 |     0.524  |
    | train_cifar_ebd82_00006 | RUNNING    | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |      8 |         222.267  | 1.69956 |     0.3408 |
    | train_cifar_ebd82_00007 | RUNNING    | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |      7 |         218.283  | 1.24563 |     0.5851 |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00002 | TERMINATED | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |      1 |         106.257  | 2.31386 |     0.1013 |
    | train_cifar_ebd82_00003 | TERMINATED | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      4 |         125.331  | 2.1302  |     0.1661 |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    | train_cifar_ebd82_00008 | TERMINATED | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |      2 |          66.3861 | 2.0581  |     0.199  |
    | train_cifar_ebd82_00009 | TERMINATED | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |      1 |          91.5385 | 2.34375 |     0.1016 |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    == Status ==
    Current time: 2023-12-14 16:26:01 (running for 00:03:55.91)
    Using AsyncHyperBand: num_stopped=6
    Bracket: Iter 8.000: -1.699555038356781 | Iter 4.000: -1.6208987382471562 | Iter 2.000: -1.979915303504467 | Iter 1.000: -2.234733338880539
    Logical resource usage: 8.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (4 RUNNING, 6 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00000 | RUNNING    | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |      2 |         178.762  | 1.90173 |     0.1865 |
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      4 |         195.101  | 1.31013 |     0.524  |
    | train_cifar_ebd82_00006 | RUNNING    | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |      8 |         222.267  | 1.69956 |     0.3408 |
    | train_cifar_ebd82_00007 | RUNNING    | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |      7 |         218.283  | 1.24563 |     0.5851 |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00002 | TERMINATED | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |      1 |         106.257  | 2.31386 |     0.1013 |
    | train_cifar_ebd82_00003 | TERMINATED | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      4 |         125.331  | 2.1302  |     0.1661 |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    | train_cifar_ebd82_00008 | TERMINATED | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |      2 |          66.3861 | 2.0581  |     0.199  |
    | train_cifar_ebd82_00009 | TERMINATED | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |      1 |          91.5385 | 2.34375 |     0.1016 |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    (func pid=2713) [3, 18000] loss: 0.214 [repeated 2x across cluster]
    == Status ==
    Current time: 2023-12-14 16:26:06 (running for 00:04:00.92)
    Using AsyncHyperBand: num_stopped=6
    Bracket: Iter 8.000: -1.699555038356781 | Iter 4.000: -1.6208987382471562 | Iter 2.000: -1.979915303504467 | Iter 1.000: -2.234733338880539
    Logical resource usage: 8.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (4 RUNNING, 6 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00000 | RUNNING    | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |      2 |         178.762  | 1.90173 |     0.1865 |
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      4 |         195.101  | 1.31013 |     0.524  |
    | train_cifar_ebd82_00006 | RUNNING    | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |      8 |         222.267  | 1.69956 |     0.3408 |
    | train_cifar_ebd82_00007 | RUNNING    | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |      7 |         218.283  | 1.24563 |     0.5851 |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00002 | TERMINATED | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |      1 |         106.257  | 2.31386 |     0.1013 |
    | train_cifar_ebd82_00003 | TERMINATED | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      4 |         125.331  | 2.1302  |     0.1661 |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    | train_cifar_ebd82_00008 | TERMINATED | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |      2 |          66.3861 | 2.0581  |     0.199  |
    | train_cifar_ebd82_00009 | TERMINATED | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |      1 |          91.5385 | 2.34375 |     0.1016 |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    (func pid=2713) [3, 20000] loss: 0.191 [repeated 4x across cluster]
    Result for train_cifar_ebd82_00005:
      accuracy: 0.5113
      date: 2023-12-14_16-26-09
      done: false
      hostname: 5c54f08e0f8e
      iterations_since_restore: 5
      loss: 1.3406894617438316
      node_ip: 172.17.0.2
      pid: 2794
      should_checkpoint: true
      time_since_restore: 234.9130299091339
      time_this_iter_s: 39.81239032745361
      time_total_s: 234.9130299091339
      timestamp: 1702571169
      training_iteration: 5
      trial_id: ebd82_00005
  
    == Status ==
    Current time: 2023-12-14 16:26:14 (running for 00:04:08.80)
    Using AsyncHyperBand: num_stopped=6
    Bracket: Iter 8.000: -1.699555038356781 | Iter 4.000: -1.6208987382471562 | Iter 2.000: -1.979915303504467 | Iter 1.000: -2.234733338880539
    Logical resource usage: 8.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (4 RUNNING, 6 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00000 | RUNNING    | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |      2 |         178.762  | 1.90173 |     0.1865 |
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      5 |         234.913  | 1.34069 |     0.5113 |
    | train_cifar_ebd82_00006 | RUNNING    | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |      8 |         222.267  | 1.69956 |     0.3408 |
    | train_cifar_ebd82_00007 | RUNNING    | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |      7 |         218.283  | 1.24563 |     0.5851 |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00002 | TERMINATED | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |      1 |         106.257  | 2.31386 |     0.1013 |
    | train_cifar_ebd82_00003 | TERMINATED | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      4 |         125.331  | 2.1302  |     0.1661 |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    | train_cifar_ebd82_00008 | TERMINATED | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |      2 |          66.3861 | 2.0581  |     0.199  |
    | train_cifar_ebd82_00009 | TERMINATED | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |      1 |          91.5385 | 2.34375 |     0.1016 |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    (func pid=2794) [6,  2000] loss: 1.238 [repeated 3x across cluster]
    Result for train_cifar_ebd82_00000:
      accuracy: 0.206
      date: 2023-12-14_16-26-18
      done: false
      hostname: 5c54f08e0f8e
      iterations_since_restore: 3
      loss: 1.9172988490223886
      node_ip: 172.17.0.2
      pid: 2713
      should_checkpoint: true
      time_since_restore: 248.28772568702698
      time_this_iter_s: 69.52545547485352
      time_total_s: 248.28772568702698
      timestamp: 1702571178
      training_iteration: 3
      trial_id: ebd82_00000
  
    Result for train_cifar_ebd82_00007:
      accuracy: 0.5652
      date: 2023-12-14_16-26-20
      done: false
      hostname: 5c54f08e0f8e
      iterations_since_restore: 8
      loss: 1.349819334757328
      node_ip: 172.17.0.2
      pid: 2798
      should_checkpoint: true
      time_since_restore: 245.87101244926453
      time_this_iter_s: 27.588478088378906
      time_total_s: 245.87101244926453
      timestamp: 1702571180
      training_iteration: 8
      trial_id: ebd82_00007
  
    == Status ==
    Current time: 2023-12-14 16:26:20 (running for 00:04:14.43)
    Using AsyncHyperBand: num_stopped=6
    Bracket: Iter 8.000: -1.5246871865570544 | Iter 4.000: -1.6208987382471562 | Iter 2.000: -1.979915303504467 | Iter 1.000: -2.234733338880539
    Logical resource usage: 8.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (4 RUNNING, 6 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00000 | RUNNING    | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |      3 |         248.288  | 1.9173  |     0.206  |
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      5 |         234.913  | 1.34069 |     0.5113 |
    | train_cifar_ebd82_00006 | RUNNING    | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |      8 |         222.267  | 1.69956 |     0.3408 |
    | train_cifar_ebd82_00007 | RUNNING    | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |      8 |         245.871  | 1.34982 |     0.5652 |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00002 | TERMINATED | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |      1 |         106.257  | 2.31386 |     0.1013 |
    | train_cifar_ebd82_00003 | TERMINATED | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      4 |         125.331  | 2.1302  |     0.1661 |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    | train_cifar_ebd82_00008 | TERMINATED | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |      2 |          66.3861 | 2.0581  |     0.199  |
    | train_cifar_ebd82_00009 | TERMINATED | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |      1 |          91.5385 | 2.34375 |     0.1016 |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    Result for train_cifar_ebd82_00006:
      accuracy: 0.364
      date: 2023-12-14_16-26-21
      done: false
      hostname: 5c54f08e0f8e
      iterations_since_restore: 9
      loss: 1.642243495464325
      node_ip: 172.17.0.2
      pid: 2796
      should_checkpoint: true
      time_since_restore: 246.61127591133118
      time_this_iter_s: 24.34392523765564
      time_total_s: 246.61127591133118
      timestamp: 1702571181
      training_iteration: 9
      trial_id: ebd82_00006
  
    (func pid=2794) [6,  4000] loss: 0.610
    (func pid=2713) [4,  2000] loss: 1.905
    == Status ==
    Current time: 2023-12-14 16:26:26 (running for 00:04:20.25)
    Using AsyncHyperBand: num_stopped=6
    Bracket: Iter 8.000: -1.5246871865570544 | Iter 4.000: -1.6208987382471562 | Iter 2.000: -1.979915303504467 | Iter 1.000: -2.234733338880539
    Logical resource usage: 8.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (4 RUNNING, 6 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00000 | RUNNING    | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |      3 |         248.288  | 1.9173  |     0.206  |
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      5 |         234.913  | 1.34069 |     0.5113 |
    | train_cifar_ebd82_00006 | RUNNING    | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |      9 |         246.611  | 1.64224 |     0.364  |
    | train_cifar_ebd82_00007 | RUNNING    | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |      8 |         245.871  | 1.34982 |     0.5652 |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00002 | TERMINATED | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |      1 |         106.257  | 2.31386 |     0.1013 |
    | train_cifar_ebd82_00003 | TERMINATED | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      4 |         125.331  | 2.1302  |     0.1661 |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    | train_cifar_ebd82_00008 | TERMINATED | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |      2 |          66.3861 | 2.0581  |     0.199  |
    | train_cifar_ebd82_00009 | TERMINATED | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |      1 |          91.5385 | 2.34375 |     0.1016 |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    (func pid=2713) [4,  4000] loss: 0.953 [repeated 3x across cluster]
    == Status ==
    Current time: 2023-12-14 16:26:31 (running for 00:04:25.26)
    Using AsyncHyperBand: num_stopped=6
    Bracket: Iter 8.000: -1.5246871865570544 | Iter 4.000: -1.6208987382471562 | Iter 2.000: -1.979915303504467 | Iter 1.000: -2.234733338880539
    Logical resource usage: 8.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (4 RUNNING, 6 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00000 | RUNNING    | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |      3 |         248.288  | 1.9173  |     0.206  |
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      5 |         234.913  | 1.34069 |     0.5113 |
    | train_cifar_ebd82_00006 | RUNNING    | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |      9 |         246.611  | 1.64224 |     0.364  |
    | train_cifar_ebd82_00007 | RUNNING    | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |      8 |         245.871  | 1.34982 |     0.5652 |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00002 | TERMINATED | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |      1 |         106.257  | 2.31386 |     0.1013 |
    | train_cifar_ebd82_00003 | TERMINATED | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      4 |         125.331  | 2.1302  |     0.1661 |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    | train_cifar_ebd82_00008 | TERMINATED | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |      2 |          66.3861 | 2.0581  |     0.199  |
    | train_cifar_ebd82_00009 | TERMINATED | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |      1 |          91.5385 | 2.34375 |     0.1016 |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    == Status ==
    Current time: 2023-12-14 16:26:36 (running for 00:04:30.26)
    Using AsyncHyperBand: num_stopped=6
    Bracket: Iter 8.000: -1.5246871865570544 | Iter 4.000: -1.6208987382471562 | Iter 2.000: -1.979915303504467 | Iter 1.000: -2.234733338880539
    Logical resource usage: 8.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (4 RUNNING, 6 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00000 | RUNNING    | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |      3 |         248.288  | 1.9173  |     0.206  |
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      5 |         234.913  | 1.34069 |     0.5113 |
    | train_cifar_ebd82_00006 | RUNNING    | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |      9 |         246.611  | 1.64224 |     0.364  |
    | train_cifar_ebd82_00007 | RUNNING    | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |      8 |         245.871  | 1.34982 |     0.5652 |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00002 | TERMINATED | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |      1 |         106.257  | 2.31386 |     0.1013 |
    | train_cifar_ebd82_00003 | TERMINATED | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      4 |         125.331  | 2.1302  |     0.1661 |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    | train_cifar_ebd82_00008 | TERMINATED | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |      2 |          66.3861 | 2.0581  |     0.199  |
    | train_cifar_ebd82_00009 | TERMINATED | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |      1 |          91.5385 | 2.34375 |     0.1016 |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    (func pid=2713) [4,  6000] loss: 0.636 [repeated 2x across cluster]
    == Status ==
    Current time: 2023-12-14 16:26:41 (running for 00:04:35.27)
    Using AsyncHyperBand: num_stopped=6
    Bracket: Iter 8.000: -1.5246871865570544 | Iter 4.000: -1.6208987382471562 | Iter 2.000: -1.979915303504467 | Iter 1.000: -2.234733338880539
    Logical resource usage: 8.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (4 RUNNING, 6 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00000 | RUNNING    | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |      3 |         248.288  | 1.9173  |     0.206  |
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      5 |         234.913  | 1.34069 |     0.5113 |
    | train_cifar_ebd82_00006 | RUNNING    | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |      9 |         246.611  | 1.64224 |     0.364  |
    | train_cifar_ebd82_00007 | RUNNING    | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |      8 |         245.871  | 1.34982 |     0.5652 |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00002 | TERMINATED | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |      1 |         106.257  | 2.31386 |     0.1013 |
    | train_cifar_ebd82_00003 | TERMINATED | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      4 |         125.331  | 2.1302  |     0.1661 |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    | train_cifar_ebd82_00008 | TERMINATED | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |      2 |          66.3861 | 2.0581  |     0.199  |
    | train_cifar_ebd82_00009 | TERMINATED | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |      1 |          91.5385 | 2.34375 |     0.1016 |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    (func pid=2713) [4,  8000] loss: 0.478 [repeated 4x across cluster]
    Result for train_cifar_ebd82_00006:
      accuracy: 0.388
      date: 2023-12-14_16-26-44
      done: true
      hostname: 5c54f08e0f8e
      iterations_since_restore: 10
      loss: 1.6147724454402923
      node_ip: 172.17.0.2
      pid: 2796
      should_checkpoint: true
      time_since_restore: 270.5616581439972
      time_this_iter_s: 23.950382232666016
      time_total_s: 270.5616581439972
      timestamp: 1702571204
      training_iteration: 10
      trial_id: ebd82_00006
  
    Trial train_cifar_ebd82_00006 completed.
    Result for train_cifar_ebd82_00007:
      accuracy: 0.576
      date: 2023-12-14_16-26-46
      done: false
      hostname: 5c54f08e0f8e
      iterations_since_restore: 9
      loss: 1.335799443218112
      node_ip: 172.17.0.2
      pid: 2798
      should_checkpoint: true
      time_since_restore: 272.16592955589294
      time_this_iter_s: 26.294917106628418
      time_total_s: 272.16592955589294
      timestamp: 1702571206
      training_iteration: 9
      trial_id: ebd82_00007
  
    == Status ==
    Current time: 2023-12-14 16:26:46 (running for 00:04:40.73)
    Using AsyncHyperBand: num_stopped=7
    Bracket: Iter 8.000: -1.5246871865570544 | Iter 4.000: -1.6208987382471562 | Iter 2.000: -1.979915303504467 | Iter 1.000: -2.234733338880539
    Logical resource usage: 6.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (3 RUNNING, 7 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00000 | RUNNING    | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |      3 |         248.288  | 1.9173  |     0.206  |
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      5 |         234.913  | 1.34069 |     0.5113 |
    | train_cifar_ebd82_00007 | RUNNING    | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |      9 |         272.166  | 1.3358  |     0.576  |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00002 | TERMINATED | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |      1 |         106.257  | 2.31386 |     0.1013 |
    | train_cifar_ebd82_00003 | TERMINATED | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      4 |         125.331  | 2.1302  |     0.1661 |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    | train_cifar_ebd82_00006 | TERMINATED | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |     10 |         270.562  | 1.61477 |     0.388  |
    | train_cifar_ebd82_00008 | TERMINATED | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |      2 |          66.3861 | 2.0581  |     0.199  |
    | train_cifar_ebd82_00009 | TERMINATED | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |      1 |          91.5385 | 2.34375 |     0.1016 |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    (func pid=2713) [4, 10000] loss: 0.383 [repeated 2x across cluster]
    Result for train_cifar_ebd82_00005:
      accuracy: 0.5474
      date: 2023-12-14_16-26-49
      done: false
      hostname: 5c54f08e0f8e
      iterations_since_restore: 6
      loss: 1.2502008007586003
      node_ip: 172.17.0.2
      pid: 2794
      should_checkpoint: true
      time_since_restore: 274.8948256969452
      time_this_iter_s: 39.98179578781128
      time_total_s: 274.8948256969452
      timestamp: 1702571209
      training_iteration: 6
      trial_id: ebd82_00005
  
    == Status ==
    Current time: 2023-12-14 16:26:54 (running for 00:04:48.79)
    Using AsyncHyperBand: num_stopped=7
    Bracket: Iter 8.000: -1.5246871865570544 | Iter 4.000: -1.6208987382471562 | Iter 2.000: -1.979915303504467 | Iter 1.000: -2.234733338880539
    Logical resource usage: 6.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (3 RUNNING, 7 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00000 | RUNNING    | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |      3 |         248.288  | 1.9173  |     0.206  |
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      6 |         274.895  | 1.2502  |     0.5474 |
    | train_cifar_ebd82_00007 | RUNNING    | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |      9 |         272.166  | 1.3358  |     0.576  |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00002 | TERMINATED | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |      1 |         106.257  | 2.31386 |     0.1013 |
    | train_cifar_ebd82_00003 | TERMINATED | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      4 |         125.331  | 2.1302  |     0.1661 |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    | train_cifar_ebd82_00006 | TERMINATED | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |     10 |         270.562  | 1.61477 |     0.388  |
    | train_cifar_ebd82_00008 | TERMINATED | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |      2 |          66.3861 | 2.0581  |     0.199  |
    | train_cifar_ebd82_00009 | TERMINATED | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |      1 |          91.5385 | 2.34375 |     0.1016 |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    (func pid=2713) [4, 12000] loss: 0.321
    (func pid=2798) [10,  2000] loss: 0.884
    == Status ==
    Current time: 2023-12-14 16:26:59 (running for 00:04:53.80)
    Using AsyncHyperBand: num_stopped=7
    Bracket: Iter 8.000: -1.5246871865570544 | Iter 4.000: -1.6208987382471562 | Iter 2.000: -1.979915303504467 | Iter 1.000: -2.234733338880539
    Logical resource usage: 6.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (3 RUNNING, 7 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00000 | RUNNING    | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |      3 |         248.288  | 1.9173  |     0.206  |
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      6 |         274.895  | 1.2502  |     0.5474 |
    | train_cifar_ebd82_00007 | RUNNING    | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |      9 |         272.166  | 1.3358  |     0.576  |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00002 | TERMINATED | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |      1 |         106.257  | 2.31386 |     0.1013 |
    | train_cifar_ebd82_00003 | TERMINATED | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      4 |         125.331  | 2.1302  |     0.1661 |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    | train_cifar_ebd82_00006 | TERMINATED | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |     10 |         270.562  | 1.61477 |     0.388  |
    | train_cifar_ebd82_00008 | TERMINATED | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |      2 |          66.3861 | 2.0581  |     0.199  |
    | train_cifar_ebd82_00009 | TERMINATED | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |      1 |          91.5385 | 2.34375 |     0.1016 |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    (func pid=2794) [7,  4000] loss: 0.591 [repeated 3x across cluster]
    == Status ==
    Current time: 2023-12-14 16:27:04 (running for 00:04:58.81)
    Using AsyncHyperBand: num_stopped=7
    Bracket: Iter 8.000: -1.5246871865570544 | Iter 4.000: -1.6208987382471562 | Iter 2.000: -1.979915303504467 | Iter 1.000: -2.234733338880539
    Logical resource usage: 6.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (3 RUNNING, 7 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00000 | RUNNING    | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |      3 |         248.288  | 1.9173  |     0.206  |
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      6 |         274.895  | 1.2502  |     0.5474 |
    | train_cifar_ebd82_00007 | RUNNING    | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |      9 |         272.166  | 1.3358  |     0.576  |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00002 | TERMINATED | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |      1 |         106.257  | 2.31386 |     0.1013 |
    | train_cifar_ebd82_00003 | TERMINATED | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      4 |         125.331  | 2.1302  |     0.1661 |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    | train_cifar_ebd82_00006 | TERMINATED | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |     10 |         270.562  | 1.61477 |     0.388  |
    | train_cifar_ebd82_00008 | TERMINATED | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |      2 |          66.3861 | 2.0581  |     0.199  |
    | train_cifar_ebd82_00009 | TERMINATED | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |      1 |          91.5385 | 2.34375 |     0.1016 |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    (func pid=2794) [7,  6000] loss: 0.401 [repeated 3x across cluster]
    == Status ==
    Current time: 2023-12-14 16:27:09 (running for 00:05:03.81)
    Using AsyncHyperBand: num_stopped=7
    Bracket: Iter 8.000: -1.5246871865570544 | Iter 4.000: -1.6208987382471562 | Iter 2.000: -1.979915303504467 | Iter 1.000: -2.234733338880539
    Logical resource usage: 6.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (3 RUNNING, 7 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00000 | RUNNING    | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |      3 |         248.288  | 1.9173  |     0.206  |
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      6 |         274.895  | 1.2502  |     0.5474 |
    | train_cifar_ebd82_00007 | RUNNING    | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |      9 |         272.166  | 1.3358  |     0.576  |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00002 | TERMINATED | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |      1 |         106.257  | 2.31386 |     0.1013 |
    | train_cifar_ebd82_00003 | TERMINATED | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      4 |         125.331  | 2.1302  |     0.1661 |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    | train_cifar_ebd82_00006 | TERMINATED | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |     10 |         270.562  | 1.61477 |     0.388  |
    | train_cifar_ebd82_00008 | TERMINATED | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |      2 |          66.3861 | 2.0581  |     0.199  |
    | train_cifar_ebd82_00009 | TERMINATED | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |      1 |          91.5385 | 2.34375 |     0.1016 |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    Result for train_cifar_ebd82_00007:
      accuracy: 0.5766
      date: 2023-12-14_16-27-11
      done: true
      hostname: 5c54f08e0f8e
      iterations_since_restore: 10
      loss: 1.3997352298259735
      node_ip: 172.17.0.2
      pid: 2798
      should_checkpoint: true
      time_since_restore: 296.6647000312805
      time_this_iter_s: 24.498770475387573
      time_total_s: 296.6647000312805
      timestamp: 1702571231
      training_iteration: 10
      trial_id: ebd82_00007
  
    Trial train_cifar_ebd82_00007 completed.
    (func pid=2794) [7,  8000] loss: 0.298 [repeated 2x across cluster]
    == Status ==
    Current time: 2023-12-14 16:27:16 (running for 00:05:10.23)
    Using AsyncHyperBand: num_stopped=8
    Bracket: Iter 8.000: -1.5246871865570544 | Iter 4.000: -1.6208987382471562 | Iter 2.000: -1.979915303504467 | Iter 1.000: -2.234733338880539
    Logical resource usage: 4.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (2 RUNNING, 8 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00000 | RUNNING    | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |      3 |         248.288  | 1.9173  |     0.206  |
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      6 |         274.895  | 1.2502  |     0.5474 |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00002 | TERMINATED | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |      1 |         106.257  | 2.31386 |     0.1013 |
    | train_cifar_ebd82_00003 | TERMINATED | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      4 |         125.331  | 2.1302  |     0.1661 |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    | train_cifar_ebd82_00006 | TERMINATED | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |     10 |         270.562  | 1.61477 |     0.388  |
    | train_cifar_ebd82_00007 | TERMINATED | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |     10 |         296.665  | 1.39974 |     0.5766 |
    | train_cifar_ebd82_00008 | TERMINATED | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |      2 |          66.3861 | 2.0581  |     0.199  |
    | train_cifar_ebd82_00009 | TERMINATED | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |      1 |          91.5385 | 2.34375 |     0.1016 |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    (func pid=2794) [7, 10000] loss: 0.238 [repeated 2x across cluster]
    == Status ==
    Current time: 2023-12-14 16:27:21 (running for 00:05:15.24)
    Using AsyncHyperBand: num_stopped=8
    Bracket: Iter 8.000: -1.5246871865570544 | Iter 4.000: -1.6208987382471562 | Iter 2.000: -1.979915303504467 | Iter 1.000: -2.234733338880539
    Logical resource usage: 4.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (2 RUNNING, 8 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00000 | RUNNING    | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |      3 |         248.288  | 1.9173  |     0.206  |
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      6 |         274.895  | 1.2502  |     0.5474 |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00002 | TERMINATED | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |      1 |         106.257  | 2.31386 |     0.1013 |
    | train_cifar_ebd82_00003 | TERMINATED | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      4 |         125.331  | 2.1302  |     0.1661 |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    | train_cifar_ebd82_00006 | TERMINATED | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |     10 |         270.562  | 1.61477 |     0.388  |
    | train_cifar_ebd82_00007 | TERMINATED | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |     10 |         296.665  | 1.39974 |     0.5766 |
    | train_cifar_ebd82_00008 | TERMINATED | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |      2 |          66.3861 | 2.0581  |     0.199  |
    | train_cifar_ebd82_00009 | TERMINATED | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |      1 |          91.5385 | 2.34375 |     0.1016 |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    Result for train_cifar_ebd82_00000:
      accuracy: 0.1952
      date: 2023-12-14_16-27-24
      done: true
      hostname: 5c54f08e0f8e
      iterations_since_restore: 4
      loss: 1.9641742376804352
      node_ip: 172.17.0.2
      pid: 2713
      should_checkpoint: true
      time_since_restore: 314.6894814968109
      time_this_iter_s: 66.40175580978394
      time_total_s: 314.6894814968109
      timestamp: 1702571244
      training_iteration: 4
      trial_id: ebd82_00000
  
    Trial train_cifar_ebd82_00000 completed.
    Result for train_cifar_ebd82_00005:
      accuracy: 0.5533
      date: 2023-12-14_16-27-25
      done: false
      hostname: 5c54f08e0f8e
      iterations_since_restore: 7
      loss: 1.2479408249080182
      node_ip: 172.17.0.2
      pid: 2794
      should_checkpoint: true
      time_since_restore: 310.5549578666687
      time_this_iter_s: 35.66013216972351
      time_total_s: 310.5549578666687
      timestamp: 1702571245
      training_iteration: 7
      trial_id: ebd82_00005
  
    == Status ==
    Current time: 2023-12-14 16:27:30 (running for 00:05:24.45)
    Using AsyncHyperBand: num_stopped=9
    Bracket: Iter 8.000: -1.5246871865570544 | Iter 4.000: -1.9316639205932618 | Iter 2.000: -1.979915303504467 | Iter 1.000: -2.234733338880539
    Logical resource usage: 2.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      7 |         310.555  | 1.24794 |     0.5533 |
    | train_cifar_ebd82_00000 | TERMINATED | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |      4 |         314.689  | 1.96417 |     0.1952 |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00002 | TERMINATED | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |      1 |         106.257  | 2.31386 |     0.1013 |
    | train_cifar_ebd82_00003 | TERMINATED | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      4 |         125.331  | 2.1302  |     0.1661 |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    | train_cifar_ebd82_00006 | TERMINATED | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |     10 |         270.562  | 1.61477 |     0.388  |
    | train_cifar_ebd82_00007 | TERMINATED | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |     10 |         296.665  | 1.39974 |     0.5766 |
    | train_cifar_ebd82_00008 | TERMINATED | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |      2 |          66.3861 | 2.0581  |     0.199  |
    | train_cifar_ebd82_00009 | TERMINATED | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |      1 |          91.5385 | 2.34375 |     0.1016 |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    (func pid=2794) [8,  2000] loss: 1.158
    == Status ==
    Current time: 2023-12-14 16:27:35 (running for 00:05:29.45)
    Using AsyncHyperBand: num_stopped=9
    Bracket: Iter 8.000: -1.5246871865570544 | Iter 4.000: -1.9316639205932618 | Iter 2.000: -1.979915303504467 | Iter 1.000: -2.234733338880539
    Logical resource usage: 2.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      7 |         310.555  | 1.24794 |     0.5533 |
    | train_cifar_ebd82_00000 | TERMINATED | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |      4 |         314.689  | 1.96417 |     0.1952 |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00002 | TERMINATED | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |      1 |         106.257  | 2.31386 |     0.1013 |
    | train_cifar_ebd82_00003 | TERMINATED | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      4 |         125.331  | 2.1302  |     0.1661 |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    | train_cifar_ebd82_00006 | TERMINATED | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |     10 |         270.562  | 1.61477 |     0.388  |
    | train_cifar_ebd82_00007 | TERMINATED | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |     10 |         296.665  | 1.39974 |     0.5766 |
    | train_cifar_ebd82_00008 | TERMINATED | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |      2 |          66.3861 | 2.0581  |     0.199  |
    | train_cifar_ebd82_00009 | TERMINATED | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |      1 |          91.5385 | 2.34375 |     0.1016 |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    (func pid=2794) [8,  4000] loss: 0.588
    == Status ==
    Current time: 2023-12-14 16:27:40 (running for 00:05:34.46)
    Using AsyncHyperBand: num_stopped=9
    Bracket: Iter 8.000: -1.5246871865570544 | Iter 4.000: -1.9316639205932618 | Iter 2.000: -1.979915303504467 | Iter 1.000: -2.234733338880539
    Logical resource usage: 2.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      7 |         310.555  | 1.24794 |     0.5533 |
    | train_cifar_ebd82_00000 | TERMINATED | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |      4 |         314.689  | 1.96417 |     0.1952 |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00002 | TERMINATED | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |      1 |         106.257  | 2.31386 |     0.1013 |
    | train_cifar_ebd82_00003 | TERMINATED | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      4 |         125.331  | 2.1302  |     0.1661 |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    | train_cifar_ebd82_00006 | TERMINATED | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |     10 |         270.562  | 1.61477 |     0.388  |
    | train_cifar_ebd82_00007 | TERMINATED | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |     10 |         296.665  | 1.39974 |     0.5766 |
    | train_cifar_ebd82_00008 | TERMINATED | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |      2 |          66.3861 | 2.0581  |     0.199  |
    | train_cifar_ebd82_00009 | TERMINATED | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |      1 |          91.5385 | 2.34375 |     0.1016 |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    (func pid=2794) [8,  6000] loss: 0.386
    == Status ==
    Current time: 2023-12-14 16:27:45 (running for 00:05:39.47)
    Using AsyncHyperBand: num_stopped=9
    Bracket: Iter 8.000: -1.5246871865570544 | Iter 4.000: -1.9316639205932618 | Iter 2.000: -1.979915303504467 | Iter 1.000: -2.234733338880539
    Logical resource usage: 2.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      7 |         310.555  | 1.24794 |     0.5533 |
    | train_cifar_ebd82_00000 | TERMINATED | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |      4 |         314.689  | 1.96417 |     0.1952 |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00002 | TERMINATED | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |      1 |         106.257  | 2.31386 |     0.1013 |
    | train_cifar_ebd82_00003 | TERMINATED | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      4 |         125.331  | 2.1302  |     0.1661 |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    | train_cifar_ebd82_00006 | TERMINATED | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |     10 |         270.562  | 1.61477 |     0.388  |
    | train_cifar_ebd82_00007 | TERMINATED | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |     10 |         296.665  | 1.39974 |     0.5766 |
    | train_cifar_ebd82_00008 | TERMINATED | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |      2 |          66.3861 | 2.0581  |     0.199  |
    | train_cifar_ebd82_00009 | TERMINATED | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |      1 |          91.5385 | 2.34375 |     0.1016 |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    (func pid=2794) [8,  8000] loss: 0.292
    == Status ==
    Current time: 2023-12-14 16:27:50 (running for 00:05:44.47)
    Using AsyncHyperBand: num_stopped=9
    Bracket: Iter 8.000: -1.5246871865570544 | Iter 4.000: -1.9316639205932618 | Iter 2.000: -1.979915303504467 | Iter 1.000: -2.234733338880539
    Logical resource usage: 2.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      7 |         310.555  | 1.24794 |     0.5533 |
    | train_cifar_ebd82_00000 | TERMINATED | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |      4 |         314.689  | 1.96417 |     0.1952 |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00002 | TERMINATED | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |      1 |         106.257  | 2.31386 |     0.1013 |
    | train_cifar_ebd82_00003 | TERMINATED | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      4 |         125.331  | 2.1302  |     0.1661 |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    | train_cifar_ebd82_00006 | TERMINATED | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |     10 |         270.562  | 1.61477 |     0.388  |
    | train_cifar_ebd82_00007 | TERMINATED | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |     10 |         296.665  | 1.39974 |     0.5766 |
    | train_cifar_ebd82_00008 | TERMINATED | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |      2 |          66.3861 | 2.0581  |     0.199  |
    | train_cifar_ebd82_00009 | TERMINATED | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |      1 |          91.5385 | 2.34375 |     0.1016 |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    (func pid=2794) [8, 10000] loss: 0.235
    == Status ==
    Current time: 2023-12-14 16:27:55 (running for 00:05:49.48)
    Using AsyncHyperBand: num_stopped=9
    Bracket: Iter 8.000: -1.5246871865570544 | Iter 4.000: -1.9316639205932618 | Iter 2.000: -1.979915303504467 | Iter 1.000: -2.234733338880539
    Logical resource usage: 2.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      7 |         310.555  | 1.24794 |     0.5533 |
    | train_cifar_ebd82_00000 | TERMINATED | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |      4 |         314.689  | 1.96417 |     0.1952 |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00002 | TERMINATED | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |      1 |         106.257  | 2.31386 |     0.1013 |
    | train_cifar_ebd82_00003 | TERMINATED | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      4 |         125.331  | 2.1302  |     0.1661 |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    | train_cifar_ebd82_00006 | TERMINATED | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |     10 |         270.562  | 1.61477 |     0.388  |
    | train_cifar_ebd82_00007 | TERMINATED | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |     10 |         296.665  | 1.39974 |     0.5766 |
    | train_cifar_ebd82_00008 | TERMINATED | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |      2 |          66.3861 | 2.0581  |     0.199  |
    | train_cifar_ebd82_00009 | TERMINATED | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |      1 |          91.5385 | 2.34375 |     0.1016 |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    Result for train_cifar_ebd82_00005:
      accuracy: 0.5488
      date: 2023-12-14_16-27-56
      done: false
      hostname: 5c54f08e0f8e
      iterations_since_restore: 8
      loss: 1.2525220602072775
      node_ip: 172.17.0.2
      pid: 2794
      should_checkpoint: true
      time_since_restore: 341.87203907966614
      time_this_iter_s: 31.317081212997437
      time_total_s: 341.87203907966614
      timestamp: 1702571276
      training_iteration: 8
      trial_id: ebd82_00005
  
    == Status ==
    Current time: 2023-12-14 16:28:01 (running for 00:05:55.76)
    Using AsyncHyperBand: num_stopped=9
    Bracket: Iter 8.000: -1.349819334757328 | Iter 4.000: -1.9316639205932618 | Iter 2.000: -1.979915303504467 | Iter 1.000: -2.234733338880539
    Logical resource usage: 2.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      8 |         341.872  | 1.25252 |     0.5488 |
    | train_cifar_ebd82_00000 | TERMINATED | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |      4 |         314.689  | 1.96417 |     0.1952 |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00002 | TERMINATED | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |      1 |         106.257  | 2.31386 |     0.1013 |
    | train_cifar_ebd82_00003 | TERMINATED | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      4 |         125.331  | 2.1302  |     0.1661 |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    | train_cifar_ebd82_00006 | TERMINATED | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |     10 |         270.562  | 1.61477 |     0.388  |
    | train_cifar_ebd82_00007 | TERMINATED | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |     10 |         296.665  | 1.39974 |     0.5766 |
    | train_cifar_ebd82_00008 | TERMINATED | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |      2 |          66.3861 | 2.0581  |     0.199  |
    | train_cifar_ebd82_00009 | TERMINATED | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |      1 |          91.5385 | 2.34375 |     0.1016 |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    (func pid=2794) [9,  2000] loss: 1.155
    == Status ==
    Current time: 2023-12-14 16:28:06 (running for 00:06:00.77)
    Using AsyncHyperBand: num_stopped=9
    Bracket: Iter 8.000: -1.349819334757328 | Iter 4.000: -1.9316639205932618 | Iter 2.000: -1.979915303504467 | Iter 1.000: -2.234733338880539
    Logical resource usage: 2.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      8 |         341.872  | 1.25252 |     0.5488 |
    | train_cifar_ebd82_00000 | TERMINATED | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |      4 |         314.689  | 1.96417 |     0.1952 |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00002 | TERMINATED | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |      1 |         106.257  | 2.31386 |     0.1013 |
    | train_cifar_ebd82_00003 | TERMINATED | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      4 |         125.331  | 2.1302  |     0.1661 |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    | train_cifar_ebd82_00006 | TERMINATED | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |     10 |         270.562  | 1.61477 |     0.388  |
    | train_cifar_ebd82_00007 | TERMINATED | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |     10 |         296.665  | 1.39974 |     0.5766 |
    | train_cifar_ebd82_00008 | TERMINATED | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |      2 |          66.3861 | 2.0581  |     0.199  |
    | train_cifar_ebd82_00009 | TERMINATED | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |      1 |          91.5385 | 2.34375 |     0.1016 |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    (func pid=2794) [9,  4000] loss: 0.570
    == Status ==
    Current time: 2023-12-14 16:28:11 (running for 00:06:05.77)
    Using AsyncHyperBand: num_stopped=9
    Bracket: Iter 8.000: -1.349819334757328 | Iter 4.000: -1.9316639205932618 | Iter 2.000: -1.979915303504467 | Iter 1.000: -2.234733338880539
    Logical resource usage: 2.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      8 |         341.872  | 1.25252 |     0.5488 |
    | train_cifar_ebd82_00000 | TERMINATED | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |      4 |         314.689  | 1.96417 |     0.1952 |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00002 | TERMINATED | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |      1 |         106.257  | 2.31386 |     0.1013 |
    | train_cifar_ebd82_00003 | TERMINATED | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      4 |         125.331  | 2.1302  |     0.1661 |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    | train_cifar_ebd82_00006 | TERMINATED | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |     10 |         270.562  | 1.61477 |     0.388  |
    | train_cifar_ebd82_00007 | TERMINATED | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |     10 |         296.665  | 1.39974 |     0.5766 |
    | train_cifar_ebd82_00008 | TERMINATED | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |      2 |          66.3861 | 2.0581  |     0.199  |
    | train_cifar_ebd82_00009 | TERMINATED | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |      1 |          91.5385 | 2.34375 |     0.1016 |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    (func pid=2794) [9,  6000] loss: 0.385
    == Status ==
    Current time: 2023-12-14 16:28:16 (running for 00:06:10.78)
    Using AsyncHyperBand: num_stopped=9
    Bracket: Iter 8.000: -1.349819334757328 | Iter 4.000: -1.9316639205932618 | Iter 2.000: -1.979915303504467 | Iter 1.000: -2.234733338880539
    Logical resource usage: 2.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      8 |         341.872  | 1.25252 |     0.5488 |
    | train_cifar_ebd82_00000 | TERMINATED | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |      4 |         314.689  | 1.96417 |     0.1952 |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00002 | TERMINATED | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |      1 |         106.257  | 2.31386 |     0.1013 |
    | train_cifar_ebd82_00003 | TERMINATED | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      4 |         125.331  | 2.1302  |     0.1661 |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    | train_cifar_ebd82_00006 | TERMINATED | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |     10 |         270.562  | 1.61477 |     0.388  |
    | train_cifar_ebd82_00007 | TERMINATED | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |     10 |         296.665  | 1.39974 |     0.5766 |
    | train_cifar_ebd82_00008 | TERMINATED | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |      2 |          66.3861 | 2.0581  |     0.199  |
    | train_cifar_ebd82_00009 | TERMINATED | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |      1 |          91.5385 | 2.34375 |     0.1016 |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    (func pid=2794) [9,  8000] loss: 0.280
    == Status ==
    Current time: 2023-12-14 16:28:21 (running for 00:06:15.79)
    Using AsyncHyperBand: num_stopped=9
    Bracket: Iter 8.000: -1.349819334757328 | Iter 4.000: -1.9316639205932618 | Iter 2.000: -1.979915303504467 | Iter 1.000: -2.234733338880539
    Logical resource usage: 2.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      8 |         341.872  | 1.25252 |     0.5488 |
    | train_cifar_ebd82_00000 | TERMINATED | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |      4 |         314.689  | 1.96417 |     0.1952 |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00002 | TERMINATED | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |      1 |         106.257  | 2.31386 |     0.1013 |
    | train_cifar_ebd82_00003 | TERMINATED | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      4 |         125.331  | 2.1302  |     0.1661 |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    | train_cifar_ebd82_00006 | TERMINATED | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |     10 |         270.562  | 1.61477 |     0.388  |
    | train_cifar_ebd82_00007 | TERMINATED | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |     10 |         296.665  | 1.39974 |     0.5766 |
    | train_cifar_ebd82_00008 | TERMINATED | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |      2 |          66.3861 | 2.0581  |     0.199  |
    | train_cifar_ebd82_00009 | TERMINATED | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |      1 |          91.5385 | 2.34375 |     0.1016 |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    (func pid=2794) [9, 10000] loss: 0.230
    == Status ==
    Current time: 2023-12-14 16:28:26 (running for 00:06:20.79)
    Using AsyncHyperBand: num_stopped=9
    Bracket: Iter 8.000: -1.349819334757328 | Iter 4.000: -1.9316639205932618 | Iter 2.000: -1.979915303504467 | Iter 1.000: -2.234733338880539
    Logical resource usage: 2.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      8 |         341.872  | 1.25252 |     0.5488 |
    | train_cifar_ebd82_00000 | TERMINATED | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |      4 |         314.689  | 1.96417 |     0.1952 |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00002 | TERMINATED | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |      1 |         106.257  | 2.31386 |     0.1013 |
    | train_cifar_ebd82_00003 | TERMINATED | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      4 |         125.331  | 2.1302  |     0.1661 |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    | train_cifar_ebd82_00006 | TERMINATED | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |     10 |         270.562  | 1.61477 |     0.388  |
    | train_cifar_ebd82_00007 | TERMINATED | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |     10 |         296.665  | 1.39974 |     0.5766 |
    | train_cifar_ebd82_00008 | TERMINATED | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |      2 |          66.3861 | 2.0581  |     0.199  |
    | train_cifar_ebd82_00009 | TERMINATED | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |      1 |          91.5385 | 2.34375 |     0.1016 |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    Result for train_cifar_ebd82_00005:
      accuracy: 0.5742
      date: 2023-12-14_16-28-27
      done: false
      hostname: 5c54f08e0f8e
      iterations_since_restore: 9
      loss: 1.1944589300751687
      node_ip: 172.17.0.2
      pid: 2794
      should_checkpoint: true
      time_since_restore: 373.2614846229553
      time_this_iter_s: 31.389445543289185
      time_total_s: 373.2614846229553
      timestamp: 1702571307
      training_iteration: 9
      trial_id: ebd82_00005
  
    == Status ==
    Current time: 2023-12-14 16:28:32 (running for 00:06:27.15)
    Using AsyncHyperBand: num_stopped=9
    Bracket: Iter 8.000: -1.349819334757328 | Iter 4.000: -1.9316639205932618 | Iter 2.000: -1.979915303504467 | Iter 1.000: -2.234733338880539
    Logical resource usage: 2.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      9 |         373.261  | 1.19446 |     0.5742 |
    | train_cifar_ebd82_00000 | TERMINATED | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |      4 |         314.689  | 1.96417 |     0.1952 |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00002 | TERMINATED | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |      1 |         106.257  | 2.31386 |     0.1013 |
    | train_cifar_ebd82_00003 | TERMINATED | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      4 |         125.331  | 2.1302  |     0.1661 |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    | train_cifar_ebd82_00006 | TERMINATED | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |     10 |         270.562  | 1.61477 |     0.388  |
    | train_cifar_ebd82_00007 | TERMINATED | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |     10 |         296.665  | 1.39974 |     0.5766 |
    | train_cifar_ebd82_00008 | TERMINATED | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |      2 |          66.3861 | 2.0581  |     0.199  |
    | train_cifar_ebd82_00009 | TERMINATED | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |      1 |          91.5385 | 2.34375 |     0.1016 |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    (func pid=2794) [10,  2000] loss: 1.133
    == Status ==
    Current time: 2023-12-14 16:28:37 (running for 00:06:32.16)
    Using AsyncHyperBand: num_stopped=9
    Bracket: Iter 8.000: -1.349819334757328 | Iter 4.000: -1.9316639205932618 | Iter 2.000: -1.979915303504467 | Iter 1.000: -2.234733338880539
    Logical resource usage: 2.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      9 |         373.261  | 1.19446 |     0.5742 |
    | train_cifar_ebd82_00000 | TERMINATED | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |      4 |         314.689  | 1.96417 |     0.1952 |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00002 | TERMINATED | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |      1 |         106.257  | 2.31386 |     0.1013 |
    | train_cifar_ebd82_00003 | TERMINATED | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      4 |         125.331  | 2.1302  |     0.1661 |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    | train_cifar_ebd82_00006 | TERMINATED | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |     10 |         270.562  | 1.61477 |     0.388  |
    | train_cifar_ebd82_00007 | TERMINATED | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |     10 |         296.665  | 1.39974 |     0.5766 |
    | train_cifar_ebd82_00008 | TERMINATED | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |      2 |          66.3861 | 2.0581  |     0.199  |
    | train_cifar_ebd82_00009 | TERMINATED | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |      1 |          91.5385 | 2.34375 |     0.1016 |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    (func pid=2794) [10,  4000] loss: 0.556
    == Status ==
    Current time: 2023-12-14 16:28:42 (running for 00:06:37.16)
    Using AsyncHyperBand: num_stopped=9
    Bracket: Iter 8.000: -1.349819334757328 | Iter 4.000: -1.9316639205932618 | Iter 2.000: -1.979915303504467 | Iter 1.000: -2.234733338880539
    Logical resource usage: 2.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      9 |         373.261  | 1.19446 |     0.5742 |
    | train_cifar_ebd82_00000 | TERMINATED | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |      4 |         314.689  | 1.96417 |     0.1952 |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00002 | TERMINATED | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |      1 |         106.257  | 2.31386 |     0.1013 |
    | train_cifar_ebd82_00003 | TERMINATED | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      4 |         125.331  | 2.1302  |     0.1661 |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    | train_cifar_ebd82_00006 | TERMINATED | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |     10 |         270.562  | 1.61477 |     0.388  |
    | train_cifar_ebd82_00007 | TERMINATED | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |     10 |         296.665  | 1.39974 |     0.5766 |
    | train_cifar_ebd82_00008 | TERMINATED | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |      2 |          66.3861 | 2.0581  |     0.199  |
    | train_cifar_ebd82_00009 | TERMINATED | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |      1 |          91.5385 | 2.34375 |     0.1016 |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    (func pid=2794) [10,  6000] loss: 0.377
    == Status ==
    Current time: 2023-12-14 16:28:47 (running for 00:06:42.17)
    Using AsyncHyperBand: num_stopped=9
    Bracket: Iter 8.000: -1.349819334757328 | Iter 4.000: -1.9316639205932618 | Iter 2.000: -1.979915303504467 | Iter 1.000: -2.234733338880539
    Logical resource usage: 2.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      9 |         373.261  | 1.19446 |     0.5742 |
    | train_cifar_ebd82_00000 | TERMINATED | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |      4 |         314.689  | 1.96417 |     0.1952 |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00002 | TERMINATED | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |      1 |         106.257  | 2.31386 |     0.1013 |
    | train_cifar_ebd82_00003 | TERMINATED | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      4 |         125.331  | 2.1302  |     0.1661 |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    | train_cifar_ebd82_00006 | TERMINATED | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |     10 |         270.562  | 1.61477 |     0.388  |
    | train_cifar_ebd82_00007 | TERMINATED | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |     10 |         296.665  | 1.39974 |     0.5766 |
    | train_cifar_ebd82_00008 | TERMINATED | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |      2 |          66.3861 | 2.0581  |     0.199  |
    | train_cifar_ebd82_00009 | TERMINATED | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |      1 |          91.5385 | 2.34375 |     0.1016 |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    (func pid=2794) [10,  8000] loss: 0.281
    == Status ==
    Current time: 2023-12-14 16:28:52 (running for 00:06:47.18)
    Using AsyncHyperBand: num_stopped=9
    Bracket: Iter 8.000: -1.349819334757328 | Iter 4.000: -1.9316639205932618 | Iter 2.000: -1.979915303504467 | Iter 1.000: -2.234733338880539
    Logical resource usage: 2.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      9 |         373.261  | 1.19446 |     0.5742 |
    | train_cifar_ebd82_00000 | TERMINATED | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |      4 |         314.689  | 1.96417 |     0.1952 |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00002 | TERMINATED | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |      1 |         106.257  | 2.31386 |     0.1013 |
    | train_cifar_ebd82_00003 | TERMINATED | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      4 |         125.331  | 2.1302  |     0.1661 |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    | train_cifar_ebd82_00006 | TERMINATED | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |     10 |         270.562  | 1.61477 |     0.388  |
    | train_cifar_ebd82_00007 | TERMINATED | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |     10 |         296.665  | 1.39974 |     0.5766 |
    | train_cifar_ebd82_00008 | TERMINATED | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |      2 |          66.3861 | 2.0581  |     0.199  |
    | train_cifar_ebd82_00009 | TERMINATED | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |      1 |          91.5385 | 2.34375 |     0.1016 |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    (func pid=2794) [10, 10000] loss: 0.224
    == Status ==
    Current time: 2023-12-14 16:28:57 (running for 00:06:52.18)
    Using AsyncHyperBand: num_stopped=9
    Bracket: Iter 8.000: -1.349819334757328 | Iter 4.000: -1.9316639205932618 | Iter 2.000: -1.979915303504467 | Iter 1.000: -2.234733338880539
    Logical resource usage: 2.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00005 | RUNNING    | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |      9 |         373.261  | 1.19446 |     0.5742 |
    | train_cifar_ebd82_00000 | TERMINATED | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |      4 |         314.689  | 1.96417 |     0.1952 |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00002 | TERMINATED | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |      1 |         106.257  | 2.31386 |     0.1013 |
    | train_cifar_ebd82_00003 | TERMINATED | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      4 |         125.331  | 2.1302  |     0.1661 |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    | train_cifar_ebd82_00006 | TERMINATED | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |     10 |         270.562  | 1.61477 |     0.388  |
    | train_cifar_ebd82_00007 | TERMINATED | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |     10 |         296.665  | 1.39974 |     0.5766 |
    | train_cifar_ebd82_00008 | TERMINATED | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |      2 |          66.3861 | 2.0581  |     0.199  |
    | train_cifar_ebd82_00009 | TERMINATED | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |      1 |          91.5385 | 2.34375 |     0.1016 |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    Result for train_cifar_ebd82_00005:
      accuracy: 0.5821
      date: 2023-12-14_16-28-59
      done: true
      hostname: 5c54f08e0f8e
      iterations_since_restore: 10
      loss: 1.1640282270863653
      node_ip: 172.17.0.2
      pid: 2794
      should_checkpoint: true
      time_since_restore: 404.7640337944031
      time_this_iter_s: 31.502549171447754
      time_total_s: 404.7640337944031
      timestamp: 1702571339
      training_iteration: 10
      trial_id: ebd82_00005
  
    Trial train_cifar_ebd82_00005 completed.
    == Status ==
    Current time: 2023-12-14 16:28:59 (running for 00:06:53.65)
    Using AsyncHyperBand: num_stopped=10
    Bracket: Iter 8.000: -1.349819334757328 | Iter 4.000: -1.9316639205932618 | Iter 2.000: -1.979915303504467 | Iter 1.000: -2.234733338880539
    Logical resource usage: 0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)
    Result logdir: /var/lib/jenkins/ray_results/train_cifar_2023-12-14_16-22-05
    Number of trials: 10/10 (10 TERMINATED)
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+
    | Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |   iter |   total time (s) |    loss |   accuracy |
    |-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------|
    | train_cifar_ebd82_00000 | TERMINATED | 172.17.0.2:2713 |            2 |   16 |    1 | 0.00213327  |      4 |         314.689  | 1.96417 |     0.1952 |
    | train_cifar_ebd82_00001 | TERMINATED | 172.17.0.2:2778 |            4 |    1 |    2 | 0.013416    |      1 |          55.0959 | 2.30874 |     0.101  |
    | train_cifar_ebd82_00002 | TERMINATED | 172.17.0.2:2780 |            2 |  256 |   64 | 0.0113784   |      1 |         106.257  | 2.31386 |     0.1013 |
    | train_cifar_ebd82_00003 | TERMINATED | 172.17.0.2:2781 |            8 |   64 |  256 | 0.0274071   |      4 |         125.331  | 2.1302  |     0.1661 |
    | train_cifar_ebd82_00004 | TERMINATED | 172.17.0.2:2786 |            4 |   16 |    2 | 0.056666    |      1 |          54.9235 | 2.33711 |     0.1015 |
    | train_cifar_ebd82_00005 | TERMINATED | 172.17.0.2:2794 |            4 |    8 |   64 | 0.000353097 |     10 |         404.764  | 1.16403 |     0.5821 |
    | train_cifar_ebd82_00006 | TERMINATED | 172.17.0.2:2796 |            8 |   16 |    4 | 0.000147684 |     10 |         270.562  | 1.61477 |     0.388  |
    | train_cifar_ebd82_00007 | TERMINATED | 172.17.0.2:2798 |            8 |  256 |  256 | 0.00477469  |     10 |         296.665  | 1.39974 |     0.5766 |
    | train_cifar_ebd82_00008 | TERMINATED | 172.17.0.2:2786 |            8 |  128 |  256 | 0.0306227   |      2 |          66.3861 | 2.0581  |     0.199  |
    | train_cifar_ebd82_00009 | TERMINATED | 172.17.0.2:2778 |            2 |    2 |   16 | 0.0286986   |      1 |          91.5385 | 2.34375 |     0.1016 |
    +-------------------------+------------+-----------------+--------------+------+------+-------------+--------+------------------+---------+------------+


    2023-12-14 16:28:59,448 INFO tune.py:945 -- Total run time: 413.72 seconds (413.65 seconds for the tuning loop).
    Best trial config: {'l1': 8, 'l2': 64, 'lr': 0.0003530972286268149, 'batch_size': 4}
    Best trial final validation loss: 1.1640282270863653
    Best trial final validation accuracy: 0.5821
    Files already downloaded and verified
    Files already downloaded and verified
    Best trial test set accuracy: 0.5994




.. GENERATED FROM PYTHON SOURCE LINES 463-493

If you run the code, an example output could look like this:

::

    Number of trials: 10/10 (10 TERMINATED)
    +-----+--------------+------+------+-------------+--------+---------+------------+
    | ... |   batch_size |   l1 |   l2 |          lr |   iter |    loss |   accuracy |
    |-----+--------------+------+------+-------------+--------+---------+------------|
    | ... |            2 |    1 |  256 | 0.000668163 |      1 | 2.31479 |     0.0977 |
    | ... |            4 |   64 |    8 | 0.0331514   |      1 | 2.31605 |     0.0983 |
    | ... |            4 |    2 |    1 | 0.000150295 |      1 | 2.30755 |     0.1023 |
    | ... |           16 |   32 |   32 | 0.0128248   |     10 | 1.66912 |     0.4391 |
    | ... |            4 |    8 |  128 | 0.00464561  |      2 | 1.7316  |     0.3463 |
    | ... |            8 |  256 |    8 | 0.00031556  |      1 | 2.19409 |     0.1736 |
    | ... |            4 |   16 |  256 | 0.00574329  |      2 | 1.85679 |     0.3368 |
    | ... |            8 |    2 |    2 | 0.00325652  |      1 | 2.30272 |     0.0984 |
    | ... |            2 |    2 |    2 | 0.000342987 |      2 | 1.76044 |     0.292  |
    | ... |            4 |   64 |   32 | 0.003734    |      8 | 1.53101 |     0.4761 |
    +-----+--------------+------+------+-------------+--------+---------+------------+

    Best trial config: {'l1': 64, 'l2': 32, 'lr': 0.0037339984519545164, 'batch_size': 4}
    Best trial final validation loss: 1.5310075663924216
    Best trial final validation accuracy: 0.4761
    Best trial test set accuracy: 0.4737

Most trials have been stopped early in order to avoid wasting resources.
The best performing trial achieved a validation accuracy of about 47%, which could
be confirmed on the test set.

So that's it! You can now tune the parameters of your PyTorch models.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 7 minutes  6.544 seconds)


.. _sphx_glr_download_beginner_hyperparameter_tuning_tutorial.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example


    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: hyperparameter_tuning_tutorial.py <hyperparameter_tuning_tutorial.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: hyperparameter_tuning_tutorial.ipynb <hyperparameter_tuning_tutorial.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
