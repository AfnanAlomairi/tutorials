
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "beginner/vt_tutorial.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_beginner_vt_tutorial.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_beginner_vt_tutorial.py:


Optimizing Vision Transformer Model for Deployment
===========================

`Jeff Tang <https://github.com/jeffxtang>`_,
`Geeta Chauhan <https://github.com/gchauhan/>`_

Vision Transformer models apply the cutting-edge attention-based
transformer models, introduced in Natural Language Processing to achieve
all kinds of the state of the art (SOTA) results, to Computer Vision
tasks. Facebook Data-efficient Image Transformers `DeiT <https://ai.facebook.com/blog/data-efficient-image-transformers-a-promising-new-technique-for-image-classification>`_
is a Vision Transformer model trained on ImageNet for image
classification.

In this tutorial, we will first cover what DeiT is and how to use it,
then go through the complete steps of scripting, quantizing, optimizing,
and using the model in iOS and Android apps. We will also compare the
performance of quantized, optimized and non-quantized, non-optimized
models, and show the benefits of applying quantization and optimization
to the model along the steps.

.. GENERATED FROM PYTHON SOURCE LINES 27-47

What is DeiT
---------------------

Convolutional Neural Networks (CNNs) have been the main models for image
classification since deep learning took off in 2012, but CNNs typically
require hundreds of millions of images for training to achieve the
SOTAresults. DeiT is a vision transformer model that requires a lot less
data and computing resources for training to compete with the leading
CNNs in performing image classification, which is made possible by two
key components of of DeiT:

-  Data augmentation that simulates training on a much larger dataset;
-  Native distillation that allows the transformer network to learn from
   a CNN’s output.

DeiT shows that Transformers can be successfully applied to computer
vision tasks, with limited access to data and resources. For more
details on DeiT, see the `repo <https://github.com/facebookresearch/deit>`_
and `paper <https://arxiv.org/abs/2012.12877>`_.


.. GENERATED FROM PYTHON SOURCE LINES 50-56

Classifying Images with DeiT
-------------------------------

Follow the README at the DeiT repo for detailed information on how to
classify images using DeiT, or for a quick test, first install the
required packages:

.. GENERATED FROM PYTHON SOURCE LINES 56-59

.. code-block:: default


    # pip install torch torchvision timm pandas requests








.. GENERATED FROM PYTHON SOURCE LINES 60-61

To run in Google Colab, uncomment the following line:

.. GENERATED FROM PYTHON SOURCE LINES 61-64

.. code-block:: default


    # !pip install timm pandas requests








.. GENERATED FROM PYTHON SOURCE LINES 65-66

then run the script below:

.. GENERATED FROM PYTHON SOURCE LINES 66-95

.. code-block:: default


    from PIL import Image
    import torch
    import timm
    import requests
    import torchvision.transforms as transforms
    from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD

    print(torch.__version__)
    # should be 1.8.0


    model = torch.hub.load('facebookresearch/deit:main', 'deit_base_patch16_224', pretrained=True)
    model.eval()

    transform = transforms.Compose([
        transforms.Resize(256, interpolation=3),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),
    ])

    img = Image.open(requests.get("https://raw.githubusercontent.com/pytorch/ios-demo-app/master/HelloWorld/HelloWorld/HelloWorld/image.png", stream=True).raw)
    img = transform(img)[None,]
    out = model(img)
    clsidx = torch.argmax(out)
    print(clsidx.item())






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    1.13.1+cu117
    Downloading: "https://github.com/facebookresearch/deit/zipball/main" to /var/lib/jenkins/.cache/torch/hub/main.zip
    Downloading: "https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth" to /var/lib/jenkins/.cache/torch/hub/checkpoints/deit_base_patch16_224-b5f2ef4d.pth

      0%|          | 0.00/330M [00:00<?, ?B/s]
      0%|          | 56.0k/330M [00:00<10:57, 526kB/s]
      0%|          | 176k/330M [00:00<06:48, 848kB/s] 
      0%|          | 312k/330M [00:00<05:24, 1.07MB/s]
      0%|          | 432k/330M [00:00<05:19, 1.08MB/s]
      0%|          | 568k/330M [00:00<05:02, 1.14MB/s]
      0%|          | 680k/330M [00:00<05:04, 1.13MB/s]
      0%|          | 816k/330M [00:00<04:52, 1.18MB/s]
      0%|          | 936k/330M [00:00<05:03, 1.14MB/s]
      0%|          | 1.03M/330M [00:00<05:04, 1.13MB/s]
      0%|          | 1.28M/330M [00:01<03:54, 1.47MB/s]
      1%|          | 2.46M/330M [00:01<01:16, 4.48MB/s]
      2%|2         | 7.04M/330M [00:01<00:20, 16.8MB/s]
      4%|4         | 13.8M/330M [00:01<00:10, 32.3MB/s]
      6%|5         | 19.4M/330M [00:01<00:08, 39.9MB/s]
      8%|8         | 27.1M/330M [00:01<00:06, 52.0MB/s]
     10%|9         | 32.2M/330M [00:01<00:06, 50.7MB/s]
     12%|#1        | 39.4M/330M [00:01<00:05, 56.1MB/s]
     14%|#4        | 47.4M/330M [00:01<00:04, 63.3MB/s]
     16%|#6        | 53.8M/330M [00:02<00:04, 64.4MB/s]
     18%|#8        | 60.4M/330M [00:02<00:04, 64.5MB/s]
     20%|##        | 66.6M/330M [00:02<00:04, 59.0MB/s]
     22%|##1       | 72.3M/330M [00:02<00:04, 57.4MB/s]
     24%|##4       | 79.6M/330M [00:02<00:04, 62.4MB/s]
     26%|##6       | 85.9M/330M [00:02<00:04, 63.3MB/s]
     28%|##7       | 92.0M/330M [00:02<00:04, 55.1MB/s]
     30%|##9       | 98.1M/330M [00:02<00:04, 57.3MB/s]
     31%|###1      | 104M/330M [00:02<00:04, 55.6MB/s] 
     33%|###3      | 109M/330M [00:03<00:04, 51.0MB/s]
     35%|###4      | 114M/330M [00:03<00:04, 49.2MB/s]
     36%|###6      | 119M/330M [00:03<00:04, 47.6MB/s]
     37%|###7      | 124M/330M [00:03<00:04, 46.4MB/s]
     39%|###8      | 128M/330M [00:03<00:04, 45.6MB/s]
     40%|####      | 132M/330M [00:03<00:04, 45.0MB/s]
     41%|####1     | 137M/330M [00:03<00:04, 44.0MB/s]
     43%|####2     | 141M/330M [00:03<00:04, 44.6MB/s]
     44%|####4     | 146M/330M [00:03<00:04, 44.9MB/s]
     45%|####5     | 150M/330M [00:04<00:04, 45.1MB/s]
     47%|####6     | 155M/330M [00:04<00:04, 45.1MB/s]
     48%|####8     | 159M/330M [00:04<00:04, 44.7MB/s]
     49%|####9     | 163M/330M [00:04<00:04, 43.2MB/s]
     51%|#####     | 168M/330M [00:04<00:03, 44.3MB/s]
     52%|#####2    | 172M/330M [00:04<00:03, 44.3MB/s]
     53%|#####3    | 176M/330M [00:04<00:04, 38.8MB/s]
     55%|#####4    | 181M/330M [00:04<00:03, 40.2MB/s]
     56%|#####5    | 185M/330M [00:04<00:03, 38.5MB/s]
     57%|#####7    | 188M/330M [00:05<00:04, 37.1MB/s]
     58%|#####8    | 192M/330M [00:05<00:04, 35.7MB/s]
     59%|#####9    | 195M/330M [00:05<00:04, 34.8MB/s]
     60%|######    | 199M/330M [00:05<00:04, 34.3MB/s]
     61%|######1   | 202M/330M [00:05<00:04, 31.9MB/s]
     62%|######2   | 205M/330M [00:05<00:03, 32.8MB/s]
     63%|######3   | 209M/330M [00:05<00:03, 32.9MB/s]
     64%|######4   | 212M/330M [00:05<00:03, 34.0MB/s]
     65%|######5   | 216M/330M [00:05<00:03, 34.3MB/s]
     66%|######6   | 220M/330M [00:06<00:03, 34.5MB/s]
     68%|######7   | 223M/330M [00:06<00:03, 35.3MB/s]
     69%|######8   | 226M/330M [00:06<00:03, 34.7MB/s]
     70%|######9   | 230M/330M [00:06<00:03, 33.8MB/s]
     71%|#######   | 233M/330M [00:06<00:03, 33.3MB/s]
     72%|#######1  | 237M/330M [00:06<00:02, 34.3MB/s]
     73%|#######2  | 240M/330M [00:06<00:02, 35.2MB/s]
     74%|#######3  | 244M/330M [00:06<00:02, 35.7MB/s]
     75%|#######4  | 247M/330M [00:06<00:02, 35.2MB/s]
     76%|#######5  | 251M/330M [00:07<00:02, 31.4MB/s]
     77%|#######7  | 255M/330M [00:07<00:02, 36.0MB/s]
     78%|#######8  | 259M/330M [00:07<00:02, 34.8MB/s]
     79%|#######9  | 262M/330M [00:07<00:02, 35.2MB/s]
     80%|########  | 266M/330M [00:07<00:01, 35.3MB/s]
     81%|########1 | 269M/330M [00:07<00:02, 31.8MB/s]
     83%|########2 | 273M/330M [00:07<00:01, 35.1MB/s]
     84%|########3 | 277M/330M [00:07<00:01, 30.5MB/s]
     85%|########4 | 280M/330M [00:07<00:01, 30.1MB/s]
     86%|########5 | 283M/330M [00:08<00:01, 29.7MB/s]
     87%|########6 | 286M/330M [00:08<00:01, 28.6MB/s]
     87%|########7 | 289M/330M [00:08<00:01, 28.0MB/s]
     88%|########8 | 291M/330M [00:08<00:01, 27.4MB/s]
     89%|########8 | 294M/330M [00:08<00:01, 27.0MB/s]
     90%|########9 | 297M/330M [00:08<00:01, 27.1MB/s]
     91%|######### | 299M/330M [00:08<00:01, 26.9MB/s]
     91%|#########1| 302M/330M [00:08<00:01, 26.6MB/s]
     92%|#########2| 305M/330M [00:08<00:00, 27.8MB/s]
     93%|#########3| 307M/330M [00:09<00:00, 27.4MB/s]
     94%|#########3| 310M/330M [00:09<00:00, 27.6MB/s]
     95%|#########4| 313M/330M [00:09<00:00, 27.6MB/s]
     96%|#########5| 316M/330M [00:09<00:00, 25.9MB/s]
     97%|#########6| 319M/330M [00:09<00:00, 29.4MB/s]
     98%|#########7| 322M/330M [00:09<00:00, 29.0MB/s]
     98%|#########8| 325M/330M [00:09<00:00, 28.7MB/s]
     99%|#########9| 328M/330M [00:09<00:00, 28.7MB/s]
    100%|##########| 330M/330M [00:09<00:00, 35.1MB/s]
    /opt/conda/lib/python3.10/site-packages/torchvision/transforms/transforms.py:329: UserWarning:

    Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.

    269




.. GENERATED FROM PYTHON SOURCE LINES 96-104

The output should be 269, which, according to the ImageNet list of class
index to `labels file <https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a>`_, maps to ‘timber
wolf, grey wolf, gray wolf, Canis lupus’.

Now that we have verified that we can use the DeiT model to classify
images, let’s see how to modify the model so it can run on iOS and
Android apps.


.. GENERATED FROM PYTHON SOURCE LINES 107-114

Scripting DeiT
----------------------
To use the model on mobile, we first need to script the
model. See the `Script and Optimize recipe <https://pytorch.org/tutorials/recipes/script_optimized.html>`_ for a
quick overview. Run the code below to convert the DeiT model used in the
previous step to the TorchScript format that can run on mobile.


.. GENERATED FROM PYTHON SOURCE LINES 114-122

.. code-block:: default



    model = torch.hub.load('facebookresearch/deit:main', 'deit_base_patch16_224', pretrained=True)
    model.eval()
    scripted_model = torch.jit.script(model)
    scripted_model.save("fbdeit_scripted.pt")






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Using cache found in /var/lib/jenkins/.cache/torch/hub/facebookresearch_deit_main




.. GENERATED FROM PYTHON SOURCE LINES 123-126

The scripted model file fbdeit_scripted.pt of size about 346MB is
generated.


.. GENERATED FROM PYTHON SOURCE LINES 129-140

Quantizing DeiT
---------------------
To reduce the trained model size significantly while
keeping the inference accuracy about the same, quantization can be
applied to the model. Thanks to the transformer model used in DeiT, we
can easily apply dynamic-quantization to the model, because dynamic
quantization works best for LSTM and transformer models (see `here <https://pytorch.org/docs/stable/quantization.html?highlight=quantization#dynamic-quantization>`_
for more details).

Now run the code below:


.. GENERATED FROM PYTHON SOURCE LINES 140-151

.. code-block:: default


    # Use 'fbgemm' for server inference and 'qnnpack' for mobile inference
    backend = "fbgemm" # replaced with qnnpack causing much worse inference speed for quantized model on this notebook
    model.qconfig = torch.quantization.get_default_qconfig(backend)
    torch.backends.quantized.engine = backend

    quantized_model = torch.quantization.quantize_dynamic(model, qconfig_spec={torch.nn.Linear}, dtype=torch.qint8)
    scripted_quantized_model = torch.jit.script(quantized_model)
    scripted_quantized_model.save("fbdeit_scripted_quantized.pt")






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /opt/conda/lib/python3.10/site-packages/torch/ao/quantization/observer.py:214: UserWarning:

    Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.





.. GENERATED FROM PYTHON SOURCE LINES 152-156

This generates the scripted and quantized version of the model
fbdeit_quantized_scripted.pt, with size about 89MB, a 74% reduction of
the non-quantized model size of 346MB!


.. GENERATED FROM PYTHON SOURCE LINES 158-161

You can use the ``scripted_quantized_model`` to generate the same
inference result:


.. GENERATED FROM PYTHON SOURCE LINES 161-167

.. code-block:: default


    out = scripted_quantized_model(img)
    clsidx = torch.argmax(out)
    print(clsidx.item())
    # The same output 269 should be printed





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    269




.. GENERATED FROM PYTHON SOURCE LINES 168-173

Optimizing DeiT
---------------------
The final step before using the quantized and scripted
model on mobile is to optimize it:


.. GENERATED FROM PYTHON SOURCE LINES 173-179

.. code-block:: default


    from torch.utils.mobile_optimizer import optimize_for_mobile
    optimized_scripted_quantized_model = optimize_for_mobile(scripted_quantized_model)
    optimized_scripted_quantized_model.save("fbdeit_optimized_scripted_quantized.pt")









.. GENERATED FROM PYTHON SOURCE LINES 180-184

The generated fbdeit_optimized_scripted_quantized.pt file has about the
same size as the quantized, scripted, but non-optimized model. The
inference result remains the same.


.. GENERATED FROM PYTHON SOURCE LINES 184-193

.. code-block:: default




    out = optimized_scripted_quantized_model(img)
    clsidx = torch.argmax(out)
    print(clsidx.item())
    # Again, the same output 269 should be printed






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    269




.. GENERATED FROM PYTHON SOURCE LINES 194-200

Using Lite Interpreter
------------------------

To see how much model size reduction and inference speed up the Lite
Interpreter can result in, let’s create the lite version of the model.


.. GENERATED FROM PYTHON SOURCE LINES 200-205

.. code-block:: default


    optimized_scripted_quantized_model._save_for_lite_interpreter("fbdeit_optimized_scripted_quantized_lite.ptl")
    ptl = torch.jit.load("fbdeit_optimized_scripted_quantized_lite.ptl")









.. GENERATED FROM PYTHON SOURCE LINES 206-209

Although the lite model size is comparable to the non-lite version, when
running the lite version on mobile, the inference speed up is expected.


.. GENERATED FROM PYTHON SOURCE LINES 212-219

Comparing Inference Speed
---------------------------

To see how the inference speed differs for the four models - the
original model, the scripted model, the quantized-and-scripted model,
the optimized-quantized-and-scripted model - run the code below:


.. GENERATED FROM PYTHON SOURCE LINES 219-237

.. code-block:: default


    with torch.autograd.profiler.profile(use_cuda=False) as prof1:
        out = model(img)
    with torch.autograd.profiler.profile(use_cuda=False) as prof2:
        out = scripted_model(img)
    with torch.autograd.profiler.profile(use_cuda=False) as prof3:
        out = scripted_quantized_model(img)
    with torch.autograd.profiler.profile(use_cuda=False) as prof4:
        out = optimized_scripted_quantized_model(img)
    with torch.autograd.profiler.profile(use_cuda=False) as prof5:
        out = ptl(img)

    print("original model: {:.2f}ms".format(prof1.self_cpu_time_total/1000))
    print("scripted model: {:.2f}ms".format(prof2.self_cpu_time_total/1000))
    print("scripted & quantized model: {:.2f}ms".format(prof3.self_cpu_time_total/1000))
    print("scripted & quantized & optimized model: {:.2f}ms".format(prof4.self_cpu_time_total/1000))
    print("lite model: {:.2f}ms".format(prof5.self_cpu_time_total/1000))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    original model: 354.80ms
    scripted model: 384.63ms
    scripted & quantized model: 280.62ms
    scripted & quantized & optimized model: 270.27ms
    lite model: 256.81ms




.. GENERATED FROM PYTHON SOURCE LINES 238-248

The results running on a Google Colab are:

::

   original model: 1236.69ms
   scripted model: 1226.72ms
   scripted & quantized model: 593.19ms
   scripted & quantized & optimized model: 598.01ms
   lite model: 600.72ms


.. GENERATED FROM PYTHON SOURCE LINES 251-255

The following results summarize the inference time taken by each model
and the percentage reduction of each model relative to the original
model.


.. GENERATED FROM PYTHON SOURCE LINES 255-283

.. code-block:: default


    import pandas as pd
    import numpy as np

    df = pd.DataFrame({'Model': ['original model','scripted model', 'scripted & quantized model', 'scripted & quantized & optimized model', 'lite model']})
    df = pd.concat([df, pd.DataFrame([
        ["{:.2f}ms".format(prof1.self_cpu_time_total/1000), "0%"],
        ["{:.2f}ms".format(prof2.self_cpu_time_total/1000),
         "{:.2f}%".format((prof1.self_cpu_time_total-prof2.self_cpu_time_total)/prof1.self_cpu_time_total*100)],
        ["{:.2f}ms".format(prof3.self_cpu_time_total/1000),
         "{:.2f}%".format((prof1.self_cpu_time_total-prof3.self_cpu_time_total)/prof1.self_cpu_time_total*100)],
        ["{:.2f}ms".format(prof4.self_cpu_time_total/1000),
         "{:.2f}%".format((prof1.self_cpu_time_total-prof4.self_cpu_time_total)/prof1.self_cpu_time_total*100)],
        ["{:.2f}ms".format(prof5.self_cpu_time_total/1000),
         "{:.2f}%".format((prof1.self_cpu_time_total-prof5.self_cpu_time_total)/prof1.self_cpu_time_total*100)]],
        columns=['Inference Time', 'Reduction'])], axis=1)

    print(df)

    """
            Model                             Inference Time    Reduction
    0	original model                             1236.69ms           0%
    1	scripted model                             1226.72ms        0.81%
    2	scripted & quantized model                  593.19ms       52.03%
    3	scripted & quantized & optimized model      598.01ms       51.64%
    4	lite model                                  600.72ms       51.43%
    """





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

                                        Model Inference Time Reduction
    0                          original model       354.80ms        0%
    1                          scripted model       384.63ms    -8.41%
    2              scripted & quantized model       280.62ms    20.91%
    3  scripted & quantized & optimized model       270.27ms    23.83%
    4                              lite model       256.81ms    27.62%

    '\n        Model                             Inference Time    Reduction\n0\toriginal model                             1236.69ms           0%\n1\tscripted model                             1226.72ms        0.81%\n2\tscripted & quantized model                  593.19ms       52.03%\n3\tscripted & quantized & optimized model      598.01ms       51.64%\n4\tlite model                                  600.72ms       51.43%\n'



.. GENERATED FROM PYTHON SOURCE LINES 284-290

Learn More
~~~~~~~~~~~~~~~~~

- `Facebook Data-efficient Image Transformers <https://ai.facebook.com/blog/data-efficient-image-transformers-a-promising-new-technique-for-image-classification>`__
- `Vision Transformer with ImageNet and MNIST on iOS <https://github.com/pytorch/ios-demo-app/tree/master/ViT4MNIST>`__
- `Vision Transformer with ImageNet and MNIST on Android <https://github.com/pytorch/android-demo-app/tree/master/ViT4MNIST>`__


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  33.448 seconds)


.. _sphx_glr_download_beginner_vt_tutorial.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example


    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: vt_tutorial.py <vt_tutorial.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: vt_tutorial.ipynb <vt_tutorial.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
